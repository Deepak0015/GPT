{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339249,"sourceType":"datasetVersion","datasetId":7093845},{"sourceId":11378548,"sourceType":"datasetVersion","datasetId":7124129},{"sourceId":11378997,"sourceType":"datasetVersion","datasetId":7124489}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math \nimport torch \nimport torch.nn as nn \nimport transformers \nfrom tqdm.notebook import tqdm\n# Memory Network\n\nimport torch.nn.functional as F \nfrom typing import Tuple , Optional\nimport torch.bin \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:40.917188Z","iopub.execute_input":"2025-04-24T09:34:40.917517Z","iopub.status.idle":"2025-04-24T09:34:40.921905Z","shell.execute_reply.started":"2025-04-24T09:34:40.917494Z","shell.execute_reply":"2025-04-24T09:34:40.920962Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:40.966135Z","iopub.execute_input":"2025-04-24T09:34:40.966402Z","iopub.status.idle":"2025-04-24T09:34:40.970032Z","shell.execute_reply.started":"2025-04-24T09:34:40.966380Z","shell.execute_reply":"2025-04-24T09:34:40.969063Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# RMS NORM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim)) \n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        norm_x = self._norm(x.float()).type_as(x) \n        return norm_x * self.scale \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:40.976580Z","iopub.execute_input":"2025-04-24T09:34:40.976781Z","iopub.status.idle":"2025-04-24T09:34:40.981932Z","shell.execute_reply.started":"2025-04-24T09:34:40.976756Z","shell.execute_reply":"2025-04-24T09:34:40.981318Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Embedding Layer","metadata":{}},{"cell_type":"code","source":"import torch \n\nclass EmbeddingLayer(torch.nn.Module):\n    def __init__(self , vocab_size , embedding_dim):\n        super().__init__()\n\n        self.embedding_layer= torch.nn.Embedding(vocab_size , embedding_dim)\n\n    def forward(self , input_tokens):\n        return self.embedding_layer(input_tokens)\n        \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:40.985447Z","iopub.execute_input":"2025-04-24T09:34:40.985652Z","iopub.status.idle":"2025-04-24T09:34:40.994634Z","shell.execute_reply.started":"2025-04-24T09:34:40.985635Z","shell.execute_reply":"2025-04-24T09:34:40.993826Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# FeedForward Layer ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n      \n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n            GELU(),\n            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n        )\n    def forward(self, x ):\n        return self.layers(x)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:40.995767Z","iopub.execute_input":"2025-04-24T09:34:40.995959Z","iopub.status.idle":"2025-04-24T09:34:41.006887Z","shell.execute_reply.started":"2025-04-24T09:34:40.995943Z","shell.execute_reply":"2025-04-24T09:34:41.006035Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Normalization Layer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch \n\nclass LayerNorm(nn.Module):\n    def __init__(self , emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale  = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self , x):\n        mean = x.mean(dim= -1, keepdim = True)\n        var = x.var(dim =-1, keepdim = True)\n        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n        return self.scale * norm_x + self.shift \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:41.008027Z","iopub.execute_input":"2025-04-24T09:34:41.008214Z","iopub.status.idle":"2025-04-24T09:34:41.019600Z","shell.execute_reply.started":"2025-04-24T09:34:41.008197Z","shell.execute_reply":"2025-04-24T09:34:41.018853Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# RoPE Embdding ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport torch \nfrom dataclasses import dataclass\n\n\nclass NRopE: # RopE in Numpy \n    def rotate_2d(self,vec , theta_p):\n        cos_theta  , sin_theta  = np.cos(theta_p) , np.sin(theta_p)\n        rotat_vec = np.array([[cos_theta , -sin_theta],\n                    [sin_theta ,cos_theta]])\n        \n        return rotat_vec @ vec\n\n\n    def RoPe(self,x , p , theta = 10000):\n        d = len(x)\n        x_rotate =  np.zeros_like(x)\n        for i in range(0 , d , 2):\n            if i +1< d:\n                theta_p = (theta **(-2*(i//2)))**p \n                roted_pair = self.rotate_2d(x[i:i+1] , theta_p)    \n                x_rotate[i:i+1] = roted_pair\n\n        return x_rotate\n\n\n\n@dataclass\nclass TRopE(torch.nn.Module): # RopE in torch \n    def __init__(self, dim:int ,theta:float = 10000):\n        self.dim = dim \n        self.theta = theta \n        self.freq =  torch.pow(self.theta ,-torch.arange(0 ,dim  , 2)/dim )\n        torch.nn.Parameter('freq' , self.freq)\n\n    def forward(self, x:torch.Tensor , pos:torch.Tensor):\n        batch_size , seq_len, dim = x.shape\n        assert dim ==self.dim ,\"Error dim must be same\"\n        theta_p = torch.einsum(\"n,d->nd\" , pos, self.freq.to(x.device))\n        cos_theta  , sin_theta = torch.cos(theta_p) , torch.sin(theta_p)\n        x_even , x_odd =  x[... , ::2] , x[... , 1::2]\n        x_rotated =  torch.empty_like(x)\n        x_rotated[...,::2] =  x_even * cos_theta - x_odd * sin_theta\n        x_rotated[...,1::2] =  x_even * sin_theta + x_odd * cos_theta\n\n        return x_rotated\n\n\n\n\n\n\n\ndef precompute_freq_cis(  dim:int , end:int , theta:float = 10000.0):\n        \"\"\"dim : dimentions \n        end: end index   \n        \"\"\"\n        freqs =  1/(theta **(torch.arange(0 , dim , 2)[:dim//2].float() / dim))\n        t =  torch.arange(end, device=freqs.device)\n        freqs = torch.outer(t , freqs).float()\n        freqs_cis =  torch.polar(torch.ones_like(freqs), freqs)\n        return freqs_cis \n\n\ndef reshape_for_broadcast(freq_cis  , x):\n        \"\"\" reshape the freqcies to match x dimentions \"\"\"\n        ndim=  x.ndim\n        assert 0<=1<ndim \n        assert freq_cis.shape == (x.shape[1], x.shape[-1]), f\"Expected {(x.shape[1], x.shape[-1])}, got {freq_cis.shape}\" \n        shape = [d if i == 1 or i ==  ndim -1 else 1 for i , d in enumerate(x.shape)]\n        return freq_cis.view(*shape)\n\n\ndef apply_rotary_embedding( xq:torch.Tensor ,xk:torch.Tensor ,  freq_cis:torch.Tensor):\n\n            xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n            xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n\n\n            freq_cies =  reshape_for_broadcast(freq_cis , xq_)\n    \n\n            xq_out = torch.view_as_real(xq_* freq_cies).flatten(3)\n            \n            xk_out = torch.view_as_real(xk_*freq_cies).flatten(3)\n\n\n            return  xq_out.type_as(xq)   ,  xk_out.type_as(xq) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:41.047848Z","iopub.execute_input":"2025-04-24T09:34:41.048040Z","iopub.status.idle":"2025-04-24T09:34:41.059464Z","shell.execute_reply.started":"2025-04-24T09:34:41.048023Z","shell.execute_reply":"2025-04-24T09:34:41.058678Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# MultiHead & MultiQuery Attention Layer ","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention_V2(nn.Module):\n    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n        super().__init__()\n        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n        self.d_in =d_in\n        self.d_out = d_out\n        self.dropout = nn.Dropout(dropout)\n        self.num_heads  = num_heads\n        self.head_dim = d_out // num_heads\n        self.out_proj  = nn.Linear(d_out , d_out)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens , d_in = x.shape\n        keys = self.w_key(x)\n        queries  = self.w_query(x)\n        values = self.w_value(x)\n        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(2, 3)\n        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n        attn_scores.masked_fill(mask_bool , -torch.inf)\n        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n        attn_weights = self.dropout(attn_weights)\n        context_vector = (attn_weights  @ values).transpose(1, 2)\n        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n        context_vector = self.out_proj(context_vector)\n        return context_vector\n\n\n\n\ndef apply_rotary_embedding(xq:torch.Tensor , xk:torch.Tensor , freq_cies:torch.Tensor):\n\n    assert xq.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n    assert xk.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1] , -1, 2))\n\n    freq_cies = reshape_for_broadcast(freq_cies , xq_)\n\n    xq_out = torch.view_as_real(xq_ * freq_cies ).flatten(3)\n\n    xk_out = torch.view_as_real(xk_ * freq_cies).flatten(3)\n\n    return xq_out.type_as(xq) ,  xk_out.type_as(xk)\n\n\n\n\nclass MultiQueryAttentionBlock(nn.Module):\n    def __init__(self, d_model:int , h:int , dropout:float , seq_len:int , qkv_bias =  False ):\n        super().__init__()\n        self.d_model  = d_model \n\n        self.seq_len=  seq_len\n\n        assert d_model % h == 0, \"d_model is must be divided by th head\"\n        self.dropout = nn.Dropout(dropout)\n\n        self.h = h  \n\n        self.d_k = d_model // h \n\n        self.w_qkv =  nn.Linear(d_model , d_model +2 * self.d_k )\n\n        self.w_o = nn.Linear(d_model  , d_model)\n\n        freq_cies = precompute_freq_cis(dim=self.d_k , end=self.seq_len * 2 )\n\n        self.register_buffer('freq_cies' , freq_cies , persistent= False )\n\n    def generate_causal_mask(self, seq_len, device):\n        # shape: (1, 1, seq_len, seq_len)\n        return torch.tril(torch.ones((1, 1, seq_len, seq_len), device=device)).bool()\n\n    @staticmethod\n    def attention(q, k  , v,mask  , dropout):\n        d_k = q.shape[-1]\n\n        attention_score =  (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n\n        if mask is not None :\n            if mask.dim() == 2:\n                      mask = mask.unsqueeze(1).unsqueeze(2)\n            elif mask.dim() == 3:\n                     mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n        \n        attention_score = attention_score.softmax(dim=-1)\n\n        if dropout is not None :\n            attention_score = dropout(attention_score)\n\n        context_vector =  attention_score @ v\n\n        return context_vector  , attention_score\n    \n\n\n    def forward(self, q, mask= None):\n        if mask is None:\n            mask = self.generate_causal_mask(self.seq_len , device = q.device)\n        qkv =  self.w_qkv(q)\n\n        query , key, value =  torch.split(qkv , [self.d_model  , self.d_k , self.d_k], dim=-1)\n\n        query = query.view(query.shape[0] , -1 , self.h , self.d_k).transpose(1, 2)\n\n        key =  key.unsqueeze(1)\n\n        value =  value.unsqueeze(1)\n\n        seq_len =  q.size(1)\n\n        freq_cies = self.freq_cies[:query.shape[1]].to(q.device)\n\n        # freq_cies =  self.freq_cies[:seq_len].to(q.device)\n\n        query , key = apply_rotary_embedding(query , key , freq_cies)\n\n        x , self.attention_score = MultiQueryAttentionBlock.attention(q = query,k =  key,v= value ,mask=mask , dropout= self.dropout)\n\n        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h* self.d_k)\n\n        x = self.w_o(x)\n\n        return x \n    \n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:41.145715Z","iopub.execute_input":"2025-04-24T09:34:41.145910Z","iopub.status.idle":"2025-04-24T09:34:41.161167Z","shell.execute_reply.started":"2025-04-24T09:34:41.145893Z","shell.execute_reply":"2025-04-24T09:34:41.160477Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\n\nclass MemorySummaryWriter(nn.Module):\n    def __init__(self, hidden_dim: int, compress_dim: int , num_heads :int =  4 , dropout_rate:float=  0.1):\n        super().__init__()\n\n        self.num_heads = num_heads \n        self.compress_dim = compress_dim\n        self.dropout =  nn.Dropout(dropout_rate)\n\n        assert hidden_dim % num_heads == 0 , \"hidden dim must divisible by num_heads\"\n        self.importance_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dim , hidden_dim // num_heads) , \n                nn.GELU(), \n                nn.Linear(hidden_dim // num_heads, 1)\n            ) for _ in range(num_heads)\n        ])\n\n        self.compresser = nn.Sequential(\n            nn.Linear(hidden_dim * num_heads , hidden_dim * 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate), \n            nn.Linear(hidden_dim * 2, compress_dim),\n            nn.LayerNorm(compress_dim),\n            nn.GELU()\n        )\n    def forward(self , x:torch.Tensor , mask:Optional[torch.Tensor] = None):\n\n        bs, seq_len , d = x.shape \n\n        head_weights =  []\n\n        for head in self.importance_heads:\n            head_weights.append(head(x)) # [B, S , 1]\n\n        attn_weights =  torch.cat(head_weights, dim= -1) # [B , S, num_heads]\n\n        if mask is not None:\n            attn_weights = attn_weights.masked_fill(mask.unsqueeze(-1)==0,float('-inf'))\n\n        attn_weights =  F.softmax(attn_weights , dim= 1 ) # Normalize across the Seq [B , S , num_heads ]\n\n        summary =  torch.einsum('bth, btd->bhd', attn_weights , x )\n# Replace reshape with more robust flattening\n        summary = summary.contiguous().view(bs, -1)  # Safer flattening\n        # pass to the compresser \n        compressed_emb = self.compresser(summary)\n        # compressed_emb =  self.output_projection(compressed_emb)\n\n        return compressed_emb ,  attn_weights\n\n\n\n# use summary writter after the transformer block \n\ndef init_weight_router(route_layer, preferred='semantic'):\n    with torch.no_grad():\n        route_layer.bias.fill_(-1.0)  # Set all to -1\n        idx = {'episodic': 0, 'semantic': 1}[preferred]\n        route_layer.bias[idx] = 1.0   # Prefer this one\n\n\nclass MemoryRouter(nn.Module):\n    \"\"\"\n        The Router Help to select which memory to use [Episodic , semantic ]\n\n    \"\"\"\n\n    def __init__(self,input_dim:int ,temperature:float= 1.0 , dropout:float = 0.1 ):\n        super().__init__()\n        \"\"\"\n            input_dim:input tensor dimention , \n            episodic_capacity : episodic memory size \n            semantic_capcity : semantic memory size \n            temprature :select the memory \n            dropout: dropout rate for Dropout layer \n\n\n        \"\"\"\n\n        self.input_dim =input_dim\n        # self.episodic_memory = episodic_capacity \n        # self.semantic_memory  = semantic_capacity \n        # self.context_lenth =  context_length \n        self.temprature = temperature \n        self.num_memories =  2\n        self.memories_names =  ['episodic', 'semantic' ]\n        self.training = True \n        self.dropout = nn.Dropout(dropout)\n\n        self.router = nn.Sequential(\n            nn.Linear(input_dim , input_dim *2 ),\n            nn.GELU(),\n            nn.Linear(input_dim * 2 , self.num_memories )    # Ouput the num of memories 3 \n        )\n     \n        init_weight_router(self.router[-1]  , preferred='semantic')\n\n\n        self.register_buffer('memory_counts' , torch.zeros(self.num_memories)) # Use to track how many time each memory is used \n        self.register_buffer('total_routed' , torch.tensor(0))  # counter for how many time the router decisions \n\n\n\n\n    def forward(self , x:torch.Tensor , current_context:Optional[torch.Tensor] =  None ) -> Tuple[torch.Tensor , torch.Tensor, dict ]:\n\n       \"\"\"x:input tensor\n        current_context: current context or previous memory\n\n        return memory wights , meta deta, write indices \n       \"\"\"\n       batch_size, seq_len , _ =  x.shape \n\n       route_logits =  self.router(self.dropout(x)) /  self.temprature \n       memory_weights =  F.softmax(route_logits , dim=-1)\n\n     \n\n    \n       if self.training:\n           self._update_stats(memory_weights)\n           capacity_utilization  = self.memory_counts / self.total_routed \n           memory_weights = memory_weights * (1.0-capacity_utilization)\n       meta_data = {\n           'usage':self._get_memory_usage(), \n           'weights':memory_weights.mean(dim= (0,1)).detach(),\n       }\n\n       return memory_weights , meta_data \n     \n\n    def _update_stats(self, memories_weights: torch.Tensor):\n        decisions =  torch.argmax(memories_weights , dim=-1).flatten()\n        counts  =  torch.bincount(decisions ,minlength=self.num_memories)\n        self.memory_counts += counts \n        self.total_routed += decisions.numel()\n\n    def _get_memory_usage(self):\n        if self.total_routed == 0:\n            return {name:0.0 for name in self.memories_names}\n        usage = self.memory_counts /  self.total_routed\n        return {name:usage[i].item() for i , name in enumerate (self.memories_names)}\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:41.162175Z","iopub.execute_input":"2025-04-24T09:34:41.162420Z","iopub.status.idle":"2025-04-24T09:34:41.176522Z","shell.execute_reply.started":"2025-04-24T09:34:41.162401Z","shell.execute_reply":"2025-04-24T09:34:41.175763Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"\nclass EpisodicMemoryCell(nn.Module):\n    def __init__(self, episodic_memory_dim, input_dim, num_slots=100, comp_ratio=0.25, memory_compression_ratio:int =0.5 , temp=0.1 , apply_compression= True ):\n        super().__init__()\n        self.num_slots_dim = num_slots\n        self.temp = temp\n        self.episodic_memory_dim =  episodic_memory_dim\n        self.comp_dim = int(episodic_memory_dim * comp_ratio)\n        self.memory_compression_dim = int(episodic_memory_dim * memory_compression_ratio)\n        # Learnable initial memory\n        self.register_buffer(\"access_count\", torch.zeros(num_slots))\n        self.register_buffer(\"last_accessed\", torch.zeros(num_slots))\n        self.register_buffer(\"memory_strength\", torch.ones(num_slots))\n        self.register_buffer(\"slot_importance\", torch.zeros(num_slots))\n        self.episodic_memory =  nn.Parameter(torch.randn(self.num_slots_dim , episodic_memory_dim))\n        self.num_slots = num_slots \n        self.register_buffer(\"episodic_memory_age\", torch.zeros(num_slots))\n        self.fill_count = 0 \n        self.memory_compress = nn.Sequential(\n            nn.Linear(episodic_memory_dim, self.memory_compression_dim),\n            nn.RMSNorm(self.memory_compression_dim),\n            nn.GELU(),\n            nn.Linear(self.memory_compression_dim, episodic_memory_dim)\n        )\n        self.alpha = nn.Parameter(torch.ones(1)) \n        self.compress = nn.Sequential(\n            nn.Linear(input_dim, episodic_memory_dim),\n            nn.RMSNorm(episodic_memory_dim),\n            nn.GELU(),\n            nn.Linear(episodic_memory_dim, self.comp_dim)\n        )\n        self.decompress = nn.Sequential(\n            nn.Linear(self.comp_dim, episodic_memory_dim),\n            nn.RMSNorm(episodic_memory_dim),\n            nn.GELU()\n        )\n        self.write_gate = nn.Sequential(\n            nn.Linear(self.comp_dim+ episodic_memory_dim, 1),\n            nn.Sigmoid()\n        )\n        self.apply_flag =  apply_compression \n        self.memory_projection = nn.Linear(self.episodic_memory_dim , input_dim)\n        self.W4 = nn.Parameter(torch.randn( episodic_memory_dim , self.comp_dim))\n\n    def forward(self, x: torch.Tensor, prev_memory: torch.Tensor = None, memory_recall:bool =  True):\n        bs, s, _ = x.shape\n        compressed_x = self.compress(x)\n\n        if prev_memory is None:\n            prev_memory , prev_memory_attn_weights  = self.retrive_prev_experience(x=x)\n\n        cell_state = torch.sigmoid(prev_memory + F.linear(compressed_x, self.W4))\n\n        cell_state = F.layer_norm(cell_state, (self.episodic_memory_dim,))\n\n\n        mem_exp = self.episodic_memory.unsqueeze(0).expand(bs, -1, -1)\n\n    \n        comp_exp = compressed_x.unsqueeze(1).expand(-1, self.num_slots, -1, -1).mean(dim=2) \n        gate_input = torch.cat([mem_exp, comp_exp], dim=-1)  \n\n        gate = self.write_gate(gate_input).squeeze(-1)\n        with torch.no_grad():\n            self.episodic_memory_age.data = 0.9 * self.episodic_memory_age + 0.1 *  torch.ones_like(self.episodic_memory_age)\n        decay = torch.exp(-self.episodic_memory_age / self.temp).unsqueeze(0)\n        write_weights = F.softmax((gate * decay) / self.temp, dim=-1)\n        cell_state_mean = cell_state.mean(dim=1)\n\n        with torch.no_grad():\n            memory_target =  torch.einsum(\n\n                'bs,bd->sd' , write_weights , cell_state_mean\n            )\n           \n        delta =  memory_target - self.episodic_memory  \n        learned_gate = gate.mean(dim=0 ).unsqueeze(-1)\n        update_memory=  self.episodic_memory +self.alpha + learned_gate * delta\n\n        with torch.no_grad():\n            # Replace in-place update with cloned copy\n            new_memory = self.episodic_memory.data.clone()\n            new_memory =new_memory +  self.alpha + learned_gate * delta\n            self.episodic_memory.data.copy_(new_memory)\n        return   self.memory_projection(prev_memory)\n    def retrive_prev_experience(self, x:torch.Tensor):\n        compress_x =  self.compress(x)\n        decompress_x =  self.decompress(compress_x)\n        query  =  F.normalize(decompress_x, dim=-1)\n\n        memory = F.normalize(self.episodic_memory , dim=-1)\n        attn_scores = torch.matmul(query,memory.T) / self.temp \n\n        attn_weights =  F.softmax(attn_scores , dim=-1)        \n        \n        attended_memory = torch.matmul(attn_weights, memory)  # (B, S, D)\n        return attended_memory , attn_weights \n\n\n    def get_memory(self):\n        return self.episodic_memory\n    \n    def is_full(self):\n        return self.fill_count >= self.num_slots\n \n    def replay(self, top_k=5, compress=True):\n        \n        # Get top-k important memory indices (e.g., based on slot importance or access_count)\n        importance_scores = self.slot_importance if hasattr(self, \"slot_importance\") else self.access_count\n        topk_indices = torch.topk(importance_scores, k=top_k, largest=True).indices\n    \n        # Select memories\n        selected_memories = self.episodic_memory[topk_indices]  # Shape: (top_k, episodic_memory_dim)\n    \n        # Optionally compress replayed memories\n        if compress:\n            compressed = self.compress(selected_memories.unsqueeze(0))  # Add batch dim\n            return compressed.squeeze(0)  # Shape: (top_k, compressed_dim)\n        else:\n            return selected_memories  # Shape: (top_k, episodic_memory_dim)\n\n\n\n\n    def memory_regularization_loss(self, lambda_reg=0.01):\n        \"\"\" Apply regularization on memory to prevent overfitting. \"\"\"\n        norm_loss = torch.mean(torch.norm(self.episodic_memory, p=2))  # Regularization on key memory\n        return lambda_reg * norm_loss\n    def _update_memory_metrics(self, write_weights, attn_weights=None):\n        \"\"\"Update memory usage statistics\"\"\"\n        # Track write operations\n        self.access_count += write_weights.sum(dim=0)\n        self.last_accessed += 1\n        self.last_accessed[write_weights.sum(dim=0) > 0] = 0  # Reset counter for accessed slots\n        \n        # Update memory strength (exponential decay)\n        self.memory_strength = 0.9 * self.memory_strength + 0.1 * write_weights.mean(dim=0)\n        \n        # Track read operations if available\n        if attn_weights is not None:\n            read_counts = attn_weights.sum(dim=[0,1])  # Sum across batch and sequence\n            self.access_count += read_counts\n            self.slot_importance = 0.95 * self.slot_importance + 0.05 * (write_weights + read_counts).mean(dim=0)\n\n    def get_memory_metrics(self):\n        \"\"\"Return comprehensive memory statistics\"\"\"\n        return {\n            # Basic stats\n            'total_accesses': self.access_count.sum().item(),\n            'mean_access_count': self.access_count.float().mean().item(),\n            'most_used_slot': self.access_count.argmax().item(),\n            'least_used_slot': self.access_count.argmin().item(),\n            \n            # Temporal stats\n            'mean_age': self.episodic_memory_age.float().mean().item(),\n            'oldest_slot': self.episodic_memory_age.argmax().item(),\n            \n            # Memory quality metrics\n            'mean_memory_strength': self.memory_strength.mean().item(),\n            'importance_distribution': self.slot_importance.detach(),\n            'active_slots': int((self.memory_strength > 0).sum().item()) ,\n            # Diversity metrics\n            'memory_similarity': self._calculate_memory_similarity(),\n            'unique_memories': self._calculate_unique_memories()\n        }\n\n    def _calculate_memory_similarity(self):\n        \"\"\"Calculate cosine similarity between memory slots\"\"\"\n        with torch.no_grad():\n            norms = F.normalize(self.episodic_memory, p=2, dim=-1)\n            similarity = torch.mm(norms, norms.T)\n            return {\n                'mean_similarity': similarity.mean().item(),\n                'max_similarity': similarity.max().item(),\n                'min_similarity': similarity.min().item()\n            }\n\n    def _calculate_unique_memories(self, threshold=0.8):\n        \"\"\"Count unique memories based on similarity threshold\"\"\"\n        with torch.no_grad():\n            norms = F.normalize(self.episodic_memory, p=2, dim=-1)\n            similarity = torch.mm(norms, norms.T)\n            unique_count = (similarity < threshold).sum(dim=1).gt(0).sum()\n            return unique_count.item()\n\n    def memory_health_check(self):\n        \"\"\"Diagnostic check for memory system\"\"\"\n        metrics = self.get_memory_metrics()\n        print(metrics)\n        return {\n            'potential_issues': [\n                (\"High similarity\", metrics['memory_similarity']['mean_similarity'] > 0.75),\n                (\"Low uniqueness\", metrics['unique_memories'] < self.num_slots//2),\n                (\"Memory leakage\", metrics['total_accesses'] > 1e6)\n            ],\n            'recommended_actions': [\n                (\"Consolidate memories\", metrics['memory_similarity']['mean_similarity'] > 0.75),\n                (\"Increase diversity\", metrics['unique_memories'] < self.num_slots//3),\n                (\"Purge old memories\", metrics['mean_age'] > 1000)\n            ]\n        }\n\n    def reset_usage_stats(self):\n        \"\"\"Reset transient statistics while preserving memories\"\"\"\n        self.access_count.fill_(0)\n        self.last_accessed.fill_(0)\n        self.slot_importance.fill_(0)\n    def prune_infrequent(self , access_threshold:int = 10):\n        infrequent_mask  = self.access_count  < access_threshold \n        self.episodic_memory[infrequent_mask] = 0\n        self.episodic_memory_age +=1 \n        self.access_count[infrequent_mask] = 0\n\n    \n    def episodic_loss(self, lambda_sparsity=0.1, lambda_stability=0.5):\n        \"\"\"\n        Combines:\n        - Memory regularization\n        - Access sparsity\n        - Temporal stability\n        - Slot diversity\n        \"\"\"\n        # Base regularization (existing)\n        reg_loss = self.memory_regularization_loss()\n        \n        # Access pattern sparsity (encourage efficient slot usage)\n        access_prob = F.softmax(self.access_count.float(), dim=0)\n        sparsity_loss = lambda_sparsity * torch.sum(access_prob * torch.log(access_prob + 1e-8))\n        \n        # Temporal stability (prevent catastrophic forgetting)\n        age_weight = torch.sigmoid(-self.episodic_memory_age/self.max_age)\n        stability_loss = lambda_stability * torch.mean(\n            age_weight * torch.norm(self.episodic_memory - self.episodic_memory.detach(), dim=-1)\n        )\n        \n        # Slot diversity (anti-collapse)\n        similarity = F.cosine_similarity(\n            self.episodic_memory.unsqueeze(1),\n            self.episodic_memory.unsqueeze(0),\n            dim=-1\n        )\n        diversity_loss = torch.mean(torch.triu(similarity, diagonal=1)**2)\n        \n        return reg_loss + sparsity_loss + stability_loss + diversity_loss\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:41.250655Z","iopub.execute_input":"2025-04-24T09:34:41.250846Z","iopub.status.idle":"2025-04-24T09:34:41.273574Z","shell.execute_reply.started":"2025-04-24T09:34:41.250830Z","shell.execute_reply":"2025-04-24T09:34:41.272828Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\nclass BioSemanticMemoryCell(nn.Module):\n    def __init__(self, semantic_memory_dim: int, input_dim: int, num_concepts: int = 1000,\n                 comp_ratio: float = 0.25, temp: float = 0.1, top_k: int = 5, expand_rate: float = 0.1):\n        super().__init__()\n        self.semantic_memory_dim = semantic_memory_dim\n        self.input_dim = input_dim\n        self.num_concepts = num_concepts\n        self.temp = temp\n        self.top_k = top_k\n        self.expand_rate = expand_rate\n        self.comp_dim = int(semantic_memory_dim * comp_ratio)\n        self.register_buffer('memory_age', torch.zeros(num_concepts))\n        self.register_buffer('access_count', torch.zeros(num_concepts))\n        self.register_buffer('concept_energy', torch.ones(num_concepts))\n        self.energy_threshold = nn.Parameter(torch.tensor(0.1))\n        self.max_age = 1000\n        self.age_embedding = nn.Embedding(self.max_age, semantic_memory_dim)\n        \n        # Memory importance estimation\n        self.importance_net = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        \n        # Adaptive forgetting parameters\n        self.decay_rate = nn.Parameter(torch.tensor(0.9))\n        self.consolidation_threshold = 0.7\n        \n        # Pre-allocate memory with buffer\n        self.key_memory  =  nn.Parameter(torch.randn(num_concepts , semantic_memory_dim))\n        self.value_memory  =  nn.Parameter(torch.randn(num_concepts , semantic_memory_dim))\n        self.cell_state  =  nn.Parameter(torch.randn(num_concepts , semantic_memory_dim))\n        nn.init.xavier_normal_(self.key_memory)\n        nn.init.xavier_normal_(self.value_memory)\n        nn.init.xavier_normal_(self.cell_state)  # Initialize cell_state similarly\n\n\n        # Compression network with optimized layers\n        self.compression = nn.Sequential(\n            nn.Linear(input_dim, semantic_memory_dim),\n            nn.LayerNorm(semantic_memory_dim),\n            nn.GELU(),\n            nn.Linear(semantic_memory_dim, self.comp_dim),\n            nn.Tanh()\n        )\n\n        # Vectorized gating mechanisms\n        self.update_gate = nn.Sequential(\n            nn.Linear(3 * semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        self.age = nn.Parameter(torch.zeros(num_concepts, dtype=torch.long), requires_grad=False)\n        self.memory_projection = nn.Linear(self.semantic_memory_dim, self.input_dim)\n\n        # Shared forget gate parameters\n        self.forget_gate = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 3),  # 3 outputs for cell/key/value\n            nn.Sigmoid()\n        )\n\n        # Projection with pre-allocated weights\n        self.W_cell = nn.Linear(self.comp_dim, semantic_memory_dim, bias=False)\n        self.synaptic_scale = nn.Parameter(torch.tensor(0.1))\n\n        # Memory management\n        self.sparsity = nn.Parameter(torch.tensor(0.5))\n        self.register_buffer('usage', torch.zeros(num_concepts))\n        # self.register_buffer('active_concepts', torch.tensor(num_concepts))\n\n\n    def forward(self, x: torch.Tensor, training=False):\n        bs , seq_len , _ =  x.shape    \n        compressed = self.compression(x.mean(dim=1))\n        projected = self.W_cell(compressed)\n        self.active_concepts = int(self.num_concepts * self.sparsity)\n\n        # Compute similarity scores\n        age_idx = self.memory_age[:self.active_concepts].clamp(0, self.max_age - 1).long()\n        age_embed = self.age_embedding(age_idx)  # [C, D]\n\n        # Add age signal to key\n        age_enriched_key = self.key_memory[:self.active_concepts] + age_embed[:self.active_concepts]\n        \n        # Dot similarity with age-aware key\n        sim = torch.mm(projected, age_enriched_key.T) / self.temp\n        \n        # Top-K selection\n        topk_scores, topk_idx = torch.topk(sim, self.top_k, dim=1)\n        self._update_energy_levels()\n        \n        # Memory interaction\n        batch_indices = torch.arange(x.size(0), device=x.device)[:, None]\n        keys = self.key_memory[topk_idx]\n        values = self.value_memory[topk_idx]\n        cells = self.cell_state[topk_idx]\n        self._update_memory_metadata(topk_idx)\n        self.usage[topk_idx] = (self.usage[topk_idx] + 1).clamp(max=1)\n\n        # Vectorized gating\n        gate_input = torch.cat([\n            keys,\n            cells,\n            projected.unsqueeze(1).expand(-1, self.top_k, -1)\n        ], dim=-1)\n        update_gates = self.update_gate(gate_input.view(-1, 3 * self.semantic_memory_dim))\n        update_gates = update_gates.view(-1, self.top_k)\n\n        # Efficient cell updates\n        decay_factor = self._adaptive_decay(topk_idx)\n        decay_factor = decay_factor.unsqueeze(-1)  # Shape: [16, 5, 1]\n  \n        cell_updates = decay_factor * torch.sigmoid(cells + projected.unsqueeze(1))\n        delta = update_gates.unsqueeze(-1) * cell_updates\n        \n        if self.training:\n            self._consolidate_important_memories()\n\n        with torch.no_grad():\n            # Replace in-place updates with cloned copies\n            new_cell = self.cell_state.data.clone()\n            new_key = self.key_memory.data.clone()\n            new_value = self.value_memory.data.clone()\n            \n            # Update clones instead of original parameters\n            new_cell[topk_idx] += (delta * self.synaptic_scale).mean(0)\n            new_key[topk_idx] += F.normalize(cells, dim=-1)\n            new_value[topk_idx] += F.normalize(cells, dim=-1)\n            \n            # Copy back to parameters\n            self.cell_state.data.copy_(new_cell)\n            self.key_memory.data.copy_(new_key)\n            self.value_memory.data.copy_(new_value)\n            \n        # Normalize using out-of-place operations\n        self.key_memory.data[topk_idx] = F.normalize(self.key_memory.data[topk_idx], dim=-1)\n        self.value_memory.data[topk_idx] = F.normalize(self.value_memory.data[topk_idx], dim=-1)\n        # Output with fused operations\n        output = (values * F.softmax(topk_scores, dim=1).unsqueeze(-1)).sum(1)\n\n        # Conditional expansion\n        \n        if self.training:\n            self._consolidate_important_memories()\n        if  self._needs_expansion():\n            self._expand_memory()\n        output = output.unsqueeze(1)\n        output = output.repeat(1, seq_len , 1)\n        output = self.memory_projection(output)\n        self.age += 1\n        return output\n\n    def _consolidate_important_memories(self):\n        consolidation_factor = torch.where(self.concept_energy > self.consolidation_threshold,\n                                           torch.tensor(1.2),\n                                           torch.tensor(1.0))\n        self.key_memory *= consolidation_factor.unsqueeze(-1)\n        self.value_memory *= consolidation_factor.unsqueeze(-1)\n    def _adaptive_decay(self, memory_idx):\n        # Calculate the adaptive decay based on importance (energy and access count)\n        energy = self.concept_energy[memory_idx]\n        access = self.access_count[memory_idx]\n        \n        # The decay is stronger for less accessed and less energetic memories\n        decay = torch.exp(-self.decay_rate * (1 - energy) * (1 - access))\n        \n        return decay\n    def _update_energy_levels(self):\n        \"\"\"Energy-based memory retention\"\"\"\n        # Calculate memory importance using the importance network\n        importance = self.importance_net(self.key_memory).squeeze()  # [batch_size, num_concepts]\n\n        # Energy decay with importance factor and previous energy levels\n        self.concept_energy = torch.clamp(\n            self.concept_energy * self.decay_rate + importance * 0.1,  # Decay + importance influence\n            0, 1  # Bound energy between 0 and 1\n        )\n\n        # Identify low-energy memories\n        deactivated = self.concept_energy < 0.1  # Memory slots with energy below threshold\n\n        # Update active concepts count (deactivate low-energy ones)\n        self.active_concepts = self.active_concepts - deactivated.sum()\n\n        # Update memory values in-place using .data (avoid reassigning nn.Parameter directly)\n        self.key_memory.data[deactivated] = 0\n        self.value_memory.data[deactivated] = 0\n        self.cell_state.data[deactivated] = 0\n\n        # Optionally, if you want to keep track of the inactive ones for future use:\n        self.inactive_concepts = deactivated.sum()\n\n\n    def purge_inactive_memories(self):\n        # Use registered buffers properly\n        active_mask = self.usage > self.threshold\n        self.key_memory = self.key_memory[active_mask]\n        self.value_memory = self.value_memory[active_mask]\n        self.cell_state = self.cell_state[active_mask]\n        self.active_concepts = active_mask.sum()\n        def _expand_memory(self):\n            \"\"\"Efficient memory expansion with pre-allocation\"\"\"\n            new_size = int(self.num_concepts * (1 + self.expand_rate))\n            if new_size > self.key_memory.size(0):\n                # Double memory allocation for amortized O(1) expansion\n                new_size = max(new_size, self.key_memory.size(0) * 2)\n                self._reallocate_memory(new_size)\n            \n            self.active_concepts = int(self.active_concepts * (1 + self.expand_rate))\n\n    def _reallocate_memory(self, new_size):\n        \"\"\"Efficient memory reallocation\"\"\"\n        new_key = torch.empty(new_size, self.semantic_memory_dim, device=self.key_memory.device)\n        new_value = torch.empty_like(new_key)\n        new_cell = torch.zeros_like(new_key)\n        \n        new_key[:self.key_memory.size(0)] = self.key_memory\n        new_value[:self.value_memory.size(0)] = self.value_memory\n        new_cell[:self.cell_state.size(0)] = self.cell_state\n        \n        self.key_memory = new_key\n        self.value_memory = new_value\n        self.cell_state = new_cell\n\n    def _needs_expansion(self):\n        \"\"\"Efficient expansion check\"\"\"\n        return (self.usage[:self.active_concepts] > 0.8).float().mean() > 0.5\n\n    def get_memory(self):\n        return self.key_memory[:self.active_concepts], \\\n               self.value_memory[:self.active_concepts], \\\n               self.cell_state[:self.active_concepts]\n\n    def _update_memory_metadata(self, used_indices):\n        \"\"\"Track memory usage patterns\"\"\"\n        # Update access statistics\n        self.access_count[used_indices] += 1\n        \n        # Update memory age (non-accessed memories age faster)\n        self.memory_age += 1\n        self.memory_age[used_indices] *= 0.5\n        \n        # Update energy levels\n        self.concept_energy[used_indices] += 0.1\n        self.concept_energy = torch.clamp(self.concept_energy * 0.95, 0, 1)\n    def _consolidate_important_memories(self):\n        \"\"\"Memory consolidation mechanism\"\"\"\n        # Find important memories\n        importance = self.importance_net(self.key_memory).squeeze()\n        consolidate_mask = importance > self.consolidation_threshold\n        \n        # Stabilize important memories\n        if consolidate_mask.any():\n            self.key_memory.data[consolidate_mask] = F.normalize(\n                self.key_memory[consolidate_mask], dim=-1\n            )\n            self.value_memory.data[consolidate_mask] = (\n                0.9 * self.value_memory[consolidate_mask] + \n                0.1 * self.value_memory[consolidate_mask].mean(dim=0)\n            )\n    def get_memory_metrics(self):\n        \"\"\"Return memory health statistics\"\"\"\n        return {\n            'active_concepts': self.active_concepts.item(),\n            'energy_mean': self.concept_energy.mean().item(),\n            'access_count': self.access_count.sum().item(),\n            'age_distribution': torch.histc(self.memory_age, bins=10),\n            'importance_distribution': self.importance_net(self.key_memory).detach()\n        }\n    \n    def replay(self, n_samples):\n        \"\"\"Periodically replay important memories\"\"\"\n        idx = torch.topk(self.usage, n_samples).indices\n        return self.key_memory[idx], self.value_memory[idx]\n\n\n    def semantic_loss(self, lambda_energy=0.3, lambda_semantic=0.2):\n        \"\"\"\n        Combines:\n        - Energy maintenance\n        - Semantic coherence\n        - Importance sparsity\n        - Adaptive consolidation\n        \"\"\"\n        # Energy preservation (prevent premature forgetting)\n        energy_loss = lambda_energy * torch.mean(\n            (self.concept_energy - self.energy_threshold).clamp(min=0)**2\n        )\n        \n        # Semantic coherence (cluster similar concepts)\n        key_norm = F.normalize(self.key_memory, dim=-1)\n        value_norm = F.normalize(self.value_memory, dim=-1)\n        alignment_loss = torch.mean(1 - (key_norm * value_norm).sum(dim=-1))\n        \n        # Importance sparsity (focus on key concepts)\n        importance = self.importance_net(self.key_memory).squeeze()\n        sparsity_loss = torch.mean(importance**2)\n        \n        # Consolidation stability\n        consolidated = (self.concept_energy > self.consolidation_threshold).float()\n        consolidation_loss = torch.mean(\n            consolidated * torch.norm(self.key_memory - self.key_memory.detach(), dim=-1)\n        )\n        \n        return energy_loss + alignment_loss + sparsity_loss + consolidation_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:41.348589Z","iopub.execute_input":"2025-04-24T09:34:41.348775Z","iopub.status.idle":"2025-04-24T09:34:41.374777Z","shell.execute_reply.started":"2025-04-24T09:34:41.348759Z","shell.execute_reply":"2025-04-24T09:34:41.374116Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class MemorySystem(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        \n        # Initialize memory components\n        self.memory_writer = MemorySummaryWriter(\n            hidden_dim=cfg['emb_dim'],\n            compress_dim=cfg['compress_dim'],\n            num_heads=cfg['memory_router_head']\n        )\n        self.memory_router = MemoryRouter(\n            input_dim=cfg['emb_dim'],\n            temperature=cfg['temperature'],\n            dropout=cfg['drop_rate']\n        )\n        \n        # Memory subsystems\n        self.episodic_memory = EpisodicMemoryCell(\n            episodic_memory_dim=cfg['episodic_memory_dim'],\n            input_dim=cfg['emb_dim'],\n            num_slots=cfg['num_slots'],\n            comp_ratio=cfg['comp_ratio'],\n            memory_compression_ratio=cfg['memory_compression_ratio'],\n            temp=cfg['epic_temp'],\n            apply_compression=cfg['apply_compression']\n        )\n        \n        self.semantic_memory = BioSemanticMemoryCell(\n            semantic_memory_dim=cfg['semantic_memory_dim'],\n            input_dim=cfg['emb_dim'],\n            num_concepts=cfg['num_concepts'],\n            comp_ratio=cfg['semantic_comp_ratio'],\n            temp=cfg['semantic_temp'],\n            top_k=cfg['top_k'],\n            expand_rate=cfg['expand_rate']\n        )\n        self.usage_history = []\n        self.overflow_threshold = cfg.get('overflow_threshold', 0.9)\n        # Adaptive gating\n        self.gate_norm = nn.LayerNorm(cfg['emb_dim'])\n        self.temperature = nn.Parameter(torch.tensor(1.0))\n        self.output_projection = nn.Linear(cfg['compress_dim'], cfg['emb_dim'])\n        # In MemorySystem __init__:\n    def forward(self, x: torch.Tensor):\n        bs, seq_len, dim = x.shape\n        \n        # Memory routing\n        router_weights, meta_data = self.memory_router(x)\n        router_weights = F.softmax(router_weights / (self.temperature + 1e-6), dim=-1)\n        \n        # Memory writing\n        summary_out, _ = self.memory_writer(x)\n        summary_out = self.output_projection(summary_out)\n        summary_out = summary_out.unsqueeze(1).expand(-1, seq_len, -1)\n        \n        # Memory retrieval\n        episodic_out = self.episodic_memory(summary_out)\n        semantic_out = self.semantic_memory(summary_out)\n        \n        # Memory consolidation check\n        if self.episodic_memory.is_full():\n            self._consolidate_memory()\n            \n        # Adaptive memory fusion\n        episodic_gate = router_weights[..., 0].unsqueeze(-1)\n        semantic_gate = router_weights[..., 1].unsqueeze(-1)\n        \n        combined = self.gate_norm(\n            episodic_gate * episodic_out + \n            semantic_gate * semantic_out\n        )\n        current_usage = self.episodic_memory.get_memory_metrics()['active_slots'] / self.cfg['num_slots']\n        if current_usage > self.overflow_threshold:\n            self._adaptive_consolidation()\n        \n        return semantic_out , episodic_out , combined\n\n    def _consolidate_memory(self):\n        \"\"\"Transfer knowledge from episodic to semantic memory\"\"\"\n        with torch.no_grad():\n            episodic_data = self.episodic_memory.replay(\n                top_k=self.cfg['consolidation_top_k'],\n                compress=True\n            )\n            self.semantic_memory(episodic_data.unsqueeze(1))\n    def _adaptive_consolidation(self):\n            \"\"\"Smart consolidation based on memory content\"\"\"\n            # Preserve important memories\n            importance = self.episodic_memory.slot_importance\n            preserve_mask = importance > torch.quantile(importance, 0.8)\n            \n            # Consolidate less important memories\n            consolidate_data = self.episodic_memory.replay(\n                top_k=int(self.cfg['num_slots'] * 0.2),\n                compress=True\n            )\n            self.semantic_memory(consolidate_data.unsqueeze(1))\n            \n            # Reset consolidated slots\n            self.episodic_memory.prune_infrequent(\n                access_threshold=self.cfg['episodic_prune_thresh'] * 2\n            )\n    def maintain_memory(self):\n        \"\"\"Perform routine memory maintenance\"\"\"\n        # Episodic maintenance\n        self.episodic_memory.prune_infrequent(\n            access_threshold=self.cfg['episodic_prune_thresh']\n        )\n        \n        # Semantic maintenance\n        self.semantic_memory.purge_inactive_memories()\n        \n        return {\n            'episodic': self.episodic_memory.get_memory_metrics(),\n            'semantic': self.semantic_memory.get_memory_metrics()\n        }\n\n    def memory_regularization_loss(self):\n        \"\"\"Combine memory regularization terms\"\"\"\n        return (\n            self.cfg['episodic_reg_weight'] * self.episodic_memory.memory_regularization_loss() +\n            self.cfg['semantic_reg_weight'] * self.semantic_memory.memory_regularization_loss()\n            \n        )\n    \n    def memory_loss(self, \n                       episodic_weight=1.0, \n                       semantic_weight=0.7,\n                       temp_scale=0.1):\n            \"\"\"\n            Unified memory loss with temperature-scaled balancing\n            \"\"\"\n            # Component losses\n            episodic_loss = self.episodic_memory.episodic_loss()\n            semantic_loss = self.semantic_memory.semantic_loss()\n            \n            # Adaptive temperature scaling\n            t = torch.sigmoid(self.temperature) * temp_scale\n            balance = torch.softmax(torch.stack([episodic_loss/t, semantic_loss/t]), dim=0)\n            \n            # Final weighted loss\n            return (\n                episodic_weight * balance[0] * episodic_loss +\n                semantic_weight * balance[1] * semantic_loss\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:40:57.786580Z","iopub.execute_input":"2025-04-24T09:40:57.786924Z","iopub.status.idle":"2025-04-24T09:40:57.799846Z","shell.execute_reply.started":"2025-04-24T09:40:57.786898Z","shell.execute_reply":"2025-04-24T09:40:57.799070Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# Transformer Block","metadata":{}},{"cell_type":"code","source":"\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention_V2(\n        d_in=cfg[\"emb_dim\"],\n        d_out=cfg[\"emb_dim\"],\n        context_length=cfg[\"context_length\"],\n        num_heads=cfg[\"n_heads\"],\n        dropout=cfg[\"drop_rate\"],\n        qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n    #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut # Add the original input back\n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut #C\n        return x\n\n\n\nclass TransformerBlock_v2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention =  MultiQueryAttentionBlock(d_model=cfg['emb_dim'], h=cfg['n_heads'] , dropout=cfg['drop_rate'], seq_len=  cfg['context_length'] ,qkv_bias=cfg['qkv_bias'])\n\n        self.feed_forward = FeedForward(cfg)\n\n        self.layernorm1 =  LayerNorm(cfg['emb_dim'])\n    \n        self.layernorm2 =  LayerNorm(cfg['emb_dim'])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x , mask= None):\n\n        attention_output =  self.attention(self.layernorm1(x) , mask =  mask)\n\n        ff_output =  self.feed_forward(self.layernorm2(x))\n\n        return x + self.drop_out(ff_output) + self.drop_out(attention_output)\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.memory = MemorySystem(cfg=cfg)\n        self.feed_forward = FeedForward(cfg=cfg)\n        \n        self.norm1 = LayerNorm(cfg['emb_dim'])\n        self.norm2 = LayerNorm(cfg['emb_dim'])\n        self.norm3 = LayerNorm(cfg['emb_dim'])\n        \n        # Memory gate\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] * 2, cfg['emb_dim']),\n            nn.Sigmoid()\n        )\n        \n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        attn_out = self.attention(self.norm1(x), mask=mask)\n        x = x + self.dropout(attn_out)\n        \n        norm_x = self.norm2(x)\n        epic_out , semantic_out , memory_out = self.memory(norm_x)\n        \n        gate_input = torch.cat([norm_x, memory_out], dim=-1)\n        memory_gate = self.memory_gate(gate_input)\n        x = x + memory_gate * memory_out\n        \n        ff_out = self.feed_forward(self.norm3(x))\n        x = x + self.dropout(ff_out)\n        \n        return x\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg, shared_memory=None):\n        super().__init__()\n        # Core components\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.ffn = FeedForward(cfg=cfg)\n        \n        # Memory system (shared across blocks)\n        self.memory = shared_memory or MemorySystem(cfg=cfg)\n        \n        # Normalization layers\n        self.pre_ln_attn = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_mem = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_ffn = RMSNorm(cfg['emb_dim'])\n        \n        # Adaptive memory gating\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'], 1),\n            nn.Sigmoid()\n        )\n        \n        # Memory residual weights\n        self.mem_alpha = nn.Parameter(torch.tensor(0.5))\n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        # Attention phase\n        resid = x\n        x = self.pre_ln_attn(x)\n        x = resid + self.dropout(self.attention(x, mask=mask))\n        \n        # Memory phase\n        resid_mem = x\n        x_mem = self.pre_ln_mem(x)\n        _, _, memory_out = self.memory(x_mem)\n        \n        # Adaptive gating\n        gate = self.memory_gate(x_mem)\n        x = resid_mem + self.mem_alpha * gate * memory_out\n        \n        # FFN phase\n        resid_ffn = x\n        x = self.pre_ln_ffn(x)\n        x = resid_ffn + self.dropout(self.ffn(x))\n        \n        return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:41:00.691707Z","iopub.execute_input":"2025-04-24T09:41:00.691990Z","iopub.status.idle":"2025-04-24T09:41:00.707428Z","shell.execute_reply.started":"2025-04-24T09:41:00.691969Z","shell.execute_reply":"2025-04-24T09:41:00.706233Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# GPTQModel","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int, embdding_layer: nn.Embedding):\n        super().__init__()\n        self.weight = embdding_layer.weight  # share weights with input embedding\n        self.bias = nn.Parameter(torch.zeros(vocab_size))  # learnable bias\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\n\n\n\nclass GPTMQModel2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim = cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks =  nn.ModuleList([\n            TransformerBlockWithMemory(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n        \n        # Shared memory system across layers\n        self.shared_memory = MemorySystem(cfg=cfg)\n        \n        # Transformer blocks with shared memory\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlockWithMemory(\n                cfg=cfg,\n                shared_memory=self.shared_memory if cfg['share_memory'] else None\n            ) for _ in range(cfg['n_layers'])\n        ])\n        \n        # Final projections\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.projection = ProjectionLayer(\n            cfg['emb_dim'], \n            cfg['vocab_size'], \n            self.embedding.embeddings\n        )\n        self.memory_retention_alpha = nn.Parameter(torch.tensor(0.9))\n\n        # Memory loss coefficient\n        self.mem_loss_coef = cfg.get('mem_loss_coef', 0.3)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n        \n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)\n            x = self.memory_retention_alpha * x + (1 - self.memory_retention_alpha) * x.detach()\n            \n        x = self.final_norm(x)\n        logits = self.projection(x)\n        \n        return logits\n    \n    def get_memory_loss(self):\n        \"\"\"Get combined memory regularization loss\"\"\"\n        return self.mem_loss_coef * self.shared_memory.memory_loss()\n    \n    def transformer_parameters(self):\n        return [p for n, p in self.named_parameters() if 'transformer_blocks' in n and p.requires_grad]\n    \n    def memory_parameters(self):\n        return [p for n, p in self.named_parameters() if 'memory_modules' in n and p.requires_grad]\n    \n    def embedding_parameters(self):\n        return [p for n, p in self.named_parameters() if 'embedding' in n and p.requires_grad]\n    \n    def norm_parameters(self):\n        return [p for n, p in self.named_parameters() if 'normalization' in n and p.requires_grad]\n    \n    def output_parameters(self):\n        return [p for n, p in self.named_parameters() if 'output_projection' in n and p.requires_grad]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:50:07.816869Z","iopub.execute_input":"2025-04-24T09:50:07.817216Z","iopub.status.idle":"2025-04-24T09:50:07.832716Z","shell.execute_reply.started":"2025-04-24T09:50:07.817187Z","shell.execute_reply":"2025-04-24T09:50:07.831792Z"}},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"\n\n\n\ndef cal_loss_batch(input_batch , target_batch , model:torch.nn.Module , device:torch.device):\n    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader , model , device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:41:05.051547Z","iopub.execute_input":"2025-04-24T09:41:05.051858Z","iopub.status.idle":"2025-04-24T09:41:05.057705Z","shell.execute_reply.started":"2025-04-24T09:41:05.051834Z","shell.execute_reply":"2025-04-24T09:41:05.056653Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Text Generation Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\n\n\n\ndef text_to_token_ids(text,  tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\n\ndef token_ids_to_text(tokens , tokenizer):\n    flat  = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n    \ndef generate_and_sample(model  , idx , context_size ,max_new_tokens ):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n            print(logits.shape)\n        logits  = logits[:, -1  , :]\n        print(logits.shape)\n        probs  = torch.softmax(logits  , dim=-1)\n        print(probs)\n        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n        print(idx_next)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx \n\n# def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n#     for _ in range(max_new_tokens):\n#         idx_cond = idx[:, -context_size:]  # shape: [1, current_seq_len]\n\n#         # Create causal mask dynamically\n#         seq_len = idx_cond.size(1)\n#         causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n#         causal_mask = causal_mask.unsqueeze(0)  # [1, seq_len, seq_len]\n\n#         with torch.no_grad():\n#             logits = model(idx_cond, mask=causal_mask)  # <--- pass mask here\n\n#         logits = logits[:, -1, :]  # only take the last token logits\n\n#         # Apply top-k sampling if needed\n#         if top_k is not None:\n#             top_logits, _ = torch.topk(logits, top_k)\n#             min_val = top_logits[:, -1]\n#             logits = torch.where(\n#                 logits < min_val,\n#                 torch.tensor(float('-inf')).to(logits.device),\n#                 logits\n#             )\n\n#         # Temperature sampling\n#         if temperature > 0.0:\n#             logits = logits / temperatureallowed_special=allowed\n#             probs = torch.softmax(logits, dim=-1)\n#             idx_next = torch.multinomial(probs, num_samples=1)\n#         else:\n#             idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n#         idx = torch.cat((idx, idx_next), dim=1)\n#     return idx\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, end_token_id=None):\n    model.eval()\n    max_seq_len = context_size\n    full_causal_mask = torch.tril(torch.ones(max_seq_len, max_seq_len)).to(idx.device).unsqueeze(0)\n\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = full_causal_mask[:, :seq_len, :seq_len]\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n        logits = logits[:, -1, :]  # Take last token logits\n\n        # Top-k sampling\n        if top_k is not None:\n            top_values, _ = torch.topk(logits, top_k)\n            threshold = top_values[:, -1].unsqueeze(1)\n            logits = logits.masked_fill(logits < threshold, float('-inf'))\n\n        logits = logits / temperature\n        logits = torch.clamp(logits, -100, 100)\n        probs = torch.softmax(logits, dim=-1)\n\n        idx_next = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        if end_token_id is not None and idx_next.item() == end_token_id:\n            break\n\n    return idx\n\n\n\n\n\ndef real_time_generation(model, initial_input, context_size, temperature, top_k=None, device=\"cpu\"):\n    # Tokenize the initial input and prepare the model context\n    idx = torch.tensor(initial_input).unsqueeze(0).to(device)  # Assuming initial_input is tokenized\n    \n    print(\"Starting real-time generation...\")\n    \n    # Start generating tokens in real-time\n    for new_token in generate(model, idx, max_new_tokens=50, context_size=context_size, temperature=temperature, top_k=top_k, device=device):\n        print(f\"Generated token: {new_token.item()}\")  # Or decode it back to a word\n        \n        # You can check for user input here and update idx with the new input\n        # For instance, wait for the user to input a prompt to append to the context\n        user_input = input(\"Enter new input (or press enter to continue generation): \")\n        \n        if user_input:\n            # Tokenize the new user input and append it to the context\n            user_input_tokens = torch.tensor(tokenize(user_input)).unsqueeze(0).to(device)\n            idx = torch.cat((idx, user_input_tokens), dim=1)  # Append the new tokens to the context\n        else:\n            # Continue generating if no new user input\n            continue\n\n# Function to tokenize input (adjust depending on your tokenizer)\ndef tokenize(text):\n    # Assuming you have a tokenizer function available\n    return [ord(c) for c in text]  # Dummy example: ord() converts char to token id\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:41:06.575559Z","iopub.execute_input":"2025-04-24T09:41:06.575942Z","iopub.status.idle":"2025-04-24T09:41:06.593049Z","shell.execute_reply.started":"2025-04-24T09:41:06.575911Z","shell.execute_reply":"2025-04-24T09:41:06.591879Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Dataset and DataLoader ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef generate_prompt(sample):\n    # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n    \n\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer =  tokenizer\n\n        all_tokens = []\n        allowed = {'<|endoftext|>'}\n        for sample in data:\n            prompt = generate_prompt(sample)\n            tokens = tokenizer.encode(prompt , allowed_special=allowed)\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n    return inputs, targets\n\ndef create_dataloader_v1(data, batch_size=4,\n    max_length=256, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n    dataset = Dataset_V1(data, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=collate_fn)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:41:08.758749Z","iopub.execute_input":"2025-04-24T09:41:08.759031Z","iopub.status.idle":"2025-04-24T09:41:08.766726Z","shell.execute_reply.started":"2025-04-24T09:41:08.759011Z","shell.execute_reply":"2025-04-24T09:41:08.765952Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# Dataset And DataLoader for Psycology Dataset ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport tiktoken\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass Dataset_v2(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n        self.input_ids = []\n\n        all_tokens = []\n        for sample in data:\n            tokens = tokenizer.encode(sample)  \n            all_tokens.extend(tokens)\n\n        # Split the tokens into chunks of size max_length with stride\n        for i in range(0, len(all_tokens) - self.max_length, self.stride):\n            input_chunk = all_tokens[i:i + self.max_length]\n            target_chunk = all_tokens[i + 1:i + self.max_length + 1]\n            self.input_ids.append((torch.tensor(input_chunk), torch.tensor(target_chunk)))\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, index):\n        return self.input_ids[index]\n\ndef collect_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0) \n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  \n    return inputs, targets\n\ndef create_dataloader_v2(data, batch_size=4, max_length=1024, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")  \n    dataset = Dataset_v2(data, tokenizer, max_length, stride) \n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=collect_fn)\n    return dataloader\n\ndef load_txt_file(filepath):\n    with open(filepath, 'r') as f:\n        text = f.read()\n    return text\n\ndef split_into_chunks(text, chunk_size=1024, overlap=200):\n\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        chunks.append(chunk)\n\n    return chunks\n\n\n\nfile =  '/kaggle/input/datasetcleaned/cleaned_books.txt'\nload_text =  load_txt_file(file)\nchunk = split_into_chunks(load_text)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-24T09:41:13.246391Z","iopub.execute_input":"2025-04-24T09:41:13.246731Z","iopub.status.idle":"2025-04-24T09:41:14.826139Z","shell.execute_reply.started":"2025-04-24T09:41:13.246703Z","shell.execute_reply":"2025-04-24T09:41:14.825459Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"# Train Script ","metadata":{}},{"cell_type":"code","source":"\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\n\n\ndef evaluate_model(model, train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,   # Increase generation length if needed\n            context_size=126,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n\n        # Trim everything before the generation\n        generated_only = decoded_text[len(start_context):].strip()\n\n        # Stop at endoftext token if present\n        end_marker = \"<|endoftext|>\"\n        if end_marker in generated_only:\n            generated_only = generated_only.split(end_marker)[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\ndef save_model_checkpoint(model, optimizer, epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch\n    }\n    torch.save(checkpoint, path.format(epoch))\ndef after_save_load():\n    checkpoint = torch.load(\"checkpoint_epoch_7.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n\n\ndef train_model(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\" Total training steps: {total_steps}\")\n    # scheduler = get_cosine_schedule_with_warmup(optimizer,\n    #                                         num_warmup_steps=500,\n    #                                         num_training_steps=total_steps)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n        save_model_checkpoint(model , optimizer , epoch+1)\n\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:41:16.726624Z","iopub.execute_input":"2025-04-24T09:41:16.726901Z","iopub.status.idle":"2025-04-24T09:41:18.571255Z","shell.execute_reply.started":"2025-04-24T09:41:16.726880Z","shell.execute_reply":"2025-04-24T09:41:18.570633Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# MemoryGPT Model training  \n","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.400181Z","iopub.status.idle":"2025-04-24T09:34:44.400583Z","shell.execute_reply":"2025-04-24T09:34:44.400420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \nfrom torch.utils.data  import Dataset , DataLoader \nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np \nfrom tqdm import tqdm \nfrom torch.optim import AdamW\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass GPTTrainingPipeline:\n    def __init__(self, model  , config ):\n\n        self.model  = model  \n\n        self.config = config \n        self.device =  config['device'] \n        self.optimizer = AdamW([\n            {'params': model.transformer_parameters(), 'lr': config['transformer_lr']},\n            {'params': model.memory_parameters(), 'lr': config['memory_lr']},\n        ])\n\n\n    \n\n        self.model.to(self.device)\n\n        total_steps = config['epochs'] * config['steps_per_epoch']\n    \n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            self.optimizer,\n            max_lr=[\n                config['transformer_lr'],\n                config['memory_lr'],\n            ],\n            total_steps=total_steps\n        )\n\n    \n   \n\n\n    class Dataset_V1(Dataset):\n        def __init__(self, data, tokenizer, max_length, stride):\n            self.max_length = max_length\n            self.input_ids = []\n            self.target_ids = []\n            self.tokenizer =  tokenizer\n\n            all_tokens = []\n            allowed = {'<|endoftext|>'}\n            for sample in data:\n                prompt = self.generate_prompt(sample)\n                tokens = tokenizer.encode(prompt , allowed_special=allowed)\n                all_tokens.extend(tokens)\n\n            for i in range(0, len(all_tokens) - max_length, stride):\n                input_chunk = all_tokens[i: i + max_length]\n                target_chunk = all_tokens[i + 1: i + max_length + 1]\n                self.input_ids.append(torch.tensor(input_chunk))\n                self.target_ids.append(torch.tensor(target_chunk))\n\n        def __len__(self):\n            return len(self.input_ids)\n\n        def __getitem__(self, idx):\n            return self.input_ids[idx], self.target_ids[idx]\n        def generate_prompt(self,sample):\n        # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n            return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n        \n    def collate_fn(self, batch):\n        inputs, targets = zip(*batch)\n        inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n        targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n        return inputs, targets\n\n    def create_dataloader_v1(self,data, batch_size=4,\n        max_length=256, stride=128, shuffle=True, drop_last=True):\n        tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n        dataset =self.Dataset_V1(data, tokenizer, max_length, stride) #B\n        dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=self.collate_fn)\n        return dataloader\n  \n\n\n    def train_epoch(self,train_loader ):\n        self.model.train()\n        total_loss = 0 \n        memory_metrics = []\n\n        for batch_idx, (inputs , targets ) in enumerate(tqdm(\n            train_loader\n        )):\n            inputs , targets =  inputs.to(self.device)  , targets.to(self.device)\n\n            self.optimizer.zero_grad()\n            logits =  self.model(inputs)\n\n            task_loss  =  torch.nn.functional.cross_entropy(\n                logits.view(-1 , logits.size(-1)) , \n                targets.view(-1),\n                ignore_index=self.tokenizer.pad_token_id)\n            memory_loss =  self.model.get_memory_loss()\n            total_batch_loss =  task_loss + memory_loss \n            total_batch_loss.backward()\n\n            clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])\n            self.optimizer.step()\n            self.scheduler.step()\n\n            if batch_idx % self.config['memory_maintenance_interval'] == 0:\n                with torch.no_grad():\n                    self.model.shared_memory.maintain_memory()\n                    metrics =  self.model.shared_memory.get_memory_metrics()\n                    memory_metrics.append(metrics)\n\n            total_loss += total_batch_loss.item()\n            del logits , task_loss , memory_loss \n            torch.cuda.empty_cache()\n\n        return {\n            'avg_loss':total_loss / len(train_loader) , \n            'memory_metric':self._aggregate_memory_metrics(memory_metrics)\n        }\n\n    def validate(self, val_loader ):\n        self.model.eval()\n        total_loss =  0 \n        memory_metrics= []\n\n        with torch.no_grad():\n            for inputs  ,targets in val_loader:\n                inputs , targets  = inputs.to(self.device) , targets.to(self.device)\n\n                logits =  self.model(inputs )\n                task_loss = torch.nn.functional.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    targets.view(-1),\n                    ignore_index=self.tokenizer.pad_token_id\n                )\n                total_loss += task_loss.item()\n                metrics =  self.model.shared_memory.get_memory_metrics()\n                memory_metrics.append(metrics)\n\n\n        return {\n            'avg_loss':total_loss / len(val_loader) , \n            'memory_metrics':self.__aggregate_memory_metrics(memory_metrics)\n        }\n    \n    def _aggregate_memory_metrics(self, metrics_list):\n        aggregated = {\n            'active_slots': np.mean([m['active_slots'] for m in metrics_list]),\n            'utilization': np.mean([m['active_slots'] / self.config['num_slots'] for m in metrics_list]),\n            'energy_mean': np.mean([m['energy_mean'] for m in metrics_list]),\n            'consolidation_rate': np.mean([m.get('consolidation_rate', 0) for m in metrics_list])\n        }\n        return aggregated\n    \n\n    def train(self, train_texts, val_texts, epochs=10):\n        train_loader = self.create_dataloader_v1(train_texts, self.config['batch_size'])\n        val_loader = self.create_dataloader_v1(val_texts, self.config['batch_size'])\n        \n        best_val_loss = float('inf')\n        \n        for epoch in range(epochs):\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            train_stats = self.train_epoch(train_loader)\n            val_stats = self.validate(val_loader)\n            \n            print(f\"Train Loss: {train_stats['avg_loss']:.4f}\")\n            print(f\"Val Loss: {val_stats['avg_loss']:.4f}\")\n            print(\"Memory Stats:\")\n            print(f\"  Active Slots: {train_stats['memory_metrics']['active_slots']:.1f}\")\n            print(f\"  Utilization: {train_stats['memory_metrics']['utilization']:.2%}\")\n            print(f\"  Avg Energy: {train_stats['memory_metrics']['energy_mean']:.2f}\")\n            \n            # Save best model\n            if val_stats['avg_loss'] < best_val_loss:\n                best_val_loss = val_stats['avg_loss']\n                self.save_checkpoint(f\"best_model.pth\")\n            \n            # Save periodic checkpoint\n            if epoch % self.config['checkpoint_interval'] == 0:\n                self.save_checkpoint(f\"checkpoint_epoch{epoch}.pth\")\n\n    def save_checkpoint(self, path):\n        torch.save({\n            'model_state': self.model.state_dict(),\n            'memory_state': self.model.shared_memory.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'scheduler': self.scheduler.state_dict(),\n            'config': self.config\n        }, path)\n\n    def load_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state'])\n        self.model.shared_memory.load_state_dict(checkpoint['memory_state'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.scheduler.load_state_dict(checkpoint['scheduler'])\n\nconfig = {\n    'emb_dim': 768,\n    'num_slots': 1000,\n    'transformer_lr': 5e-5,\n    'memory_lr': 1e-4,\n    'gate_lr': 1e-3,\n    'batch_size': 8,\n    'seq_length': 126,\n    'max_grad_norm': 1.0,\n    'memory_maintenance_interval': 50,\n    'checkpoint_interval': 2,\n    'total_steps': 10000,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n     'epochs': 5,\n    'steps_per_epoch': 1000,\n}\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:57:35.001653Z","iopub.execute_input":"2025-04-24T09:57:35.001971Z","iopub.status.idle":"2025-04-24T09:57:35.023200Z","shell.execute_reply.started":"2025-04-24T09:57:35.001949Z","shell.execute_reply":"2025-04-24T09:57:35.022337Z"}},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"# GPT Config ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nGPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,#126 \n# Context lengt\n\"emb_dim\": 768,\n# Embedding dimension\n\"n_heads\": 12,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False\n# Query-Key-Value bias\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.402810Z","iopub.status.idle":"2025-04-24T09:34:44.403177Z","shell.execute_reply":"2025-04-24T09:34:44.403022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_memory_model(\n    model: nn.Module,\n    train_dataloader: DataLoader,\n    device: torch.device,\n    eval_dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    max_grad_norm: float = 1.0,\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    memory_metrics = []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\" Total training steps: {total_steps}\")\n\n    # Create TensorBoard writer\n    # writer = SummaryWriter(log_dir='runs/memory_experiment')\n\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for batch_idx, (inputs_batch, target_batch) in enumerate(train_dataloader):\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            \n            # Forward pass with memory\n            logits = model(inputs_batch)\n            \n            # Calculate loss with memory regularization\n            loss = cal_loss_batch(inputs_batch, target_batch, model, device)\n            loss += model.memory_system.episodic_memory_cell.memory_regularization_loss()\n            loss += model.memory_system.semantic_memory_cell.memory_regularization_loss()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n\n            # Memory maintenance\n            if batch_idx % 100 == 0:\n                model.memory_system.maintain_memory()\n\n            # Logging and evaluation\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                # Evaluate and get memory metrics\n                train_loss, val_loss, mem_metrics = evaluate_memory_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n                \n                # Record metrics\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                memory_metrics.append(mem_metrics)\n                \n                # TensorBoard logging\n                writer.add_scalar('Loss/Train', train_loss, global_step)\n                writer.add_scalar('Loss/Val', val_loss, global_step)\n                log_memory_metrics(writer, mem_metrics, global_step)\n                \n                print(f\"Step {global_step}:\")\n                print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n                print_memory_health(mem_metrics)\n\n            # Generate samples with current memory state\n            if global_step % (eval_freq*2) == 0:\n                generate_with_memory(\n                    model, \n                    train_dataloader.dataset.tokenizer, \n                    device, \n                    start_context,\n                    writer,\n                    global_step\n                )\n\n        # Save model and memory state\n        save_memory_checkpoint(model, optimizer, epoch+1, global_step)\n\n    writer.close()\n    return train_losses, val_losses, track_tokens_seen, memory_metrics\n\ndef evaluate_memory_model(model, train_loader, eval_loader, device, eval_iter):\n    model.eval()\n    train_loss, val_loss = 0, 0\n    memory_stats = []\n    \n    with torch.no_grad():\n        # Evaluate on training set\n        for i, (inputs, targets) in enumerate(train_loader):\n            if i >= eval_iter: break\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            train_loss += cal_loss_batch(inputs, targets, model, device).item()\n            \n        # Evaluate on validation set\n        for i, (inputs, targets) in enumerate(eval_loader):\n            if i >= eval_iter: break\n            inputs, targets = inputs.to(device), targets.to(device)\n         \n            logits =model(inputs)\n            val_loss += cal_loss_batch(inputs, targets, model, device).item()\n            \n        # Get memory health metrics\n        mem_health = model.memory_system.maintain_memory()\n        \n    model.train()\n    return train_loss/eval_iter, val_loss/eval_iter, mem_health\n\ndef log_memory_metrics(writer, metrics, step):\n    # Episodic memory metrics\n    writer.add_scalar('Memory/Episodic/UsedSlots', \n                     metrics['episodic']['active_slots'], step)\n    writer.add_scalar('Memory/Episodic/MeanSimilarity', \n                     metrics['episodic']['memory_similarity']['mean_similarity'], step)\n    \n    # Semantic memory metrics\n    writer.add_scalar('Memory/Semantic/ActiveConcepts', \n                     metrics['semantic']['active_concepts'], step)\n    writer.add_scalar('Memory/Semantic/EnergyMean', \n                     metrics['semantic']['energy_mean'], step)\n\ndef print_memory_health(metrics):\n    print(\"Memory Health:\")\n    print(f\"  Episodic: {metrics['episodic']['active_slots']} active slots\")\n    print(f\"    Similarity: {metrics['episodic']['memory_similarity']['mean_similarity']:.3f}\")\n    print(f\"  Semantic: {metrics['semantic']['active_concepts']} concepts\")\n    print(f\"    Energy: {metrics['semantic']['energy_mean']:.3f}\")\n\ndef generate_with_memory(model, tokenizer, device, start_context, writer=None, step=None):\n    model.eval()\n    with torch.no_grad():\n        # Generate with current memory state\n        input_ids = tokenizer.encode(start_context, return_tensors='pt').to(device)\n        \n        # Generate text with memory context\n        outputs = model.generate(\n            input_ids,\n            max_length=100,\n            temperature=0.8,\n            do_sample=True,\n            memory_context=model.memory_system.get_memory_state()\n        )\n        \n        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(\"\\nGenerated Text with Memory:\")\n        print(text)\n        \n        if writer:\n            writer.add_text(\"Generated Text\", text, step)\n    \n    model.train()\n\ndef save_memory_checkpoint(model, optimizer, epoch, step):\n    state = {\n        'epoch': epoch,\n        'step': step,\n        'model_state': model.state_dict(),\n        'optimizer_state': optimizer.state_dict(),\n        'memory_system': {\n            'episodic': model.memory_system.episodic_memory_cell.get_memory(),\n            'semantic': model.memory_system.semantic_memory_cell.get_memory(),\n            'router': model.memory_system.memory_router.state_dict()\n        }\n    }\n    torch.save(state, f\"memory_checkpoint_{epoch}_{step}.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.404161Z","iopub.status.idle":"2025-04-24T09:34:44.404539Z","shell.execute_reply":"2025-04-24T09:34:44.404381Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = GPTMQModel2(GPT_CONFIG_124M)\n\n\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# model.to(device)\n# optimizer  =  torch.optim.AdamW(model.parameters() , lr = 0.0004  , weight_decay= 0.01)\n# num_epochs = 1\n# train_ratio = 0.90\n\n# text_data = chunk\n# print(len(text_data))\n# split = int(train_ratio * len(text_data))\n# print(split)\n# train_data= text_data[:split]\n# val_data = text_data[split:]\n# # train_dataloader = create_dataloader_v1(txt= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True , stride=GPT_CONFIG_124M['context_length'])\n# # val_dataloader = create_dataloader_v1(txt= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False , stride=GPT_CONFIG_124M['context_length'])\n# train_dataloader = create_dataloader_v2(data= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True )\n# val_dataloader = create_dataloader_v2(data= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False )\n# start_context = '### Instruction :Give three tips for staying healthy ### Response:'\n# print('start trainning')\n# train_losses , val_losses  , token_seen = train_model(\n#     model= model , train_dataloader= train_dataloader , \n#     eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n#     eval_iter=3 , start_context=start_context, num_epochs=2\n# )\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.405302Z","iopub.status.idle":"2025-04-24T09:34:44.405705Z","shell.execute_reply":"2025-04-24T09:34:44.405542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n        # --- GPTModel ---\n        \"vocab_size\": 50257,\n        'emb_dim': 768, \n        \"n_layers\": 12,                # Base embedding dimension\n        'n_heads': 12,\n        'drop_rate': 0.1,\n        'context_length': 126,\n        'qkv_bias': True,\n        'share_memory': True,  \n        # --- Memory Writer ---\n        'compress_dim': 64,             # Compressed embedding dimension\n        'temperature': 0.7,              # Softmax temp for sharper router weights\n        'memory_router_head': 4,        # Router head count\n\n        # --- Episodic Memory ---\n        'episodic_memory_dim': 768,     # Matches transformer emb_dim for residual\n        'num_slots': 1000,               # Number of episodic memory slots\n        'comp_ratio': 0.25,             # Controls compression granularity\n        'memory_compression_ratio': 0.5, # Compression for storage efficiency\n        'epic_temp': 0.1,                      # Matching probability threshold\n        'apply_compression': True,      # Enable latent compression (like an autoencoder)\n\n        # --- Semantic Memory ---\n        'semantic_memory_dim': 128,     # Same as emb_dim for residual compatibility\n        'num_concepts': 1000,           # Concept space size (can be large)\n        'semantic_comp_ratio': 0.25,    # Compression ratio per concept\n        'semantic_temp': 0.1,                    # Softmax temp for semantic retrieval\n        'top_k': 5,                     # Top-k memory vectors to retrieve\n        'expand_rate': 0.1,             # Optional growth for dynamic slot expansion\n\n        # --- General Memory Flags ---\n        'memory_type': 'hybrid',        # Use both episodic + semantic\n        'use_memory_router': True,\n        'memory_update_method': 'gated',# Learn to update selectively\n        'memory_decay': 0.95,           # Decay old memories slowly\n        'memory_norm': True,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:54:19.606412Z","iopub.execute_input":"2025-04-24T09:54:19.606719Z","iopub.status.idle":"2025-04-24T09:54:19.611413Z","shell.execute_reply.started":"2025-04-24T09:54:19.606694Z","shell.execute_reply":"2025-04-24T09:54:19.610633Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Initialize components\n    model = GPTMQMemoryModel(cfg=GPT_CONFIG_124M )\n    filename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\n    train_ratio = 0.90\n\n    with open(filename , 'r') as f:\n        text_data = json.load(f)\n    \n    text_data = text_data[:50]\n    print(len(text_data))\n    split = int(train_ratio * len(text_data))\n    print(split)\n    train_data= text_data[:split]\n    val_data = text_data[split:]\n    pipeline = GPTTrainingPipeline(model, config)\n    \n    \n    pipeline.train(train_data, val_data, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:57:39.026553Z","iopub.execute_input":"2025-04-24T09:57:39.026838Z","iopub.status.idle":"2025-04-24T09:57:40.857154Z","shell.execute_reply.started":"2025-04-24T09:57:39.026817Z","shell.execute_reply":"2025-04-24T09:57:40.855906Z"}},"outputs":[{"name":"stdout","text":"50\n45\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-62b7a3aefc8b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-79-dadfa9a2b396>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_texts, val_texts, epochs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mval_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-79-dadfa9a2b396>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             task_loss  =  torch.nn.functional.cross_entropy(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-66-8f23cf364a59>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tokens, mask)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_retention_alpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_retention_alpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-3cdbc8d3af5e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mresid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_ln_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Memory phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-2a3e46c681b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, mask)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfreq_cies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiQueryAttentionBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-2a3e46c681b0>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(q, k, v, mask, dropout)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                      \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mattention_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mattention_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (126) must match the size of tensor b (256) at non-singleton dimension 3"],"ename":"RuntimeError","evalue":"The size of tensor a (126) must match the size of tensor b (256) at non-singleton dimension 3","output_type":"error"}],"execution_count":80},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:48:09.976577Z","iopub.execute_input":"2025-04-24T09:48:09.976849Z","iopub.status.idle":"2025-04-24T09:48:09.982052Z","shell.execute_reply.started":"2025-04-24T09:48:09.976828Z","shell.execute_reply":"2025-04-24T09:48:09.981223Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"<generator object Module.parameters at 0x7ca4d17cba70>"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"# model = GPTMQModel2(GPT_CONFIG_124M)\nmodel = GPTMQMemoryModel(cfg=GPT_CONFIG_124M )\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\noptimizer  =  torch.optim.AdamW(model.parameters() , lr = 0.0004  , weight_decay= 0.01)\nnum_epochs = 1\ntrain_ratio = 0.90\nfilename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\n\nwith open(filename , 'r') as f:\n    text_data = json.load(f)\n\ntext_data = text_data[:50]\nprint(len(text_data))\nsplit = int(train_ratio * len(text_data))\nprint(split)\ntrain_data= text_data[:split]\nval_data = text_data[split:]\n# train_dataloader = create_dataloader_v1(txt= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True , stride=GPT_CONFIG_124M['context_length'])\n# val_dataloader = create_dataloader_v1(txt= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False , stride=GPT_CONFIG_124M['context_length'])\ntrain_dataloader = create_dataloader_v1(data= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True )\nval_dataloader = create_dataloader_v1(data= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False )\nstart_context = '### Instruction :Give three tips for staying healthy ### Response:'\nprint('start trainning')\ntrain_losses, val_losses, track_tokens_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=10\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.407715Z","iopub.status.idle":"2025-04-24T09:34:44.408080Z","shell.execute_reply":"2025-04-24T09:34:44.407916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\ndef get_param_group_summary(model):\n    groups = defaultdict(int)\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if \"embedding\" in name:\n            groups[\"embedding\"] += param.numel()\n        elif \"transformer\" in name:\n            groups[\"transformer_blocks\"] += param.numel()\n        elif \"memory\" in name or \"episodic\" in name or \"semantic\" in name:\n            groups[\"memory_modules\"] += param.numel()\n        elif \"norm\" in name:\n            groups[\"normalization\"] += param.numel()\n        elif \"lm_head\" in name or \"projection\" in name:\n            groups[\"output_projection\"] += param.numel()\n        else:\n            groups[\"other\"] += param.numel()\n    total = sum(groups.values())\n    for k, v in groups.items():\n        print(f\"{k:20s}: {v:,} parameters\")\n    print(f\"\\nTotal: {total:,}\")\nget_param_group_summary(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:47:03.421280Z","iopub.execute_input":"2025-04-24T09:47:03.421571Z","iopub.status.idle":"2025-04-24T09:47:03.430581Z","shell.execute_reply.started":"2025-04-24T09:47:03.421551Z","shell.execute_reply":"2025-04-24T09:47:03.429796Z"}},"outputs":[{"name":"stdout","text":"memory_modules      : 10,305,011 parameters\nembedding           : 38,725,376 parameters\ntransformer_blocks  : 72,061,464 parameters\nnormalization       : 768 parameters\noutput_projection   : 50,257 parameters\n\nTotal: 121,142,876\n","output_type":"stream"}],"execution_count":60},{"cell_type":"raw","source":"def count_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n# your custom GPT, MemoryLLM, etc.\ntotal_params = count_trainable_parameters(model)\nprint(f\"Trainable parameters: {total_params:,}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T09:45:21.381944Z","iopub.execute_input":"2025-04-24T09:45:21.382287Z","iopub.status.idle":"2025-04-24T09:45:21.388203Z","shell.execute_reply.started":"2025-04-24T09:45:21.382238Z","shell.execute_reply":"2025-04-24T09:45:21.387376Z"}}},{"cell_type":"code","source":"from collections import defaultdict\n\ndef modulewise_param_count(model):\n    module_params = defaultdict(int)\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            module = name.split('.')[0]  # or custom parsing\n            module_params[module] += param.numel()\n    \n    for module, count in sorted(module_params.items(), key=lambda x: -x[1]):\n        print(f\"{module:<20} : {count:,} parameters\")\n\nmodulewise_param_count(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:45:21.656375Z","iopub.execute_input":"2025-04-24T09:45:21.656704Z","iopub.status.idle":"2025-04-24T09:45:21.664403Z","shell.execute_reply.started":"2025-04-24T09:45:21.656679Z","shell.execute_reply":"2025-04-24T09:45:21.663432Z"}},"outputs":[{"name":"stdout","text":"transformer_blocks   : 72,061,464 parameters\nembedding            : 38,597,376 parameters\nshared_memory        : 10,433,010 parameters\nprojection           : 50,257 parameters\nfinal_norm           : 768 parameters\nmemory_retention_alpha : 1 parameters\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"def train_model_restart(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    checkpoint_path: str = None\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step, start_epoch = 0, -1, 0\n\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\" Total training steps: {total_steps}\")\n\n    #  Load from checkpoint if provided\n    if checkpoint_path is not None:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)\n\n        #  Reduce learning rate by half when resuming\n        for param_group in optimizer.param_groups:\n            old_lr = param_group['lr']\n            param_group['lr'] = old_lr * 0.5\n            print(f\" Reduced LR: {old_lr:.6f}  {param_group['lr']:.6f}\")\n\n        print(f\" Resuming training from Epoch {start_epoch}\")\n\n    #  Reinitialize scheduler after changing LR\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=500,\n        num_training_steps=total_steps\n    )\n\n    for epoch in tqdm(range(start_epoch, num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n\n        save_model_checkpoint(model, optimizer, epoch + 1, global_step, tokens_seen)\n\n    return train_losses, val_losses, track_tokens_seen\n\ndef save_model_checkpoint(model, optimizer, epoch, global_step=None, tokens_seen=None, path=\"checkpoint_epoch.pt\"):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    if global_step is not None:\n        checkpoint['global_step'] = global_step\n    if tokens_seen is not None:\n        checkpoint['tokens_seen'] = tokens_seen\n\n    torch.save(checkpoint, f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\")\n    print(f\" Saved checkpoint at epoch {epoch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.412648Z","iopub.status.idle":"2025-04-24T09:34:44.413047Z","shell.execute_reply":"2025-04-24T09:34:44.412872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"        checkpoint = torch.load('/kaggle/working/checkpoint_epoch_6.pt', map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.413954Z","iopub.status.idle":"2025-04-24T09:34:44.414329Z","shell.execute_reply":"2025-04-24T09:34:44.414156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses , val_losses  , token_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=2\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.415232Z","iopub.status.idle":"2025-04-24T09:34:44.415568Z","shell.execute_reply":"2025-04-24T09:34:44.415454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nloss_history = {\n    \"train_loss\": train_losses,\n    \"val_loss\": val_losses,\n    \"tokens_seen\": token_seen\n}\n\nwith open(\"loss_history.json\", \"w\") as f:\n    json.dump(loss_history, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.416549Z","iopub.status.idle":"2025-04-24T09:34:44.416821Z","shell.execute_reply":"2025-04-24T09:34:44.416720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# If you loaded from a JSON file\n# with open(\"loss_history.json\", \"r\") as f:\n#     data = json.load(f)\n#     train_losses = data[\"train_loss\"]\n#     val_losses = data[\"val_loss\"]\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\nplt.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\nplt.xlabel(\"Evaluation Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")  # Save the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.417430Z","iopub.status.idle":"2025-04-24T09:34:44.417738Z","shell.execute_reply":"2025-04-24T09:34:44.417597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\n\n# Load model\nmodel = GPTMQModel2(GPT_CONFIG_124M)\n# model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_7.pt\"))\ncheckpoint = torch.load(\"checkpoint_epoch_7.pt\")\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\nmodel.eval().to(device)\n\n# Tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Utility functions\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n# Sampling-based generate function (uses your logic)\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\nf\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n# High-level text generation function\ndef generate_response(prompt, model, tokenizer, max_new_tokens=100, context_size=128, temperature=1.0, top_k=50):\n    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n    generated_ids = generate(\n        model=model,\n        idx=input_ids,\n        max_new_tokens=max_new_tokens,\n        context_size=context_size,\n        temperature=temperature,\n        top_k=top_k\n    )\n    return token_ids_to_text(generated_ids, tokenizer)\n\n# Try it out\n# prompt = \"### Instruction:\\nExplain what is deep learning.\\n\\n### Response:\\n <bot>\"\nprompt = \"\"\"\n\n'### Instruction :Give three tips for staying healthy ### Response:'\n\"\"\".strip()\n\n\noutput = generate_response(prompt, model, tokenizer)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.418499Z","iopub.status.idle":"2025-04-24T09:34:44.418773Z","shell.execute_reply":"2025-04-24T09:34:44.418675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is generated\n        if idx_next.item() == end_token_id:\n            break\n\n    return idx\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\nraw_output = generate_response(prompt, model, tokenizer)\ncleaned_output = truncate_after_n_bullets(raw_output)\nprint(cleaned_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.419352Z","iopub.status.idle":"2025-04-24T09:34:44.419579Z","shell.execute_reply":"2025-04-24T09:34:44.419486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_response(\n    prompt, model, tokenizer,\n    temperature=0.8,  # better balance\n    top_k=40,         # a bit narrower selection\n    max_new_tokens=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.420138Z","iopub.status.idle":"2025-04-24T09:34:44.420420Z","shell.execute_reply":"2025-04-24T09:34:44.420314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device).unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is in the generated output\n        if end_token_id in idx_next:\n            break\n\n    return idx\n\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\n\n#  Input prompt\nprompt = \"### Instruction: What are the three primary colors? \\n### Response:\"\n\n#  Tokenize input\ninput_ids = text_to_token_ids(prompt, tokenizer).to(device)\n\n#  Generate output tokens\noutput_ids = generate(\n    model=model,\n    idx=input_ids,\n    max_new_tokens=100,\n    context_size=128,\n    temperature=0.7,\n    top_k=40\n)\n\n#  Decode and postprocess\noutput_text = tokenizer.decode(output_ids[0].tolist())\n\n#  Truncate after 3 bullets (optional)\nfinal_output = truncate_after_n_bullets(output_text)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.421013Z","iopub.status.idle":"2025-04-24T09:34:44.421333Z","shell.execute_reply":"2025-04-24T09:34:44.421177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}