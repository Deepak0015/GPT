{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11339249,"sourceType":"datasetVersion","datasetId":7093845}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math \nimport torch \nimport torch.nn as nn \nimport transformers \nfrom tqdm.notebook import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:28:54.565165Z","iopub.execute_input":"2025-04-11T03:28:54.565490Z","iopub.status.idle":"2025-04-11T03:29:21.769873Z","shell.execute_reply.started":"2025-04-11T03:28:54.565462Z","shell.execute_reply":"2025-04-11T03:29:21.768676Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Embedding Layer","metadata":{}},{"cell_type":"code","source":"import torch \n\nclass EmbeddingLayer(torch.nn.Module):\n    def __init__(self , vocab_size , embedding_dim):\n        super().__init__()\n\n        self.embedding_layer= torch.nn.Embedding(vocab_size , embedding_dim)\n\n    def forward(self , input_tokens):\n        return self.embedding_layer(input_tokens)\n        \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:28.570536Z","iopub.execute_input":"2025-04-11T03:29:28.570915Z","iopub.status.idle":"2025-04-11T03:29:28.575987Z","shell.execute_reply.started":"2025-04-11T03:29:28.570888Z","shell.execute_reply":"2025-04-11T03:29:28.574579Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# FeedForward Layer ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n      \n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n            GELU(),\n            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n        )\n    def forward(self, x ):\n        return self.layers(x)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:29.090808Z","iopub.execute_input":"2025-04-11T03:29:29.091139Z","iopub.status.idle":"2025-04-11T03:29:29.097561Z","shell.execute_reply.started":"2025-04-11T03:29:29.091109Z","shell.execute_reply":"2025-04-11T03:29:29.096385Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Normalization Layer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch \n\nclass LayerNorm(nn.Module):\n    def __init__(self , emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale  = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self , x):\n        mean = x.mean(dim= -1, keepdim = True)\n        var = x.var(dim =-1, keepdim = True)\n        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n        return self.scale * norm_x + self.shift \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:29.445175Z","iopub.execute_input":"2025-04-11T03:29:29.445522Z","iopub.status.idle":"2025-04-11T03:29:29.451244Z","shell.execute_reply.started":"2025-04-11T03:29:29.445493Z","shell.execute_reply":"2025-04-11T03:29:29.450304Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# RoPE Embdding ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport torch \nfrom dataclasses import dataclass\n\n\nclass NRopE: # RopE in Numpy \n    def rotate_2d(self,vec , theta_p):\n        cos_theta  , sin_theta  = np.cos(theta_p) , np.sin(theta_p)\n        rotat_vec = np.array([[cos_theta , -sin_theta],\n                    [sin_theta ,cos_theta]])\n        \n        return rotat_vec @ vec\n\n\n    def RoPe(self,x , p , theta = 10000):\n        d = len(x)\n        x_rotate =  np.zeros_like(x)\n        for i in range(0 , d , 2):\n            if i +1< d:\n                theta_p = (theta **(-2*(i//2)))**p \n                roted_pair = self.rotate_2d(x[i:i+1] , theta_p)    \n                x_rotate[i:i+1] = roted_pair\n\n        return x_rotate\n\n\n\n@dataclass\nclass TRopE(torch.nn.Module): # RopE in torch \n    def __init__(self, dim:int ,theta:float = 10000):\n        self.dim = dim \n        self.theta = theta \n        self.freq =  torch.pow(self.theta ,-torch.arange(0 ,dim  , 2)/dim )\n        torch.nn.Parameter('freq' , self.freq)\n\n    def forward(self, x:torch.Tensor , pos:torch.Tensor):\n        batch_size , seq_len, dim = x.shape\n        assert dim ==self.dim ,\"Error dim must be same\"\n        theta_p = torch.einsum(\"n,d->nd\" , pos, self.freq.to(x.device))\n        cos_theta  , sin_theta = torch.cos(theta_p) , torch.sin(theta_p)\n        x_even , x_odd =  x[... , ::2] , x[... , 1::2]\n        x_rotated =  torch.empty_like(x)\n        x_rotated[...,::2] =  x_even * cos_theta - x_odd * sin_theta\n        x_rotated[...,1::2] =  x_even * sin_theta + x_odd * cos_theta\n\n        return x_rotated\n\n\n\n\n\n\n\ndef precompute_freq_cis(  dim:int , end:int , theta:float = 10000.0):\n        \"\"\"dim : dimentions \n        end: end index   \n        \"\"\"\n        freqs =  1/(theta **(torch.arange(0 , dim , 2)[:dim//2].float() / dim))\n        t =  torch.arange(end, device=freqs.device)\n        freqs = torch.outer(t , freqs).float()\n        freqs_cis =  torch.polar(torch.ones_like(freqs), freqs)\n        return freqs_cis \n\n\ndef reshape_for_broadcast(freq_cis  , x):\n        \"\"\" reshape the freqcies to match x dimentions \"\"\"\n        ndim=  x.ndim\n        assert 0<=1<ndim \n        assert freq_cis.shape == (x.shape[1], x.shape[-1]), f\"Expected {(x.shape[1], x.shape[-1])}, got {freq_cis.shape}\" \n        shape = [d if i == 1 or i ==  ndim -1 else 1 for i , d in enumerate(x.shape)]\n        return freq_cis.view(*shape)\n\n\ndef apply_rotary_embedding( xq:torch.Tensor ,xk:torch.Tensor ,  freq_cis:torch.Tensor):\n\n            xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n            xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n\n\n            freq_cies =  reshape_for_broadcast(freq_cis , xq_)\n    \n\n            xq_out = torch.view_as_real(xq_* freq_cies).flatten(3)\n            \n            xk_out = torch.view_as_real(xk_*freq_cies).flatten(3)\n\n\n            return  xq_out.type_as(xq)   ,  xk_out.type_as(xq) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:30.171050Z","iopub.execute_input":"2025-04-11T03:29:30.171387Z","iopub.status.idle":"2025-04-11T03:29:30.185398Z","shell.execute_reply.started":"2025-04-11T03:29:30.171328Z","shell.execute_reply":"2025-04-11T03:29:30.184332Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# MultiHead & MultiQuery Attention Layer ","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention_V2(nn.Module):\n    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n        super().__init__()\n        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n        self.d_in =d_in\n        self.d_out = d_out\n        self.dropout = nn.Dropout(dropout)\n        self.num_heads  = num_heads\n        self.head_dim = d_out // num_heads\n        self.out_proj  = nn.Linear(d_out , d_out)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens , d_in = x.shape\n        keys = self.w_key(x)\n        queries  = self.w_query(x)\n        values = self.w_value(x)\n        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(2, 3)\n        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n        attn_scores.masked_fill(mask_bool , -torch.inf)\n        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n        attn_weights = self.dropout(attn_weights)\n        context_vector = (attn_weights  @ values).transpose(1, 2)\n        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n        context_vector = self.out_proj(context_vector)\n        return context_vector\n\n\n\n\ndef apply_rotary_embedding(xq:torch.Tensor , xk:torch.Tensor , freq_cies:torch.Tensor):\n\n    assert xq.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n    assert xk.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1] , -1, 2))\n\n    freq_cies = reshape_for_broadcast(freq_cies , xq_)\n\n    xq_out = torch.view_as_real(xq_ * freq_cies ).flatten(3)\n\n    xk_out = torch.view_as_real(xk_ * freq_cies).flatten(3)\n\n    return xq_out.type_as(xq) ,  xk_out.type_as(xk)\n\n\n\n\nclass MultiQueryAttentionBlock(nn.Module):\n    def __init__(self, d_model:int , h:int , dropout:float , seq_len:int , qkv_bias =  False ):\n        super().__init__()\n        self.d_model  = d_model \n\n        self.seq_len=  seq_len\n\n        assert d_model % h == 0, \"d_model is must be divided by th head\"\n        self.dropout = nn.Dropout(dropout)\n\n        self.h = h  \n\n        self.d_k = d_model // h \n\n        self.w_qkv =  nn.Linear(d_model , d_model +2 * self.d_k )\n\n        self.w_o = nn.Linear(d_model  , d_model)\n\n        freq_cies = precompute_freq_cis(dim=self.d_k , end=self.seq_len * 2 )\n\n        self.register_buffer('freq_cies' , freq_cies , persistent= False )\n\n    def generate_causal_mask(self, seq_len, device):\n        # shape: (1, 1, seq_len, seq_len)\n        return torch.tril(torch.ones((1, 1, seq_len, seq_len), device=device)).bool()\n\n    @staticmethod\n    def attention(q, k  , v,mask  , dropout):\n        d_k = q.shape[-1]\n\n        attention_score =  (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n\n        if mask is not None :\n            if mask.dim() == 2:\n                      mask = mask.unsqueeze(1).unsqueeze(2)\n            elif mask.dim() == 3:\n                     mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n        \n        attention_score = attention_score.softmax(dim=-1)\n\n        if dropout is not None :\n            attention_score = dropout(attention_score)\n\n        context_vector =  attention_score @ v\n\n        return context_vector  , attention_score\n    \n\n\n    def forward(self, q, mask= None):\n        if mask is None:\n            mask = self.generate_causal_mask(self.seq_len , device = q.device)\n        qkv =  self.w_qkv(q)\n\n        query , key, value =  torch.split(qkv , [self.d_model  , self.d_k , self.d_k], dim=-1)\n\n        query = query.view(query.shape[0] , -1 , self.h , self.d_k).transpose(1, 2)\n\n        key =  key.unsqueeze(1)\n\n        value =  value.unsqueeze(1)\n\n        seq_len =  q.size(1)\n\n        freq_cies = self.freq_cies[:query.shape[1]].to(q.device)\n\n        # freq_cies =  self.freq_cies[:seq_len].to(q.device)\n\n        query , key = apply_rotary_embedding(query , key , freq_cies)\n\n        x , self.attention_score = MultiQueryAttentionBlock.attention(q = query,k =  key,v= value ,mask=mask , dropout= self.dropout)\n\n        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h* self.d_k)\n\n        x = self.w_o(x)\n\n        return x \n    \n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:30.485532Z","iopub.execute_input":"2025-04-11T03:29:30.485809Z","iopub.status.idle":"2025-04-11T03:29:30.505083Z","shell.execute_reply.started":"2025-04-11T03:29:30.485785Z","shell.execute_reply":"2025-04-11T03:29:30.504099Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Transformer Block","metadata":{}},{"cell_type":"code","source":"\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention_V2(\n        d_in=cfg[\"emb_dim\"],\n        d_out=cfg[\"emb_dim\"],\n        context_length=cfg[\"context_length\"],\n        num_heads=cfg[\"n_heads\"],\n        dropout=cfg[\"drop_rate\"],\n        qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n    #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut # Add the original input back\n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut #C\n        return x\n\n\n\nclass TransformerBlock_v2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention =  MultiQueryAttentionBlock(d_model=cfg['emb_dim'], h=cfg['n_heads'] , dropout=cfg['drop_rate'], seq_len=  cfg['context_length'] ,qkv_bias=cfg['qkv_bias'])\n\n        self.feed_forward = FeedForward(cfg)\n\n        self.layernorm1 =  LayerNorm(cfg['emb_dim'])\n    \n        self.layernorm2 =  LayerNorm(cfg['emb_dim'])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x , mask= None):\n\n        attention_output =  self.attention(self.layernorm1(x) , mask =  mask)\n\n        ff_output =  self.feed_forward(self.layernorm2(x))\n\n        return x + self.drop_out(ff_output) + self.drop_out(attention_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:30.765983Z","iopub.execute_input":"2025-04-11T03:29:30.766326Z","iopub.status.idle":"2025-04-11T03:29:30.774823Z","shell.execute_reply.started":"2025-04-11T03:29:30.766299Z","shell.execute_reply":"2025-04-11T03:29:30.773903Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# GPTQModel","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int, embdding_layer: nn.Embedding):\n        super().__init__()\n        self.weight = embdding_layer.weight  # share weights with input embedding\n        self.bias = nn.Parameter(torch.zeros(vocab_size))  # learnable bias\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\n\n\n\nclass GPTMQModel2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:31.084831Z","iopub.execute_input":"2025-04-11T03:29:31.085129Z","iopub.status.idle":"2025-04-11T03:29:31.094081Z","shell.execute_reply.started":"2025-04-11T03:29:31.085104Z","shell.execute_reply":"2025-04-11T03:29:31.092805Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"\n\n\n\ndef cal_loss_batch(input_batch , target_batch , model:torch.nn.Module , device:torch.device):\n    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader , model , device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:31.364975Z","iopub.execute_input":"2025-04-11T03:29:31.365284Z","iopub.status.idle":"2025-04-11T03:29:31.371227Z","shell.execute_reply.started":"2025-04-11T03:29:31.365258Z","shell.execute_reply":"2025-04-11T03:29:31.370257Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Text Generation Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\n\n\n\ndef text_to_token_ids(text,  tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\n\ndef token_ids_to_text(tokens , tokenizer):\n    flat  = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n    \ndef generate_and_sample(model  , idx , context_size ,max_new_tokens ):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n            print(logits.shape)\n        logits  = logits[:, -1  , :]\n        print(logits.shape)\n        probs  = torch.softmax(logits  , dim=-1)\n        print(probs)\n        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n        print(idx_next)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx \n\n# def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n#     for _ in range(max_new_tokens):\n#         idx_cond = idx[:, -context_size:]  # shape: [1, current_seq_len]\n\n#         # Create causal mask dynamically\n#         seq_len = idx_cond.size(1)\n#         causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n#         causal_mask = causal_mask.unsqueeze(0)  # [1, seq_len, seq_len]\n\n#         with torch.no_grad():\n#             logits = model(idx_cond, mask=causal_mask)  # <--- pass mask here\n\n#         logits = logits[:, -1, :]  # only take the last token logits\n\n#         # Apply top-k sampling if needed\n#         if top_k is not None:\n#             top_logits, _ = torch.topk(logits, top_k)\n#             min_val = top_logits[:, -1]\n#             logits = torch.where(\n#                 logits < min_val,\n#                 torch.tensor(float('-inf')).to(logits.device),\n#                 logits\n#             )\n\n#         # Temperature sampling\n#         if temperature > 0.0:\n#             logits = logits / temperatureallowed_special=allowed\n#             probs = torch.softmax(logits, dim=-1)\n#             idx_next = torch.multinomial(probs, num_samples=1)\n#         else:\n#             idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n#         idx = torch.cat((idx, idx_next), dim=1)\n#     return idx\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, end_token_id=None):\n    model.eval()\n    max_seq_len = context_size\n    full_causal_mask = torch.tril(torch.ones(max_seq_len, max_seq_len)).to(idx.device).unsqueeze(0)\n\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = full_causal_mask[:, :seq_len, :seq_len]\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n        logits = logits[:, -1, :]  # Take last token logits\n\n        # Top-k sampling\n        if top_k is not None:\n            top_values, _ = torch.topk(logits, top_k)\n            threshold = top_values[:, -1].unsqueeze(1)\n            logits = logits.masked_fill(logits < threshold, float('-inf'))\n\n        logits = logits / temperature\n        logits = torch.clamp(logits, -100, 100)\n        probs = torch.softmax(logits, dim=-1)\n\n        idx_next = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        if end_token_id is not None and idx_next.item() == end_token_id:\n            break\n\n    return idx\n\n\n\n\n\ndef real_time_generation(model, initial_input, context_size, temperature, top_k=None, device=\"cpu\"):\n    # Tokenize the initial input and prepare the model context\n    idx = torch.tensor(initial_input).unsqueeze(0).to(device)  # Assuming initial_input is tokenized\n    \n    print(\"Starting real-time generation...\")\n    \n    # Start generating tokens in real-time\n    for new_token in generate(model, idx, max_new_tokens=50, context_size=context_size, temperature=temperature, top_k=top_k, device=device):\n        print(f\"Generated token: {new_token.item()}\")  # Or decode it back to a word\n        \n        # You can check for user input here and update idx with the new input\n        # For instance, wait for the user to input a prompt to append to the context\n        user_input = input(\"Enter new input (or press enter to continue generation): \")\n        \n        if user_input:\n            # Tokenize the new user input and append it to the context\n            user_input_tokens = torch.tensor(tokenize(user_input)).unsqueeze(0).to(device)\n            idx = torch.cat((idx, user_input_tokens), dim=1)  # Append the new tokens to the context\n        else:\n            # Continue generating if no new user input\n            continue\n\n# Function to tokenize input (adjust depending on your tokenizer)\ndef tokenize(text):\n    # Assuming you have a tokenizer function available\n    return [ord(c) for c in text]  # Dummy example: ord() converts char to token id\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:31.609972Z","iopub.execute_input":"2025-04-11T03:29:31.610286Z","iopub.status.idle":"2025-04-11T03:29:31.624969Z","shell.execute_reply.started":"2025-04-11T03:29:31.610262Z","shell.execute_reply":"2025-04-11T03:29:31.623954Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"! pip install tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:50.966034Z","iopub.execute_input":"2025-04-11T03:29:50.966463Z","iopub.status.idle":"2025-04-11T03:29:56.209422Z","shell.execute_reply.started":"2025-04-11T03:29:50.966434Z","shell.execute_reply":"2025-04-11T03:29:56.207998Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\nInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.9.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Dataset and DataLoader ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef generate_prompt(sample):\n    # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n    \n\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer =  tokenizer\n\n        all_tokens = []\n        allowed = {'<|endoftext|>'}\n        for sample in data:\n            prompt = generate_prompt(sample)\n            tokens = tokenizer.encode(prompt , allowed_special=allowed)\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n    return inputs, targets\n\ndef create_dataloader_v1(data, batch_size=4,\n    max_length=256, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n    dataset = Dataset_V1(data, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=collate_fn)\n    return dataloader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:29:59.730630Z","iopub.execute_input":"2025-04-11T03:29:59.731030Z","iopub.status.idle":"2025-04-11T03:29:59.837939Z","shell.execute_reply.started":"2025-04-11T03:29:59.730996Z","shell.execute_reply":"2025-04-11T03:29:59.836818Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Train Script ","metadata":{}},{"cell_type":"code","source":"\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\n\n\ndef evaluate_model(model, train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,   # Increase generation length if needed\n            context_size=126,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n\n        # Trim everything before the generation\n        generated_only = decoded_text[len(start_context):].strip()\n\n        # Stop at endoftext token if present\n        end_marker = \"<|endoftext|>\"\n        if end_marker in generated_only:\n            generated_only = generated_only.split(end_marker)[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\ndef save_model_checkpoint(model, optimizer, epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch\n    }\n    torch.save(checkpoint, path.format(epoch))\ndef after_save_load():\n    checkpoint = torch.load(\"checkpoint_epoch_7.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n\n\ndef train_model(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n    # scheduler = get_cosine_schedule_with_warmup(optimizer,\n    #                                         num_warmup_steps=500,\n    #                                         num_training_steps=total_steps)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n        save_model_checkpoint(model , optimizer , epoch+1)\n\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:30:02.975424Z","iopub.execute_input":"2025-04-11T03:30:02.975730Z","iopub.status.idle":"2025-04-11T03:30:02.989248Z","shell.execute_reply.started":"2025-04-11T03:30:02.975706Z","shell.execute_reply":"2025-04-11T03:30:02.988449Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# GPT Config ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nGPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,\n# Context length\n\"emb_dim\": 768,\n# Embedding dimension\n\"n_heads\": 12,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False\n# Query-Key-Value bias\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:30:03.325504Z","iopub.execute_input":"2025-04-11T03:30:03.325813Z","iopub.status.idle":"2025-04-11T03:30:03.332770Z","shell.execute_reply.started":"2025-04-11T03:30:03.325787Z","shell.execute_reply":"2025-04-11T03:30:03.331447Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = GPTMQModel2(GPT_CONFIG_124M)\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\noptimizer  =  torch.optim.AdamW(model.parameters() , lr = 0.0004  , weight_decay= 0.01)\nnum_epochs = 1\ntrain_ratio = 0.90\nfilename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\n\nwith open(filename , 'r') as f:\n    text_data = json.load(f)\n\n# text_data = text_data[:50]\nprint(len(text_data))\nsplit = int(train_ratio * len(text_data))\nprint(split)\ntrain_data= text_data[:split]\nval_data = text_data[split:]\n# train_dataloader = create_dataloader_v1(txt= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True , stride=GPT_CONFIG_124M['context_length'])\n# val_dataloader = create_dataloader_v1(txt= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False , stride=GPT_CONFIG_124M['context_length'])\ntrain_dataloader = create_dataloader_v1(data= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True )\nval_dataloader = create_dataloader_v1(data= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False )\nstart_context = '### Instruction :Give three tips for staying healthy ### Response:'\nprint('start trainning')\ntrain_losses , val_losses  , token_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=10\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:30:05.813890Z","iopub.execute_input":"2025-04-11T03:30:05.814203Z","iopub.status.idle":"2025-04-11T03:30:28.163878Z","shell.execute_reply.started":"2025-04-11T03:30:05.814177Z","shell.execute_reply":"2025-04-11T03:30:28.162213Z"}},"outputs":[{"name":"stdout","text":"51760\n46584\nstart trainning\n🚀 Total training steps: 311980\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1 (step 000000): Train Loss: 248.4611, Val Loss: 248.1597\nEpoch: 1 (step 000005): Train Loss: 221.7021, Val Loss: 213.9365\nEpoch: 1 (step 000010): Train Loss: 81.3126, Val Loss: 73.2363\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/10 [00:09<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m start_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m### Instruction :Give three tips for staying healthy ### Response:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart trainning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m train_losses , val_losses  , token_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[15], line 82\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, device, eval_dataloader, optimizer, eval_freq, eval_iter, start_context, num_epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m cal_loss_batch(input_batch\u001b[38;5;241m=\u001b[39minputs_batch, target_batch\u001b[38;5;241m=\u001b[39mtarget_batch, device\u001b[38;5;241m=\u001b[39mdevice, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 82\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     85\u001b[0m tokens_seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inputs_batch\u001b[38;5;241m.\u001b[39mnumel()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/adamw.py:429\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 429\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model_restart(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    checkpoint_path: str = None\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step, start_epoch = 0, -1, 0\n\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # 🔁 Load from checkpoint if provided\n    if checkpoint_path is not None:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)\n\n        # ⬇️ Reduce learning rate by half when resuming\n        for param_group in optimizer.param_groups:\n            old_lr = param_group['lr']\n            param_group['lr'] = old_lr * 0.5\n            print(f\"🔧 Reduced LR: {old_lr:.6f} ➜ {param_group['lr']:.6f}\")\n\n        print(f\"✅ Resuming training from Epoch {start_epoch}\")\n\n    # ⚙️ Reinitialize scheduler after changing LR\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=500,\n        num_training_steps=total_steps\n    )\n\n    for epoch in tqdm(range(start_epoch, num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n\n        save_model_checkpoint(model, optimizer, epoch + 1, global_step, tokens_seen)\n\n    return train_losses, val_losses, track_tokens_seen\n\ndef save_model_checkpoint(model, optimizer, epoch, global_step=None, tokens_seen=None, path=\"checkpoint_epoch.pt\"):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    if global_step is not None:\n        checkpoint['global_step'] = global_step\n    if tokens_seen is not None:\n        checkpoint['tokens_seen'] = tokens_seen\n\n    torch.save(checkpoint, f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\")\n    print(f\"💾 Saved checkpoint at epoch {epoch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.363412Z","iopub.status.idle":"2025-04-10T17:44:26.363681Z","shell.execute_reply":"2025-04-10T17:44:26.363575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"        checkpoint = torch.load('/kaggle/working/checkpoint_epoch_6.pt', map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.364431Z","iopub.status.idle":"2025-04-10T17:44:26.364863Z","shell.execute_reply":"2025-04-10T17:44:26.364665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses , val_losses  , token_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=2\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.366088Z","iopub.status.idle":"2025-04-10T17:44:26.366693Z","shell.execute_reply":"2025-04-10T17:44:26.366316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nloss_history = {\n    \"train_loss\": train_losses,\n    \"val_loss\": val_losses,\n    \"tokens_seen\": token_seen\n}\n\nwith open(\"loss_history.json\", \"w\") as f:\n    json.dump(loss_history, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.367934Z","iopub.status.idle":"2025-04-10T17:44:26.368323Z","shell.execute_reply":"2025-04-10T17:44:26.368126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# If you loaded from a JSON file\n# with open(\"loss_history.json\", \"r\") as f:\n#     data = json.load(f)\n#     train_losses = data[\"train_loss\"]\n#     val_losses = data[\"val_loss\"]\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\nplt.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\nplt.xlabel(\"Evaluation Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")  # Save the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.369381Z","iopub.status.idle":"2025-04-10T17:44:26.369713Z","shell.execute_reply":"2025-04-10T17:44:26.369555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\n\n# Load model\nmodel = GPTMQModel2(GPT_CONFIG_124M)\n# model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_7.pt\"))\ncheckpoint = torch.load(\"checkpoint_epoch_7.pt\")\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\nmodel.eval().to(device)\n\n# Tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Utility functions\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n# Sampling-based generate function (uses your logic)\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n# High-level text generation function\ndef generate_response(prompt, model, tokenizer, max_new_tokens=100, context_size=128, temperature=1.0, top_k=50):\n    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n    generated_ids = generate(\n        model=model,\n        idx=input_ids,\n        max_new_tokens=max_new_tokens,\n        context_size=context_size,\n        temperature=temperature,\n        top_k=top_k\n    )\n    return token_ids_to_text(generated_ids, tokenizer)\n\n# Try it out\n# prompt = \"### Instruction:\\nExplain what is deep learning.\\n\\n### Response:\\n <bot>\"\nprompt = \"\"\"\n\n'### Instruction :Give three tips for staying healthy ### Response:'\n\"\"\".strip()\n\n\noutput = generate_response(prompt, model, tokenizer)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.370614Z","iopub.status.idle":"2025-04-10T17:44:26.371004Z","shell.execute_reply":"2025-04-10T17:44:26.370845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is generated\n        if idx_next.item() == end_token_id:\n            break\n\n    return idx\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\nraw_output = generate_response(prompt, model, tokenizer)\ncleaned_output = truncate_after_n_bullets(raw_output)\nprint(cleaned_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.371865Z","iopub.status.idle":"2025-04-10T17:44:26.372219Z","shell.execute_reply":"2025-04-10T17:44:26.372064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_response(\n    prompt, model, tokenizer,\n    temperature=0.8,  # better balance\n    top_k=40,         # a bit narrower selection\n    max_new_tokens=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.372927Z","iopub.status.idle":"2025-04-10T17:44:26.373305Z","shell.execute_reply":"2025-04-10T17:44:26.373144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device).unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is in the generated output\n        if end_token_id in idx_next:\n            break\n\n    return idx\n\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\n\n# 🔁 Input prompt\nprompt = \"### Instruction: What are the three primary colors? \\n### Response:\"\n\n# 🔁 Tokenize input\ninput_ids = text_to_token_ids(prompt, tokenizer).to(device)\n\n# 🔁 Generate output tokens\noutput_ids = generate(\n    model=model,\n    idx=input_ids,\n    max_new_tokens=100,\n    context_size=128,\n    temperature=0.7,\n    top_k=40\n)\n\n# 🔁 Decode and postprocess\noutput_text = tokenizer.decode(output_ids[0].tolist())\n\n# ✂️ Truncate after 3 bullets (optional)\nfinal_output = truncate_after_n_bullets(output_text)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.374345Z","iopub.status.idle":"2025-04-10T17:44:26.374579Z","shell.execute_reply":"2025-04-10T17:44:26.374485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Instruction: Give three tips for staying healthy\n### Response:\n1. Plan your meals: Instead of staying hydrated, set your meals and try to keep your meals informed about whether you need it.\n2. Set achievable goals: Identify your tasks into your meals, such as using a variety of fruits, vegetables, vegetables, eggs, or fruits, vegetables.\n3. Take a break: Take a few minutes early and enjoy a lot of fresh fruits, vegetables, and vegetables.\n\n[ ]:\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T17:44:26.375288Z","iopub.status.idle":"2025-04-10T17:44:26.375632Z","shell.execute_reply":"2025-04-10T17:44:26.375498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}