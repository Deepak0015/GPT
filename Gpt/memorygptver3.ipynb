{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339249,"sourceType":"datasetVersion","datasetId":7093845},{"sourceId":11378548,"sourceType":"datasetVersion","datasetId":7124129},{"sourceId":11378997,"sourceType":"datasetVersion","datasetId":7124489}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math \nimport torch \nimport torch.nn as nn \nimport transformers \nfrom tqdm.notebook import tqdm\n# Memory Network\nimport torch.nn.functional as F \nimport random\nfrom typing import Tuple , Optional\nimport torch.bin \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:11.453716Z","iopub.execute_input":"2025-05-07T16:24:11.454104Z","iopub.status.idle":"2025-05-07T16:24:16.734785Z","shell.execute_reply.started":"2025-05-07T16:24:11.454073Z","shell.execute_reply":"2025-05-07T16:24:16.734048Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.735793Z","iopub.execute_input":"2025-05-07T16:24:16.736197Z","iopub.status.idle":"2025-05-07T16:24:16.739912Z","shell.execute_reply.started":"2025-05-07T16:24:16.736167Z","shell.execute_reply":"2025-05-07T16:24:16.739004Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# RMS NORM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim)) \n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        norm_x = self._norm(x.float()).type_as(x) \n        return norm_x * self.scale \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.741416Z","iopub.execute_input":"2025-05-07T16:24:16.741646Z","iopub.status.idle":"2025-05-07T16:24:16.759836Z","shell.execute_reply.started":"2025-05-07T16:24:16.741627Z","shell.execute_reply":"2025-05-07T16:24:16.758999Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Embedding Layer","metadata":{}},{"cell_type":"code","source":"import torch \n\nclass EmbeddingLayer(torch.nn.Module):\n    def __init__(self , vocab_size , embedding_dim):\n        super().__init__()\n\n        self.embedding_layer= torch.nn.Embedding(vocab_size , embedding_dim)\n\n    def forward(self , input_tokens):\n        return self.embedding_layer(input_tokens)\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.761122Z","iopub.execute_input":"2025-05-07T16:24:16.761418Z","iopub.status.idle":"2025-05-07T16:24:16.777474Z","shell.execute_reply.started":"2025-05-07T16:24:16.761388Z","shell.execute_reply":"2025-05-07T16:24:16.776601Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# FeedForward Layer ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n      \n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n            GELU(),\n            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n        )\n    def forward(self, x ):\n        return self.layers(x)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.778325Z","iopub.execute_input":"2025-05-07T16:24:16.778601Z","iopub.status.idle":"2025-05-07T16:24:16.794215Z","shell.execute_reply.started":"2025-05-07T16:24:16.778574Z","shell.execute_reply":"2025-05-07T16:24:16.793496Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Normalization Layer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch \n\nclass LayerNorm(nn.Module):\n    def __init__(self , emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale  = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self , x):\n        mean = x.mean(dim= -1, keepdim = True)\n        var = x.var(dim =-1, keepdim = True)\n        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n        return self.scale * norm_x + self.shift \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.794884Z","iopub.execute_input":"2025-05-07T16:24:16.795129Z","iopub.status.idle":"2025-05-07T16:24:16.810375Z","shell.execute_reply.started":"2025-05-07T16:24:16.795093Z","shell.execute_reply":"2025-05-07T16:24:16.809791Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# RoPE Embdding ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport torch \nfrom dataclasses import dataclass\n\n\nclass NRopE: # RopE in Numpy \n    def rotate_2d(self,vec , theta_p):\n        cos_theta  , sin_theta  = np.cos(theta_p) , np.sin(theta_p)\n        rotat_vec = np.array([[cos_theta , -sin_theta],\n                    [sin_theta ,cos_theta]])\n        \n        return rotat_vec @ vec\n\n\n    def RoPe(self,x , p , theta = 10000):\n        d = len(x)\n        x_rotate =  np.zeros_like(x)\n        for i in range(0 , d , 2):\n            if i +1< d:\n                theta_p = (theta **(-2*(i//2)))**p \n                roted_pair = self.rotate_2d(x[i:i+1] , theta_p)    \n                x_rotate[i:i+1] = roted_pair\n\n        return x_rotate\n\n\n\n@dataclass\nclass TRopE(torch.nn.Module): # RopE in torch \n    def __init__(self, dim:int ,theta:float = 10000):\n        self.dim = dim \n        self.theta = theta \n        self.freq =  torch.pow(self.theta ,-torch.arange(0 ,dim  , 2)/dim )\n        torch.nn.Parameter('freq' , self.freq)\n\n    def forward(self, x:torch.Tensor , pos:torch.Tensor):\n        batch_size , seq_len, dim = x.shape\n        assert dim ==self.dim ,\"Error dim must be same\"\n        theta_p = torch.einsum(\"n,d->nd\" , pos, self.freq.to(x.device))\n        cos_theta  , sin_theta = torch.cos(theta_p) , torch.sin(theta_p)\n        x_even , x_odd =  x[... , ::2] , x[... , 1::2]\n        x_rotated =  torch.empty_like(x)\n        x_rotated[...,::2] =  x_even * cos_theta - x_odd * sin_theta\n        x_rotated[...,1::2] =  x_even * sin_theta + x_odd * cos_theta\n\n        return x_rotated\n\n\n\n\n\n\n\ndef precompute_freq_cis(  dim:int , end:int , theta:float = 10000.0):\n        \"\"\"dim : dimentions \n        end: end index   \n        \"\"\"\n        freqs =  1/(theta **(torch.arange(0 , dim , 2)[:dim//2].float() / dim))\n        t =  torch.arange(end, device=freqs.device)\n        freqs = torch.outer(t , freqs).float()\n        freqs_cis =  torch.polar(torch.ones_like(freqs), freqs)\n        return freqs_cis \n\n\ndef reshape_for_broadcast(freq_cis  , x):\n        \"\"\" reshape the freqcies to match x dimentions \"\"\"\n        ndim=  x.ndim\n        assert 0<=1<ndim \n        assert freq_cis.shape == (x.shape[1], x.shape[-1]), f\"Expected {(x.shape[1], x.shape[-1])}, got {freq_cis.shape}\" \n        shape = [d if i == 1 or i ==  ndim -1 else 1 for i , d in enumerate(x.shape)]\n        return freq_cis.view(*shape)\n\n\ndef apply_rotary_embedding( xq:torch.Tensor ,xk:torch.Tensor ,  freq_cis:torch.Tensor):\n\n            xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n            xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n\n\n            freq_cies =  reshape_for_broadcast(freq_cis , xq_)\n    \n\n            xq_out = torch.view_as_real(xq_* freq_cies).flatten(3)\n            \n            xk_out = torch.view_as_real(xk_*freq_cies).flatten(3)\n\n\n            return  xq_out.type_as(xq)   ,  xk_out.type_as(xq) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.811196Z","iopub.execute_input":"2025-05-07T16:24:16.811511Z","iopub.status.idle":"2025-05-07T16:24:16.836717Z","shell.execute_reply.started":"2025-05-07T16:24:16.811478Z","shell.execute_reply":"2025-05-07T16:24:16.835950Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# MultiHead & MultiQuery Attention Layer ","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention_V2(nn.Module):\n    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n        super().__init__()\n        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n        self.d_in =d_in\n        self.d_out = d_out\n        self.dropout = nn.Dropout(dropout)\n        self.num_heads  = num_heads\n        self.head_dim = d_out // num_heads\n        self.out_proj  = nn.Linear(d_out , d_out)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens , d_in = x.shape\n        keys = self.w_key(x)\n        queries  = self.w_query(x)\n        values = self.w_value(x)\n        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(2, 3)\n        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n        attn_scores.masked_fill(mask_bool , -torch.inf)\n        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n        attn_weights = self.dropout(attn_weights)\n        context_vector = (attn_weights  @ values).transpose(1, 2)\n        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n        context_vector = self.out_proj(context_vector)\n        return context_vector\n\n\n\n\ndef apply_rotary_embedding(xq:torch.Tensor , xk:torch.Tensor , freq_cies:torch.Tensor):\n\n    assert xq.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n    assert xk.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1] , -1, 2))\n\n    freq_cies = reshape_for_broadcast(freq_cies , xq_)\n\n    xq_out = torch.view_as_real(xq_ * freq_cies ).flatten(3)\n\n    xk_out = torch.view_as_real(xk_ * freq_cies).flatten(3)\n\n    return xq_out.type_as(xq) ,  xk_out.type_as(xk)\n\n\n\n\nclass MultiQueryAttentionBlock(nn.Module):\n    def __init__(self, d_model:int , h:int , dropout:float , seq_len:int , qkv_bias =  False ):\n        super().__init__()\n        self.d_model  = d_model \n\n        self.seq_len=  seq_len\n\n        assert d_model % h == 0, \"d_model is must be divided by th head\"\n        self.dropout = nn.Dropout(dropout)\n\n        self.h = h  \n\n        self.d_k = d_model // h \n\n        self.w_qkv =  nn.Linear(d_model , d_model +2 * self.d_k )\n\n        self.w_o = nn.Linear(d_model  , d_model)\n\n        freq_cies = precompute_freq_cis(dim=self.d_k , end=self.seq_len * 2 )\n\n        self.register_buffer('freq_cies' , freq_cies , persistent= False )\n\n    def generate_causal_mask(self, seq_len, device):\n        # shape: (1, 1, seq_len, seq_len)\n        return torch.tril(torch.ones((1, 1, seq_len, seq_len), device=device)).bool()\n\n    @staticmethod\n    def attention(q, k  , v,mask  , dropout):\n        d_k = q.shape[-1]\n\n        attention_score =  (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n\n        if mask is not None :\n            if mask.dim() == 2:\n                      mask = mask.unsqueeze(1).unsqueeze(2)\n            elif mask.dim() == 3:\n                     mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n        \n        attention_score = attention_score.softmax(dim=-1)\n\n        if dropout is not None :\n            attention_score = dropout(attention_score)\n\n        context_vector =  attention_score @ v\n\n        return context_vector  , attention_score\n    \n\n\n    def forward(self, q, mask= None):\n        if mask is None:\n            mask = self.generate_causal_mask(self.seq_len , device = q.device)\n        qkv =  self.w_qkv(q)\n\n        query , key, value =  torch.split(qkv , [self.d_model  , self.d_k , self.d_k], dim=-1)\n\n        query = query.view(query.shape[0] , -1 , self.h , self.d_k).transpose(1, 2)\n\n        key =  key.unsqueeze(1)\n\n        value =  value.unsqueeze(1)\n\n        seq_len =  q.size(1)\n\n        freq_cies = self.freq_cies[:query.shape[1]].to(q.device)\n\n        # freq_cies =  self.freq_cies[:seq_len].to(q.device)\n\n        query , key = apply_rotary_embedding(query , key , freq_cies)\n\n        x , self.attention_score = MultiQueryAttentionBlock.attention(q = query,k =  key,v= value ,mask=mask , dropout= self.dropout)\n\n        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h* self.d_k)\n\n        x = self.w_o(x)\n\n        return x \n    \n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.838264Z","iopub.execute_input":"2025-05-07T16:24:16.838484Z","iopub.status.idle":"2025-05-07T16:24:16.862794Z","shell.execute_reply.started":"2025-05-07T16:24:16.838455Z","shell.execute_reply":"2025-05-07T16:24:16.862086Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Memory Network ","metadata":{}},{"cell_type":"code","source":"\n\nclass EfiBioSemanticMemory_V2(nn.Module):\n    def __init__(self, input_dim:int ,semantic_memory_dim, max_slots:int = 1000 , compress_dim:int =  128 , top_k:int = 5 , num_heads:int =  4 ):\n        super().__init__()\n\n        self.input_dim = input_dim \n        self.max_slots =  max_slots \n        self.compress_dim =  compress_dim \n        self.top_k =  top_k \n        self.num_heads =  num_heads \n        self.semantic_memory_dim = semantic_memory_dim\n        # self.memory_size =  semantic_memory_dim \n\n\n\n        self.key_memory =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.value_memory = nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.cell_state =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.register_buffer('active_mask' , torch.zeros(max_slots , dtype= torch.bool))\n        self.active_mask[:semantic_memory_dim] = True  \n\n\n        # Meta data parameter \n        self.register_buffer('age', torch.zeros(max_slots))\n        self.register_buffer('usage', torch.zeros(max_slots))\n        self.register_buffer('concept_energy', torch.ones(max_slots))\n        self.register_buffer('memory_age', torch.zeros(max_slots))\n        self.register_buffer('access_count', torch.zeros(max_slots))\n        self.register_buffer(\"_memory_version\", torch.tensor(0))\n        self.concept_energy[:semantic_memory_dim] =  0.2\n\n        #stats params\n        self.register_buffer(\"step_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer('replay_count', torch.zeros(1 , dtype= torch.long))\n        self.register_buffer(\"query_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"novel_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"write_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"hit_count\", torch.zeros(1, dtype=torch.long)) \n        self.register_buffer('merge_count' , torch.zeros(1  , dtype= torch.long))\n        self.register_buffer('neuroslot_count' , torch.zeros(1, dtype= torch.long))\n        self.register_buffer('prune_count' , torch.zeros(1, dtype=torch.long))\n        self.register_buffer('consalidate_count', torch.zeros(1,dtype=torch.long))\n        self.register_buffer('update_count', torch.zeros(1,dtype=torch.long))\n\n\n        self.initial_write_step  = 300\n        \n        # Threshold Parameter \n        self.consolidation_threshold = nn.Parameter(torch.tensor(100.0))\n        self.energy_threshold = nn.Parameter(torch.tensor(0.3))\n        self.decay_rate = nn.Parameter(torch.tensor(0.999))\n        # self.novelty_threshold = nn.Parameter(torch.tensor(0.2))\n        # self.novelty_threshold = 0.2 * (1 - (self.memory_size / self.max_slots))\n        self.novelty_threshold = 0.1\n\n        self.register_buffer(\"prune_age_threshold\", torch.tensor(100))\n        self.register_buffer(\"neurogenesis_threshold\", torch.tensor(0.9))\n        self.register_buffer(\"new_slot_maturation_steps\", torch.tensor(50)) \n        self.synaptic_scale = nn.Parameter(torch.tensor(0.1))\n        self.sparsity = nn.Parameter(torch.tensor(0.5))\n        self.sim_thershold =  nn.Parameter(torch.tensor(0.5))\n        self.confidence_threshold_att = 0.15  # You can tune this\n\n        # Concept queue Params\n        \n        \n        self.register_buffer('queue_max_size' , torch.tensor(1000))\n        self.register_buffer('concept_queue' ,  torch.zeros(self.queue_max_size , self.compress_dim))\n        self.queue_ptr = 0\n        self.queue_count = 0 \n\n                # Networks \n        self.important_net = nn.Sequential(\n            nn.Linear(compress_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        \n\n        self.update_gate = nn.Sequential(\n            nn.Linear(3 * compress_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        for layer in self.update_gate:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n                nn.init.constant_(layer.bias, 0.1) \n        self.forgot_gate = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 3),\n            nn.Sigmoid()\n        )\n        self.compression = nn.Sequential(\n            nn.Linear(input_dim, semantic_memory_dim),\n            nn.RMSNorm(semantic_memory_dim),\n            nn.GELU(),\n            nn.Linear(semantic_memory_dim, self.compress_dim)\n        )\n\n        self.decompression = nn.Sequential(\n            nn.Linear(self.compress_dim, input_dim),\n        )\n\n        self.W_cell = nn.Linear(self.compress_dim, compress_dim, bias=False)\n        self.memory_projection = nn.Linear(self.compress_dim, self.input_dim)\n        # self.query_proj =  nn.Linear(self.semantic_memory_dim  , self.compress_dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=compress_dim,\n            num_heads=num_heads,\n            batch_first=False\n        )\n        self.mem_key_proj   = nn.Linear(compress_dim, semantic_memory_dim, bias=False)\n        self.mem_value_proj = nn.Linear(compress_dim, semantic_memory_dim, bias=False)\n        self.no_memory_embedding = nn.Parameter(torch.randn(1, 1, self.input_dim))\n\n        nn.init.kaiming_uniform_(self.key_memory, mode='fan_out')\n        nn.init.xavier_normal_(self.value_memory)\n        nn.init.xavier_normal_(self.cell_state)\n\n    def _get_active_memory(self):\n\n        \"\"\"\n            Get the active memries slot \n        \"\"\"\n        idx = torch.nonzero(self.active_mask, as_tuple=False).squeeze(1)\n        assert idx.numel() > 0, \"No active memory slots\"\n       \n        return  (\n            self.key_memory[idx] , \n            self.value_memory[idx], \n            self.cell_state[idx]\n\n        )\n        \n    @property \n    def active_capacity(self):\n        return torch.sum(self.active_mask).item()/ self.max_slots\n\n\n    def _adaptive_decay(self, memory_idx):\n        energy = self.concept_energy[memory_idx]\n        access = self.usage[memory_idx]\n\n        decay = torch.exp((1-self.decay_rate) * (1-energy) * (1-access))\n        return decay  \n    \n\n    def _update_memory(self, topk_idx , cells:torch.Tensor , projected:torch.Tensor , update_gates):\n        projected =  projected.unsqueeze(1).expand(-1,cells.size(1), -1)\n        active_indices =  torch.where(self.active_mask)[0]\n        new_slot_mask  = torch.isin(topk_idx, active_indices[-10:])\n        bs , _ ,_ = projected.shape\n        cell_input = cells + projected \n        cell_input =  torch.sigmoid(cell_input)\n\n        decay_factor =  self._adaptive_decay(topk_idx)\n        decay_factor =  decay_factor.unsqueeze(-1)\n        cell_updates =  decay_factor * cell_input \n        delta = update_gates.unsqueeze(-1) * cell_updates\n        cell_updates[new_slot_mask]*= 0.1\n        cell_updates = cell_updates * torch.where(new_slot_mask.unsqueeze(-1), 0.1, 1.0)\n\n        batch_size, top_num , dim = delta.shape  \n        flat_topk_idx =  topk_idx.view(-1)\n\n        flat_delta =  (delta * self.synaptic_scale).view(-1, dim)\n        flat_cells =  F.normalize(cells, dim=-1).view(-1, dim)\n\n        with torch.no_grad():\n            self.cell_state.data.index_put_(\n                (flat_topk_idx , ),  \n                flat_delta , \n                accumulate=True \n            )\n            self.key_memory.data.index_add_(0, flat_topk_idx, flat_cells)\n            self.value_memory.data.index_add_(0, flat_topk_idx, flat_cells)\n\n            self.age.data += 1\n            self.age.data[flat_topk_idx] = 0 \n\n        decay =  self.decay_rate ** self.age.unsqueeze(-1)\n        self.update_count+=1\n        assert flat_topk_idx.max() < self.key_memory.size(0), \\\n            f\"Index {flat_topk_idx.max()} >= {self.key_memory.size(0)}\"\n        with torch.no_grad():\n            self.cell_state.data =  self.cell_state * decay\n            self.key_memory.data = self.key_memory * decay \n            self.value_memory.data =  self.value_memory  * decay \n\n            self.key_memory.data[flat_topk_idx] = torch.tanh(F.normalize(self.key_memory.data[flat_topk_idx], dim=-1))\n            self.value_memory.data[flat_topk_idx] = torch.tanh(F.normalize(self.value_memory.data[flat_topk_idx], dim=-1))\n            self.cell_state.data[flat_topk_idx] = torch.tanh(self.cell_state.data[flat_topk_idx])\n\n        assert not torch.isinf(self.key_memory).any(), \"Inf in key memory\"\n    def _get_low_energy_slots(self, candidate_indices):\n        if len(candidate_indices) == 0:\n            return candidate_indices \n        candidate_energy =  self.concept_energy[candidate_indices]\n\n        sorted_indices   =  torch.argsort(candidate_energy)\n        return candidate_indices[sorted_indices]\n       \n    @property\n    def memory_size(self):\n        return self.active_mask.sum().item()\n    \n    # @torch.no_grad()\n    def _batch_update_with_old(self, indices, new_data):\n        decay = torch.sigmoid(self.concept_energy[indices]).unsqueeze(-1)\n        old_keys = self.key_memory[indices]\n        old_vals = self.value_memory[indices]\n        # new_data =  self.query_proj(new_data)\n        with torch.no_grad():\n            self.key_memory[indices] = F.normalize( old_keys *(1-decay) +   new_data * decay, dim=-1)\n            self.value_memory[indices] = F.normalize(0.35 * old_vals + 0.65 * new_data * (1 - decay), dim=-1)\n        gate_input = torch.cat([self.cell_state[indices], old_keys, new_data], dim=-1)\n        update_gate = self.update_gate(gate_input)\n        with torch.no_grad():\n            self.cell_state[indices] = F.normalize(update_gate * new_data + (1 - update_gate) * self.cell_state[indices], dim=-1)\n\n            self.usage[indices] = 0.0\n            self.age[indices] *= 0.25\n            self.memory_age[indices] = 0\n            self.concept_energy[indices] = 0.5\n            self.access_count[indices]= 0\n\n    @torch.no_grad()\n    def _batch_update_with_new(self, new_idx, new_data):\n        self.key_memory[new_idx] = new_data\n        self.value_memory[new_idx] = new_data\n        self.cell_state[new_idx] = new_data\n        self.usage[new_idx] = 0.0\n        self.age[new_idx] = 0\n        self.access_count[new_idx] = 0\n        self.memory_age[new_idx] = 0\n        self.concept_energy[new_idx] = 0.5\n        self.active_mask[new_idx] = True\n\n    \n    def _write_memory_update(self, new_concepts: torch.Tensor, retry_count=0):\n        # if retry_count ==  0 and self.query_count > 0:\n        #     self.flush_concept_queue()\n        if retry_count > 2 or new_concepts.size(0) == 0:\n            return\n        importance = self.important_net(new_concepts).squeeze(-1)\n        keep_mask = importance > 0.10\n        if not keep_mask.any():\n            return\n\n        new_concepts = new_concepts[keep_mask]\n        remaining = new_concepts.size(0)\n\n        # 1. Update low-energy active slots\n        low_energy_candidate = torch.where(self.active_mask & (self.energy_threshold > self.concept_energy))[0]\n        if low_energy_candidate.numel() > 0:\n            candidate = self._get_low_energy_slots(low_energy_candidate)\n            num_reuse = min(len(candidate), remaining)\n            if num_reuse > 0:\n                self._batch_update_with_old(indices=candidate[:num_reuse], new_data=new_concepts[:num_reuse])\n                new_concepts = new_concepts[num_reuse:]\n                remaining = new_concepts.size(0)\n\n        # 2. Write to new slots\n        if remaining > 0 and self.memory_size < self.max_slots:\n            add = min(remaining, self.max_slots - self.memory_size)\n            new_idx = self.neurogenesis(return_index=True, required_slots=add)\n            if new_idx is not None:\n                self._batch_update_with_new(new_idx, new_concepts[:add])\n            # self.memory_size += add\n                new_concepts = new_concepts[add:]\n                remaining = new_concepts.size(0)\n\n        if remaining == 0:\n            return\n        prev_active = self.active_mask.sum()\n        self._optimize_memory()\n        if self.active_mask.sum() > prev_active and retry_count < 2:\n            \n            self._write_memory_update(new_concepts, retry_count + 1)\n        else:\n            self._enqueue_to_queue_buffer(new_concepts)\n\n        assert self.memory_size <= self.max_slots\n        assert torch.all(self.active_mask[:self.memory_size])\n\n    @torch.no_grad()\n    def flush_concept_queue(self):\n        if self.query_count > 0:\n            concepts = self.concept_queue[:self.queue_count]\n            self._write_memory_update(concepts)\n            self.queue_count = 0\n            self.queue_ptr = 0\n           \n\n    @torch.no_grad()\n    def _enqueue(self, data):\n        if data.size(0) == 0:\n            return\n\n        capacity = self.concept_queue.size(0)\n        avail = capacity - self.queue_count\n        to_add = data[:avail]\n        if to_add.size(0) == 0:\n            return\n\n        start = self.queue_ptr\n        end = (start + to_add.size(0)) % self.queue_max_size  \n        if end <= capacity:\n\n            self.concept_queue[start:end] = to_add\n        else:\n            split = capacity - start\n            self.concept_queue[start:] = to_add[:split]\n            self.concept_queue[:end % capacity] = to_add[split:]\n\n        self.queue_ptr = end % capacity\n        self.queue_count = min(self.queue_count + to_add.size(0), capacity)\n\n    @torch.no_grad()\n    def _enqueue_to_queue_buffer(self, new_concepts):\n        to_enqueue = new_concepts.size(0)\n        if to_enqueue == 0:\n            return\n\n        capacity = self.queue_max_size.item()\n        current = self.queue_count\n        overflow = max(0, current + to_enqueue - capacity)\n        important =  self.important_net(new_concepts).squeeze(-1)\n        keep_mask = important > 0.65\n        new_concepts = new_concepts[keep_mask]\n        if new_concepts.size(0) == 0:\n            return \n        if overflow >= new_concepts.size(0):\n            self._enqueue(new_concepts[-capacity:])\n            return  \n\n        kept_new = new_concepts[-(to_enqueue - overflow):]\n        self._enqueue(kept_new)\n\n    @torch.no_grad()\n    def _optimize_memory(self , aggressive = False):\n        if aggressive:\n            self._consolidate_important_memories()\n            self._merge_similar_slots()\n            self._prune_memories()\n            self._prune_slots()\n            self.neurogenesis()\n        else:\n            self._consolidate_important_memories()\n            self._prune_memories()\n            self.neurogenesis()\n\n\n        \n\n    def _update_energy_level(self):\n        key_memory = self.mem_key_proj(self.key_memory)\n        importance = self.important_net(self.key_memory).squeeze()\n        assert torch.all(self.concept_energy >= 0)\n        assert torch.all(self.concept_energy <= 1.01)  \n        new_energy =  (\n            self.decay_rate * self.concept_energy + 0.3 * self.usage + 0.1 * importance *(1-self.concept_energy)\n                    )\n        \n        deactivated  = ~self.active_mask \n\n        valid_deactivate =  deactivated.nonzero().squeeze()\n        valid_deactivate = valid_deactivate[(valid_deactivate>=0)&(valid_deactivate < self.memory_size)]\n        with torch.no_grad():\n            self.concept_energy.data =  torch.clamp(new_energy  , 0, 1)\n\n            if valid_deactivate.numel() >0:\n                self.key_memory.data[valid_deactivate] *=0.01\n                self.value_memory.data[valid_deactivate]*=0.01\n                self.cell_state.data[valid_deactivate]*=0.1\n\n        self.active_concepts =  torch.sum(self.active_mask).clamp(min=0 , max=self.memory_size)\n    \n    def forward(self ,x:torch.Tensor , training:bool = True):\n        if training:\n           x=  self.replay_consolidation(x=x)\n        batch_size  , seq_len , _  = x.shape \n        self.step_count += 1\n        self.query_count += batch_size \n        compressed = self.compression(x.mean(dim=1))\n        query = self.W_cell(compressed)\n        k_active , v_active , c_active =  self._get_active_memory()\n        k  = k_active.unsqueeze(1).expand(-1 , batch_size ,-1)\n        v = v_active.unsqueeze(1).expand(-1, batch_size , -1)\n        assert k.size(1) == batch_size\n        # attn_output , attn_weights = self.attn(\n        #         query.unsqueeze(0), k ,v , need_weights =  True \n        #     )\n\n        # Cosine similarity instead of MHA\n        sims =  F.cosine_similarity(query.unsqueeze(1),\n                                     k_active.unsqueeze(0) , dim=-1)\n        print(\"   preâ€‘write mean/sd:\", sims.mean().item(), sims.std().item())\n        top_vals , local_topk = sims.topk(self.top_k , dim=-1)\n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0] \n        topk_idx = active_indices[local_topk]\n        attn_weights = torch.zeros(1, batch_size, k_active.size(0), device=query.device)\n        attn_weights[0].scatter_(1, local_topk, 1.0)\n        v_exp    = v_active.unsqueeze(0).expand(batch_size, -1, -1)  \n        gathered = torch.gather(\n            v_exp,\n            1,\n            local_topk.unsqueeze(-1).expand(-1, -1, self.compress_dim)\n        )                                                             \n        retrieved = gathered.mean(dim=1)                            \n\n        attn_output = retrieved.unsqueeze(0)                        \n\n         \n\n        max_scores , _  = sims.max(dim = -1)\n        if self.query_count < self.initial_write_step:\n            hit_thershold = 1.0\n        else:\n            hit_thershold=  0.56\n        hit_thershold =  0.3\n        # query_proj = self.query_proj(query)    \n        # hit_threshold = 0.3 + 0.2 * (self.memory_size / self.max_slots)\n        max_scores = max_scores.squeeze(-1)\n        hits = (max_scores>hit_thershold).sum()\n        self.hit_count +=  hits\n        # # self.novelty_threshold = torch.clamp(\n        #     torch.tensor(0.4 - 0.3 * (self.memory_size / self.max_slots)), \n        #     min=0.1, \n        #     max=0.5\n        # )     \n        #   \n        novel_mask =  max_scores <  hit_thershold\n        \n        self.novel_count+= novel_mask.sum()\n        if novel_mask.any():\n            novel_projection =  query[novel_mask]\n            sim_scores =  F.cosine_similarity(novel_projection.unsqueeze(1) , k_active.unsqueeze(0) , dim=-1)\n            is_novel =  sim_scores.max(dim=-1).values< (self.sim_thershold - 0.2 * (self.memory_size/self.max_slots))\n            write_mask = is_novel\n            if write_mask.any():\n                if self.query_count >  0:\n                    self.flush_concept_queue()\n                new_concepts =  novel_projection[write_mask].detach()\n                self.write_count += new_concepts.size(0)\n                assert new_concepts.size(0) <= self.max_slots - self.memory_size \n                \"Exceeding maximum memory capacity\"\n                self._write_memory_update(new_concepts=new_concepts)\n                no_memory_out =  self.no_memory_embedding.repeat(batch_size , seq_len, 1)\n                return no_memory_out  , self.no_memory_embedding.squeeze(0), torch.tensor([], dtype= torch.long) , None \n        # attn_output = attn_output + torch.randn_like(attn_output) * 0.1\n        \n\n        with torch.no_grad():\n            self.usage *= 0.95\n            self.usage[topk_idx] +=0.1\n            self.usage.clamp(0,1)\n            self.usage.mul_(0.9)\n            self.usage.scatter_add_(0, topk_idx.flatten(), torch.ones_like(topk_idx, dtype=torch.float).flatten())\n            self.usage.clamp_(max=1.0)\n            # self.concept_energy[topk_idx] += 0.15 * max_scores.squeeze()\n            # self.concept_energy.clamp_(max=1.0)\n        if training:\n            self._update_energy_level()\n            self._update_thersholds()\n        keys= self.key_memory[topk_idx]\n        value = self.value_memory[topk_idx]\n        cells = self.cell_state[topk_idx]\n\n             \n        gate_input = torch.cat([\n            keys, cells, query.unsqueeze(1).expand(-1, self.top_k , -1)\n        ], dim= -1) \n        update_gates =  self.update_gate(gate_input.view(-1, 3 *self.compress_dim))\n        update_gates = update_gates.view(batch_size, self.top_k)\n        self._update_memory(topk_idx=topk_idx, cells=cells ,projected=query, update_gates=update_gates)\n        # Project the output to the out \n        out =  self.memory_projection(retrieved)\n        out = out.unsqueeze(1).repeat(1, seq_len, 1)\n        self._memory_version +=1 \n        self._update_memory_metadata(topk_idx)\n        return  out , retrieved , topk_idx , attn_weights\n\n\n    @torch.no_grad()\n    def _gradual_influence_increase(self):\n        \"\"\"\n        Gradually increase the influence of newly added memory slots based on their age and access.\n        \"\"\"\n        new_slots_mask  = (self.age <= self.new_slot_maturation_steps) & self.active_mask \n        if not torch.any(new_slots_mask):\n            return  \n        \n        age_normalized = self.age[new_slots_mask] / self.new_slot_maturation_steps\n        usage_normalized =  self.usage[new_slots_mask]\n\n        growth_factor =torch.sigmoid((age_normalized + usage_normalized) * 3 ).unsqueeze(-1)\n\n        self.key_memory[new_slots_mask] = F.normalize(self.key_memory[new_slots_mask] * (1 + growth_factor * 0.5),\n        dim=-1)\n        self.value_memory[new_slots_mask] =  F.normalize(self.value_memory[new_slots_mask]* (1+growth_factor * 0.3) ,dim=-1 )\n\n        energy_boost = torch.clamp(0.1 * growth_factor.squeeze(), max=0.15)\n        self.concept_energy.data[new_slots_mask] = torch.clamp(\n        self.concept_energy[new_slots_mask] + energy_boost,\n        min=0.3,\n        max=0.7\n    )\n\n        self.age.data[new_slots_mask] += 1 \n\n        \n    def _consolidate_important_memories(self):\n        key_memory =  self.mem_key_proj(self.key_memory)\n        importance =  self.important_net(self.key_memory).squeeze()\n        consolidate_mask  = importance > 0.1\n        if consolidate_mask.any():\n          with torch.no_grad():\n            self.key_memory[consolidate_mask] = F.normalize(\n                self.key_memory[consolidate_mask] , dim=-1\n            )\n            mean_value = self.value_memory[consolidate_mask].mean(dim=0)\n           \n            self.value_memory[consolidate_mask] = (\n                    0.9 * self.value_memory[consolidate_mask] +\n                    0.1 * mean_value\n                )\n            self.concept_energy[consolidate_mask] = torch.clamp(self.concept_energy[consolidate_mask] + 0.05, 0, 1)\n            self.concept_energy[~consolidate_mask] *= 0.85\n            self.consalidate_count +=1 \n\n    @torch.no_grad()\n    def _prune_memories(self):\n        prune_condidate =  ((self.age > self.prune_age_threshold) & (self.usage < 0.01) & (self.concept_energy < 0.4))\n        if prune_condidate.any():\n            self.key_memory.data[prune_condidate] *=  0.1\n            self.value_memory.data[prune_condidate]*=0.01\n            self.cell_state.data[prune_condidate] *= 0.01\n            self.age.data[prune_condidate] = 0 \n            self.usage[prune_condidate] = 0 \n            self.concept_energy.data[prune_condidate] = 0.1\n            self.prune_count +=1 \n            self._memory_version += prune_condidate.sum().item()\n\n    @torch.no_grad()\n    def _prune_slots(self):\n            mask = self.age > self.prune_age_threshold\n            self.key_memory.data[mask] *= 0.01\n            self.value_memory.data[mask] *= 0.01\n            self.cell_state.data[mask] *= 0.01\n            self.concept_energy.data[mask] = 0\n            self.usage[mask]= 0 \n            self.age.data[mask] = 0 \n            self.prune_count +=1 \n\n\n\n    def replay_consolidation(self, x: torch.Tensor):\n\n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0] \n        active_key , active_value, _ = self._get_active_memory()\n        if  self.training and random.random() < 0.2: \n            high_energy_mask = self.concept_energy[active_indices] > 0.8\n            if high_energy_mask.sum() == 0:\n                return x \n            if high_energy_mask.sum() > 0:\n                replay_keys = active_key[high_energy_mask]\n                replay_values = active_value[high_energy_mask]\n                \n                replay_input = self.decompression(replay_values.mean(dim=0, keepdim=True))\n                B, T , D =  x.shape\n                self.replay_count+= 1\n                return replay_input.unsqueeze(1).expand(B,T,D)\n        return x\n\n    def get_reusable_slots(self ,num_needed:int):\n       \n\n        age_score =  1-torch.sigmoid(self.age / 100) # old age \n        energy_score = (1-  self.concept_energy ) *2 \n        usage_score = 1 - self.usage \n        reuse_scores = (0.4 * energy_score  + 0.3 * age_score + 0.3 * usage_score \n                       )\n\n        mask =(self.concept_energy  < self.energy_threshold) & (self.age< 100)\n        reuse_scores[~mask]= -float('inf')\n\n        topk_scores  , candidates = torch.topk(reuse_scores, min(num_needed, self.memory_size))\n        return candidates \n\n            \n    def _reinitialize_slot(self, idx):\n        \"\"\"Reset a slot to initial state\"\"\"\n        with torch.no_grad():\n            scale = 0.1 + 0.05 * torch.rand(1, device=idx.device)\n            self.key_memory[idx] = torch.randn_like(self.key_memory[idx]) * scale\n            self.value_memory[idx] = torch.randn_like(self.value_memory[idx]) * scale\n            self.cell_state[idx] =  0.2 * self.cell_state.data[idx].mean(dim=0)\n            \n            # Reset metadata\n            self.concept_energy[idx] = 0.3 + 0.2 * torch.rand_like(self.concept_energy[idx])\n            self.usage[idx] =  0.1 * torch.rand_like(self.usage[idx])\n            self.age[idx] = 0\n            self.memory_age[idx] = 0\n            self.access_count[idx] = 0\n            self.active_mask[idx] = True \n\n\n\n\n\n    def _consalidate_new_slots(self):\n        new_slot_indices =  torch.arange(self.memory_size - 10 , self.memory_size )\n        new_slot_energy = self.concept_energy[new_slot_indices]\n        with torch.no_grad():\n            self.concept_energy[new_slot_indices] = torch.clamp(\n                new_slot_energy + 0.1 * self.age[new_slot_indices] , 0 ,1\n            )\n            self.key_memory[new_slot_indices] *=  0.1\n            self.value_memory[new_slot_indices] *= 0.1\n            self.age[new_slot_indices] +=1\n            self.access_count[new_slot_indices] +=1 \n\n    def _update_memory_metadata(self, used_indices):\n       \n        self.access_count[used_indices] += 1\n        \n        self.memory_age += 1\n        self.memory_age[used_indices] = 0\n        with torch.no_grad():\n            self.concept_energy[used_indices] += 0.1\n            self.concept_energy= torch.clamp(self.concept_energy * 0.95, 0, 1)\n            self.age[used_indices] -= 5 \n    @torch.no_grad()\n    def neurogenesis(self, required_slots:int= 10 , return_index = False):\n        device = self.key_memory.device\n        if self.max_slots > self.memory_size:\n\n            usage_rate =  (self.usage > 0.1).float().mean()\n            if usage_rate > self.neurogenesis_threshold:\n                reusable =  self.get_reusable_slots()\n                num_reuse =  min(reusable.numel() , required_slots)\n                reused_indices = reusable[:num_reuse]\n                if num_reuse > 0:\n                    with torch.no_grad():\n                        device =  self.key_memory.device \n                        self._reinitialize_slot(idx=reused_indices)\n                        \n                new_slots =  min(max(0 ,    required_slots -  num_reuse ), self.max_slots- self.memory_size)\n                if new_slots > 0:\n                    start_idx =self.memory_size\n                    end_idx =  start_idx  + new_slots \n                    new_indices  = torch.arange(start_idx , end_idx, device = device )\n                  \n                   \n                    self.key_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.value_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.cell_state.data[new_indices] = 0\n                    self.concept_energy.data[new_indices] = 0.5\n                    self.usage.data[new_indices] = 0.0\n                    self.age.data[new_indices] = 0.0\n                    self.access_count.data[new_indices] = 0.0\n                    self.active_mask.data[new_indices] = True\n                    self.neuroslot_count +=1 \n                    self._gradual_influence_increase()\n                    self._memory_version += new_slots\n\n                \n                    \n    \n       \n                if return_index:\n                    return torch.cat([reused_indices, new_indices]) if new_indices.numel() > 0 else reused_indices\n            \n        elif return_index:\n            return  torch.empty(0, dtype=torch.long, device=self.key_memory.device) \n\n    def emergency_recovery(self):\n        # Reset unstable memories\n        unstable = self.concept_energy < 0.2\n        self._reinitialize_slot(unstable)\n        \n        self._optimize_memory(aggressive=True)\n    def _protect_critical_memories(self):\n            # Protect top 10% of important memories\n            importance = self.important_net(self.key_memory).squeeze()\n            topk = importance.topk(int(self.max_slots * 0.1)).indices\n            self.concept_energy[topk] = 1.0\n            self.age[topk] -= 10\n    @torch.no_grad()\n    def _merge_similar_slots(self, top_k: int = 32):\n        device = self.key_memory.device\n        active_idx = torch.nonzero(self.active_mask, as_tuple=True)[0]\n        N = active_idx.size(0)\n        if N < 2:\n            return\n\n        # 1. Normalized vectors\n        keys = F.normalize(self.key_memory[active_idx], dim=-1)\n        values = F.normalize(self.value_memory[active_idx], dim=-1)\n        D = keys.size(-1)\n\n        # 2. Similarity search\n        K = min(top_k, N-1)\n        sims, nbrs = torch.topk(keys @ keys.T, k=K+1, dim=-1)\n        sims, nbrs = sims[:, 1:], nbrs[:, 1:]  # Remove self\n\n        # 3. Dynamic threshold\n        pressure = torch.tensor(N / self.max_slots, device=device)\n        threshold = (0.9 - 0.4 * pressure).clamp(0.65, 0.9)\n        mask = sims > threshold\n\n        # 4. Graph construction\n        row = torch.arange(N, device=device).unsqueeze(1).expand(-1, K)[mask]\n        col = nbrs[mask]\n        edges = torch.stack([\n            torch.cat([row, col]),\n            torch.cat([col, row])\n        ])\n\n        # 5. Label propagation\n        labels = torch.arange(N, device=device)\n        for _ in range(3):\n            neighbor_labels = labels[edges[1]]\n            updates = torch.minimum(labels[edges[0]], neighbor_labels)\n            labels.scatter_reduce_(0, edges[0], updates, reduce='amin')  # Fixed reduction\n\n        # 6. Cluster analysis\n        uniq, inv, counts = torch.unique(labels, return_inverse=True, return_counts=True)\n        cluster_mask = counts >= 2\n        big_clusters = uniq[cluster_mask]\n        big_counts = counts[cluster_mask]\n        num_clust = big_clusters.size(0)\n        if num_clust == 0:\n            return\n\n        # 7. Cluster mapping\n        cluster_id_map = torch.zeros(uniq.max()+1, dtype=torch.long, device=device)\n        cluster_id_map[big_clusters] = torch.arange(num_clust, device=device)\n        member_mask = torch.isin(inv, big_clusters)\n        global_idx = active_idx[member_mask]\n        cluster_ids = cluster_id_map[inv[member_mask]]  # Proper mapping\n        # 8. Energy aggregation\n        energy = self.concept_energy[global_idx]\n        weights = (energy / big_counts[cluster_ids].float()).unsqueeze(-1)\n        expanded_ids = cluster_ids.unsqueeze(-1).expand(-1, D)\n        # 8.1 Weighted sum of the seleceted slots datat \n        new_keys = torch.zeros((num_clust, D), device=device)\n        new_vals = torch.zeros_like(new_keys)\n        new_keys.scatter_add_(0, expanded_ids, keys[member_mask] * weights)\n        new_vals.scatter_add_(0, expanded_ids, values[member_mask] * weights)\n\n        # 9. Representative selection\n        cluster_ages = self.age[global_idx]\n        min_ages = torch.zeros(num_clust, device=device)\n        min_ages.scatter_reduce_(0, cluster_ids, cluster_ages, reduce='amin', include_self=False)\n        \n        # Find first occurrence of min age\n        _, sorted_idx = torch.sort(cluster_ids)\n        cluster_ids_sorted = cluster_ids[sorted_idx]\n        age_mask = (cluster_ages[sorted_idx] == min_ages[cluster_ids_sorted])\n        _, first_occurrence = torch.unique_consecutive(cluster_ids_sorted, return_inverse=True)\n        rep_mask = age_mask & (first_occurrence == 0)\n        rep_cluster_ids = cluster_ids_sorted[rep_mask]\n        representatives = global_idx[sorted_idx][rep_mask]\n    \n        # 10. Memory updates\n        self.key_memory[representatives] = F.normalize(new_keys[cluster_ids_sorted[rep_mask]], dim=-1)\n        self.value_memory[representatives] = F.normalize(new_vals[cluster_ids_sorted[rep_mask]], dim=-1)\n        \n        clust_energy = torch.bincount(cluster_ids, weights=energy, minlength=num_clust)\n        # self.concept_energy[representatives] = clust_energy[rep_cluster_ids].clamp(min=1e-5, max=1.0)\n        self.concept_energy[representatives] = torch.clamp(\n            clust_energy[rep_cluster_ids] * 1.2,  \n            min=0.7, \n            max=1.0\n        )\n     \n        self.merge_count+= 1 \n        self._memory_version += num_clust\n        # 11. Usage update\n        per_cluster_usage = torch.bincount(\n            cluster_ids,\n            weights=self.usage[global_idx],\n            minlength=num_clust\n        ).float() / big_counts.float() \n        # self.usage[representatives] = per_cluster_usage[rep_cluster_ids]\n        self.usage[representatives] = torch.clamp(\n            per_cluster_usage[rep_cluster_ids] * 1.5,\n            min=0.3,\n            max=1.0\n        )\n\n        # 12. Deactivation\n        active_mask_modified = torch.zeros_like(self.active_mask)\n        active_mask_modified[representatives] = True\n        deactivate_idx = member_mask & ~active_mask_modified[active_idx]\n        \n        if deactivate_idx.any():\n            to_deactivate = active_idx[deactivate_idx]\n            self.concept_energy[to_deactivate] *= 0.1\n            self.key_memory[to_deactivate] *= 0.1\n            self.value_memory[to_deactivate] *= 0.1\n            self.usage[to_deactivate] *= 0.25\n\n        self._consolidate_important_memories()\n        self._update_memory_metadata(representatives)\n            \n    @torch.no_grad()\n    def _update_thersholds(self , momentum:float=0.9 ):\n        hit_rate =  float(self.hit_count / max(self.query_count, 1))\n        write_rate =  float(self.write_count  / max(self.query_count , 1))\n        novely_rate =  float(self.novel_count / max(self.query_count  , 1))\n\n        util =  float(self.active_capacity)\n\n        new_nov =   (0.2 * (1-util) + 0.1 * novely_rate)\n        self.novelty_threshold =    momentum * self.novelty_threshold + (1-momentum) * new_nov\n\n        new_enger_thr = 0.3 + 0.3 *(1-hit_rate)\n        self.energy_threshold.data  = momentum * self.energy_threshold + (1-momentum) * new_enger_thr \n        \n        new_consal = 50.0 +50.0 * write_rate \n        self.consolidation_threshold.data.mul_(momentum).add_(new_consal * (1-momentum))\n\n        new_decay = 0.995 + 0.003 * (1-util)\n        self.decay_rate.data.mul_(momentum).add_(new_decay *(1-momentum))\n\n\n        new_prune_age = 100 * (1- util ) + 20 * util \n        self.prune_age_threshold.fill_(momentum * self.prune_age_threshold+(1-momentum) * new_prune_age)\n\n        new_neuro = 0.8 + 0.1 * write_rate - 0.1 * util\n        self.neurogenesis_threshold.fill_(momentum * self.neurogenesis_threshold + (1-momentum) * new_neuro)\n\n        new_mat = 50 + 50 * write_rate\n        self.new_slot_maturation_steps.fill_(momentum * self.new_slot_maturation_steps + (1-momentum) * new_mat)\n\n        new_scale = 0.05 + 0.2 * (1 - hit_rate)\n        self.synaptic_scale.data.mul_(momentum).add_(new_scale * (1-momentum))\n\n        new_sp = 0.5 + 0.3 * util\n        self.sparsity.data.mul_(momentum).add_(new_sp * (1-momentum))\n\n        new_sim = 0.5 - 0.2 * novely_rate\n        self.sim_thershold.data.mul_(momentum).add_(new_sim * (1-momentum))\n\n\n\n\n        \n    def get_memory_metrics(self):\n\n        \"Return memory health and retivel param details\"\n        active_mask  =  self.concept_energy > self.energy_threshold\n        energy  =  self.concept_energy \n        usage =  self.usage \n        access =  self.access_count\n\n        age_hist   = torch.histc(self.memory_age.float(), bins=10, min=0, max=float(self.memory_age.max()))\n        usage_hist = torch.histc(usage, bins=10, min=0, max=1.0)\n        access_hist= torch.histc(access.float(), bins=10, min=0, max=float(access.max()))\n        active_concepts = self.active_mask &(self.concept_energy > 0.70)\n\n        return  {\n\n            #________Memory Health _______________________________\n            'memory_size': self.memory_size  , \n            'active_concepts':self.active_mask.sum().item(),\n            'active_concepts_with_high_energy':active_concepts.sum().item(),\n            'utilization':active_mask.sum().item() / self.memory_size , \n            'energy_mean':energy.mean().item(), \n            'energy_std':energy.std().item(), \n            'age_histogram': age_hist, \n            'usage_histogram':usage_hist, \n            'access_histogram':access_hist, \n            # 'merge_rate':(energy < 0.3).sum().item() / self.memory_size ,\n            'merge_rate':self.merge_count.item() / max(1 , self.step_count.item()),\n            # \"prune_rate\":((self.age > self.prune_age_threshold) & (usage < 0.01)).float().mean().item(),            \n            'prune_rate':self.prune_count.item() / max(1 , self.step_count.item()),\n            'neuro_rate':(self.memory_age < 10).float().mean().item(), \n            \"reuse_efficiency\":      access[energy > 0.5].float().mean().item(),\n\n             # â€”â€” retrieval/write stats â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n            \"steps\":                 self.step_count.item(),\n            \"queries\":               self.query_count.item(),\n            \"novelty_rate\":          self.novel_count.item() / max(1, self.query_count.item()),\n            \"write_rate\":            self.write_count.item() / max(1, self.query_count.item()),\n            \"hit_rate\":              self.hit_count.item() / max(1, self.query_count.item()),\n            'merge_count':self.merge_count.item() , \n            'neuroslot_count':self.neuroslot_count.item(), \n            'prune_count':self.prune_count.item(), \n            'consalidate_count':self.consalidate_count.item(),\n            'hit_count':self.hit_count.item() , \n            'write_count':self.write_count.item(),\n            'memory_version':self._memory_version.item(),\n            'access_count':self.access_count[self.active_mask].tolist(), \n            'replay_count':self.replay_count.item(),\n            'access_count_sum': self.access_count[self.active_mask].sum().item(),\n\n                'access_count_mean': self.access_count[self.active_mask].mean().item(),\n            'update_count':self.update_count.item()\n            \n        }\n\n\n        \n    def model_save(self , path):\n            torch.save({\n\n                'key_memory':self.key_memory.data.cpu(), \n                'value_memory':self.value_memory.data.cpu() , \n                'cell_state':self.cell_state.data.cpu(), \n                'active_mask':self.active_mask.cpu(),\n                'age':self.age.data.cpu() , \n                'usage':self.usage.data.cpu(), \n                'access_count':self.access_count.data.cpu(), \n                'memory_version':self._memory_version.data.cpu(), \n                'memory_age':self.memory_age.data.cpu(), \n                'concept_queue':self.concept_queue.data.cpu(), \n                'queue_ptr':self.queue_ptr , \n                'queue_count':self.queue_count , \n                'query_count':self.query_count , \n                'step_count':self.step_count , \n                'novel_count':self.novel_count , \n                'write_count':self.write_count , \n                'hit_count':self.hit_count , \n                'merge_count':self.merge_count , \n                'neuroslot_count':self.neuroslot_count , \n                'prune_count':self.prune_count , \n                'consalidate_count':self.consalidate_count ,\n                'memory_size':self.memory_size,\n                'replay_count':self.replay_count\n\n             } , path)\n            \n    def model_load(self, path, map_location=None):\n        state = torch.load(path, map_location=map_location)\n        self.key_memory.data.copy_(state['key_memory'])\n        self.value_memory.data.copy_(state['value_memory'])\n        self.cell_state.data.copy_(state['cell_state'])\n        self.active_mask.data.copy_(state['active_mask'])\n        self.age.data.copy_(state['age'])\n        self.usage.data.copy_(state['usage'])\n        self.access_count.data.copy_(state['access_count'])\n        self.memory_age.data.copy_(state['memory_age'])\n        self.concept_queue.data.copy_(state['concept_queue'])\n        self.queue_ptr = state.get('queue_ptr', 0)\n        self.queue_count = state.get('queue_count', 0)\n        self._memory_version.data.copy_(\n            state.get('memory_version', torch.tensor(0))\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:16.863895Z","iopub.execute_input":"2025-05-07T16:24:16.864108Z","iopub.status.idle":"2025-05-07T16:24:16.952274Z","shell.execute_reply.started":"2025-05-07T16:24:16.864090Z","shell.execute_reply":"2025-05-07T16:24:16.951278Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Transformer Block","metadata":{}},{"cell_type":"code","source":"\n\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention_V2(\n        d_in=cfg[\"emb_dim\"],\n        d_out=cfg[\"emb_dim\"],\n        context_length=cfg[\"context_length\"],\n        num_heads=cfg[\"n_heads\"],\n        dropout=cfg[\"drop_rate\"],\n        qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n    #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut # Add the original input back\n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut #C\n        return x\n\n\n\nclass TransformerBlock_v2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention =  MultiQueryAttentionBlock(d_model=cfg['emb_dim'], h=cfg['n_heads'] , dropout=cfg['drop_rate'], seq_len=  cfg['context_length'] ,qkv_bias=cfg['qkv_bias'])\n\n        self.feed_forward = FeedForward(cfg)\n\n        self.layernorm1 =  LayerNorm(cfg['emb_dim'])\n    \n        self.layernorm2 =  LayerNorm(cfg['emb_dim'])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x , mask= None):\n\n        attention_output =  self.attention(self.layernorm1(x) , mask =  mask)\n\n        ff_output =  self.feed_forward(self.layernorm2(x))\n\n        return x + self.drop_out(ff_output) + self.drop_out(attention_output)\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.memory = MemorySystem(cfg=cfg)\n        self.feed_forward = FeedForward(cfg=cfg)\n        \n        self.norm1 = LayerNorm(cfg['emb_dim'])\n        self.norm2 = LayerNorm(cfg['emb_dim'])\n        self.norm3 = LayerNorm(cfg['emb_dim'])\n        \n        # Memory gate\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] * 2, cfg['emb_dim']),\n            nn.Sigmoid()\n        )\n        \n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        attn_out = self.attention(self.norm1(x), mask=mask)\n        x = x + self.dropout(attn_out)\n        \n        norm_x = self.norm2(x)\n        epic_out , semantic_out , memory_out = self.memory(norm_x)\n        \n        gate_input = torch.cat([norm_x, memory_out], dim=-1)\n        memory_gate = self.memory_gate(gate_input)\n        x = x + memory_gate * memory_out\n        \n        ff_out = self.feed_forward(self.norm3(x))\n        x = x + self.dropout(ff_out)\n        \n        return x\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg, shared_memory=None):\n        super().__init__()\n        # Core components\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.ffn = FeedForward(cfg=cfg)\n        \n        # Memory system (shared across blocks)\n        self.memory = shared_memory or MemorySystem(cfg=cfg)\n        \n        # Normalization layers\n        self.pre_ln_attn = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_mem = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_ffn = RMSNorm(cfg['emb_dim'])\n        \n        # Adaptive memory gating\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'], 1),\n            nn.Sigmoid()\n        )\n        \n        # Memory residual weights\n        self.mem_alpha = nn.Parameter(torch.tensor(0.5))\n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        # Attention phase\n        resid = x\n        x = self.pre_ln_attn(x)\n        x = resid + self.dropout(self.attention(x, mask=mask))\n        \n        # Memory phase\n        resid_mem = x\n        x_mem = self.pre_ln_mem(x)\n        print('x shape ', x.shape)\n        _, _, memory_out = self.memory(x_mem)\n        \n        # Adaptive gating\n        gate = self.memory_gate(x_mem)\n        x = resid_mem + self.mem_alpha * gate * memory_out\n        \n        # FFN phase\n        resid_ffn = x\n        x = self.pre_ln_ffn(x)\n        x = resid_ffn + self.dropout(self.ffn(x))\n        \n        return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:17.567771Z","iopub.execute_input":"2025-05-07T16:24:17.568173Z","iopub.status.idle":"2025-05-07T16:24:17.585817Z","shell.execute_reply.started":"2025-05-07T16:24:17.568141Z","shell.execute_reply":"2025-05-07T16:24:17.584758Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# GPTQModel","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int, embdding_layer: nn.Embedding):\n        super().__init__()\n        self.weight = embdding_layer.weight  # share weights with input embedding\n        self.bias = nn.Parameter(torch.zeros(vocab_size))  # learnable bias\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\n\n\n\nclass GPTMQModel2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim = cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel1(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks =  nn.ModuleList([\n            TransformerBlockWithMemory(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n        \n        # Shared memory system across layers\n        self.shared_memory = MemorySystem(cfg=cfg)\n        \n        # Transformer blocks with shared memory\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlockWithMemory(\n                cfg=cfg,\n                shared_memory=self.shared_memory if cfg['share_memory'] else None\n            ) for _ in range(cfg['n_layers'])\n        ])\n        \n        # Final projections\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.projection = ProjectionLayer(\n            cfg['emb_dim'], \n            cfg['vocab_size'], \n            self.embedding.embeddings\n        )\n        self.memory_retention_alpha = nn.Parameter(torch.tensor(0.9))\n\n        # Memory loss coefficient\n        self.mem_loss_coef = cfg.get('mem_loss_coef', 0.3)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n        \n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)\n            x = self.memory_retention_alpha * x + (1 - self.memory_retention_alpha) * x.detach()\n            \n        x = self.final_norm(x)\n        logits = self.projection(x)\n        \n        return logits\n    \n    def get_memory_loss(self):\n        \"\"\"Get combined memory regularization loss\"\"\"\n        return self.mem_loss_coef * self.shared_memory.memory_loss()\n    \n    def transformer_parameters(self):\n        return [p for n, p in self.named_parameters() if 'transformer_blocks' in n and p.requires_grad]\n    \n    def memory_parameters(self):\n        return [p for n, p in self.named_parameters() if 'memory_modules' in n and p.requires_grad]\n    \n    def embedding_parameters(self):\n        return [p for n, p in self.named_parameters() if 'embedding' in n and p.requires_grad]\n    \n    def norm_parameters(self):\n        return [p for n, p in self.named_parameters() if 'normalization' in n and p.requires_grad]\n    \n    def output_parameters(self):\n        return [p for n, p in self.named_parameters() if 'output_projection' in n and p.requires_grad]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:17.914860Z","iopub.execute_input":"2025-05-07T16:24:17.915164Z","iopub.status.idle":"2025-05-07T16:24:17.929964Z","shell.execute_reply.started":"2025-05-07T16:24:17.915141Z","shell.execute_reply":"2025-05-07T16:24:17.929028Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\nclass GPTMemoryEnhanced(nn.Module):\n    def __init__(self,cfg):\n        super().__init__()\n        self.embedding =  InputEmbedding(cfg['vocab_size'] , cfg['emb_dim'])\n        self.memory_proj = nn.Linear(cfg['emb_dim'], cfg['memory_dim'])\n        self.memory_expander = nn.Linear(cfg['memory_dim'], cfg['emb_dim'])\n        self.transformer_block = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n        self.dropout =  nn.Dropout(cfg['drop_rate'])\n        self.memory =  EfiBioSemanticMemory_V2(input_dim=cfg['memory_dim'] ,semantic_memory_dim=cfg['memory_dim'],num_heads=2)\n\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection =  ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'] , self.embedding.embeddings)\n\n    def forward(self,input_tokens:torch.Tensor , mask =None):\n        x =  self.embedding(input_tokens)\n        x_emb = x \n        for block in self.transformer_block:\n            x  = block(x, mask = mask)\n        memory_query = self.memory_proj(x) \n        mem_out, retrieved, topk_idx, attn_w = self.memory(memory_query)\n        # memory_out, _ ,_ =  self.memory(x.las_hidden_state.mean(1))\n        mem_out = self.memory_expander(mem_out)\n        fused  = x + mem_out\n\n        fused =  self.final_norm(fused)\n\n        logits =  self.projection(fused)\n        # return logits, {\n        #     \"memory_topk\": topk_idx, \n        #     \"memory_attn\": attn_w,\n        #     \"retrieved\":  retrieved\n        # }\n        return  logits ,x_emb ,  mem_out \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:21.253071Z","iopub.execute_input":"2025-05-07T16:24:21.253389Z","iopub.status.idle":"2025-05-07T16:24:21.260184Z","shell.execute_reply.started":"2025-05-07T16:24:21.253363Z","shell.execute_reply":"2025-05-07T16:24:21.259141Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"\n\n# Memory Reconstruction Loss\n# Ensures stored information preserves input patterns\ndef reconstruction_loss(inputs, memory_output):\n    return F.mse_loss(memory_output, inputs)\n\n\n\n# Task-Specific Loss\n# Drives memory to store task-relevant information\ndef task_loss(predictions, targets):\n    return F.cross_entropy(predictions, targets)  # For classification\n\n# Memory Sparsity Loss\n# Encourages efficient slot usage\ndef sparsity_loss(concept_energy):\n    return torch.mean(concept_energy**2)  # L2 penalty on energy levels\n\n# Memory Diversity Loss\n# Prevents slot redundancy\ndef diversity_loss(key_memory):\n    normalized_keys = F.normalize(key_memory, dim=1)\n    similarity = torch.mm(normalized_keys, normalized_keys.T)\n    return torch.mean(similarity**2) - 1/torch.numel(similarity)\n\n\n\n# Energy Maintenance Loss\n# Maintains healthy energy distribution\ndef energy_loss(concept_energy):\n    energy_mean = torch.mean(concept_energy)\n    return F.mse_loss(energy_mean, torch.tensor(0.5,device = concept_energy.device))\n\n\n\n# Pruning Incentive Loss\n# Encourages proper slot turnover\ndef pruning_loss(age, usage):\n    old_unused = (age > 100) & (usage < 0.01)\n    return torch.mean(old_unused.float())\n\n\n\n# Anti-Collapse Loss\n# Prevents memory dependency on few slots\ndef anti_collapse_loss(usage_counts):\n    return -torch.sum(usage_counts * torch.log(usage_counts + 1e-7))\n\n\n\ndef novelty_loss(new_slots, existing_memory):\n    sim = F.cosine_similarity(new_slots.unsqueeze(1), \n                            existing_memory.unsqueeze(0), dim=-1)\n    return torch.mean(sim)\n\n\n\n\ndef total_loss(inputs, outputs, targets, memory):\n    # Base losses\n    rec_loss = reconstruction_loss(inputs, outputs)\n    # t_loss = task_loss(outputs, targets)\n    \n    # Memory regularization\n    sp_loss = sparsity_loss(memory.concept_energy)\n    div_loss = diversity_loss(memory.key_memory)\n    en_loss = energy_loss(memory.concept_energy)\n    \n    # Stability terms\n    prun_loss = pruning_loss(memory.age, memory.usage)\n    anti_loss = anti_collapse_loss(F.softmax(memory.access_count, dim=0))\n    \n    # Weighted combination\n    return (1.0 * rec_loss + \n            # 0.5 * t_loss + \n            0.3 * sp_loss + \n            0.2 * div_loss +\n            0.1 * en_loss +\n            0.05 * prun_loss +\n            0.02 * anti_loss)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:21.609871Z","iopub.execute_input":"2025-05-07T16:24:21.610224Z","iopub.status.idle":"2025-05-07T16:24:21.619243Z","shell.execute_reply.started":"2025-05-07T16:24:21.610192Z","shell.execute_reply":"2025-05-07T16:24:21.618370Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n\n\n\ndef cal_loss_batch(input_batch , target_batch , model:torch.nn.Module , device:torch.device ):\n    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n    return loss\ndef cal_loss_batch(input_batch , target_batch ,model:nn.Module, device:torch.device , mem_cof:float= 0.1):\n    input_batch  , target_batch =  input_batch.to(device) , target_batch.to(device)\n    logits ,x_emb, mem_output  =  model(input_batch)\n    B,T,V = logits.shape \n    gpt_loss = F.cross_entropy(\n            logits.view(B * T, V),\n            target_batch.view(B * T),\n            ignore_index=-100,                       # if you pad with -100\n        )   \n    # utilization_loss = -torch.log(model.memory.utilization + 1e-8)\n    memory_loss =  total_loss(inputs=x_emb , memory= model.memory , outputs=mem_output , targets= target_batch.float())\n    return gpt_loss +mem_cof * memory_loss \n\n\ndef calc_loss_loader(data_loader , model , device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:21.807408Z","iopub.execute_input":"2025-05-07T16:24:21.807728Z","iopub.status.idle":"2025-05-07T16:24:21.814902Z","shell.execute_reply.started":"2025-05-07T16:24:21.807702Z","shell.execute_reply":"2025-05-07T16:24:21.813819Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Text Generation Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\n\n\n\ndef text_to_token_ids(text,  tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\n\ndef token_ids_to_text(tokens , tokenizer):\n    flat  = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n    \ndef generate_and_sample(model  , idx , context_size ,max_new_tokens ):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits  , _ , _ = model(idx_cond)\n        logits  = logits[:, -1  , :]\n        probs  = torch.softmax(logits  , dim=-1)\n        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx \n\n#\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]  # shape: [1, current_seq_len]\n\n        # Create causal mask dynamically\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)  # [1, seq_len, seq_len]\n\n        with torch.no_grad():\n            logits , _ , _= model(idx_cond, mask=causal_mask)  # <--- pass mask here\n\n        logits = logits[:, -1, :]  # only take the last token logits\n\n        # Apply top-k sampling if needed\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        # Temperature sampling\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n        \n\n    return idx\ndef real_time_generation(model, initial_input, context_size, temperature, top_k=None, device=\"cpu\"):\n    # Tokenize the initial input and prepare the model context\n    idx = torch.tensor(initial_input).unsqueeze(0).to(device)  # Assuming initial_input is tokenized\n    \n    print(\"Starting real-time generation...\")\n    \n    # Start generating tokens in real-time\n    for new_token in generate(model, idx, max_new_tokens=50, context_size=context_size, temperature=temperature, top_k=top_k, device=device):\n        print(f\"Generated token: {new_token.item()}\")  # Or decode it back to a word\n        \n        # You can check for user input here and update idx with the new input\n        # For instance, wait for the user to input a prompt to append to the context\n        user_input = input(\"Enter new input (or press enter to continue generation): \")\n        \n        if user_input:\n            # Tokenize the new user input and append it to the context\n            user_input_tokens = torch.tensor(tokenize(user_input)).unsqueeze(0).to(device)\n            idx = torch.cat((idx, user_input_tokens), dim=1)  # Append the new tokens to the context\n        else:\n            # Continue generating if no new user input\n            continue\n\n# Function to tokenize input (adjust depending on your tokenizer)\ndef tokenize(text):\n    # Assuming you have a tokenizer function available\n    return [ord(c) for c in text]  # Dummy example: ord() converts char to token id\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:22.488583Z","iopub.execute_input":"2025-05-07T16:24:22.488885Z","iopub.status.idle":"2025-05-07T16:24:22.500792Z","shell.execute_reply.started":"2025-05-07T16:24:22.488862Z","shell.execute_reply":"2025-05-07T16:24:22.499906Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Dataset and DataLoader ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef generate_prompt(sample):\n    # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n    \n\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer =  tokenizer\n\n        all_tokens = []\n        allowed = {'<|endoftext|>'}\n        for sample in data:\n            prompt = generate_prompt(sample)\n            tokens = tokenizer.encode(prompt , allowed_special=allowed)\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n    return inputs, targets\n\ndef create_dataloader_v1(data, batch_size=4,\n    max_length=256, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n    dataset = Dataset_V1(data, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=collate_fn)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:24.973714Z","iopub.execute_input":"2025-05-07T16:24:24.974043Z","iopub.status.idle":"2025-05-07T16:24:25.031963Z","shell.execute_reply.started":"2025-05-07T16:24:24.974019Z","shell.execute_reply":"2025-05-07T16:24:25.030925Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Dataset And DataLoader for Psycology Dataset ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport tiktoken\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass Dataset_v2(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n        self.input_ids = []\n\n        all_tokens = []\n        for sample in data:\n            tokens = tokenizer.encode(sample)  \n            all_tokens.extend(tokens)\n\n        # Split the tokens into chunks of size max_length with stride\n        for i in range(0, len(all_tokens) - self.max_length, self.stride):\n            input_chunk = all_tokens[i:i + self.max_length]\n            target_chunk = all_tokens[i + 1:i + self.max_length + 1]\n            self.input_ids.append((torch.tensor(input_chunk), torch.tensor(target_chunk)))\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, index):\n        return self.input_ids[index]\n\ndef collect_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0) \n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  \n    return inputs, targets\n\ndef create_dataloader_v2(data, batch_size=4, max_length=1024, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")  \n    dataset = Dataset_v2(data, tokenizer, max_length, stride) \n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=collect_fn)\n    return dataloader\n\ndef load_txt_file(filepath):\n    with open(filepath, 'r') as f:\n        text = f.read()\n    return text\n\ndef split_into_chunks(text, chunk_size=1024, overlap=200):\n\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        chunks.append(chunk)\n\n    return chunks\n\n\n\nfile =  '/kaggle/input/datasetcleaned/cleaned_books.txt'\nload_text =  load_txt_file(file)\nchunk = split_into_chunks(load_text)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-07T16:24:27.453508Z","iopub.execute_input":"2025-05-07T16:24:27.453892Z","iopub.status.idle":"2025-05-07T16:24:29.554747Z","shell.execute_reply.started":"2025-05-07T16:24:27.453865Z","shell.execute_reply":"2025-05-07T16:24:29.553971Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Train Script ","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\n\n\ndef evaluate_model(model, train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,   # Increase generation length if needed\n            context_size=126,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n\n        # Trim everything before the generation\n        generated_only = decoded_text[len(start_context):].strip()\n\n        # Stop at endoftext token if present\n        end_marker = \"<|endoftext|>\"\n        if end_marker in generated_only:\n            generated_only = generated_only.split(end_marker)[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\ndef save_model_checkpoint(model, optimizer, epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch\n    }\n    torch.save(checkpoint, path.format(epoch))\ndef after_save_load():\n    checkpoint = torch.load(\"checkpoint_epoch_7.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n\n\ndef train_model(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"ðŸš€ Total training steps: {total_steps}\")\n    # scheduler = get_cosine_schedule_with_warmup(optimizer,\n    #                                         num_warmup_steps=500,\n    #                                         num_training_steps=total_steps)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n        save_model_checkpoint(model , optimizer , epoch+1)\n\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:29.593952Z","iopub.execute_input":"2025-05-07T16:24:29.594280Z","iopub.status.idle":"2025-05-07T16:24:31.933386Z","shell.execute_reply.started":"2025-05-07T16:24:29.594256Z","shell.execute_reply":"2025-05-07T16:24:31.932734Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MemoryGPT Model training  \n","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:38.148206Z","iopub.execute_input":"2025-05-07T16:24:38.148699Z","iopub.status.idle":"2025-05-07T16:24:38.152562Z","shell.execute_reply.started":"2025-05-07T16:24:38.148668Z","shell.execute_reply":"2025-05-07T16:24:38.151579Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# GPT Config ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nGPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,#126 \n# Context lengt\n\"emb_dim\": 768,\n# Embedding dimension\n\"n_heads\": 12,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False\n# Query-Key-Value bias\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:38.637834Z","iopub.execute_input":"2025-05-07T16:24:38.638176Z","iopub.status.idle":"2025-05-07T16:24:38.648736Z","shell.execute_reply.started":"2025-05-07T16:24:38.638146Z","shell.execute_reply":"2025-05-07T16:24:38.647974Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"GPT_CONFIG_124M_Memory = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,\n# Context length\n\"emb_dim\": 128,\n# Embedding dimension\n\"n_heads\": 4,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False,\n'memory_dim':128,\n'max_slots' :1000,\n'memory_heads':2 ,\n\n# Query-Key-Value bias\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:38.848369Z","iopub.execute_input":"2025-05-07T16:24:38.848751Z","iopub.status.idle":"2025-05-07T16:24:38.853430Z","shell.execute_reply.started":"2025-05-07T16:24:38.848722Z","shell.execute_reply":"2025-05-07T16:24:38.852380Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"memory =  EfiBioSemanticMemory_V2(128,128 , 1000,128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:39.709668Z","iopub.execute_input":"2025-05-07T16:24:39.709961Z","iopub.status.idle":"2025-05-07T16:24:39.805273Z","shell.execute_reply.started":"2025-05-07T16:24:39.709940Z","shell.execute_reply":"2025-05-07T16:24:39.804327Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2025-05-07T15:37:45.439942Z","iopub.execute_input":"2025-05-07T15:37:45.440250Z","iopub.status.idle":"2025-05-07T15:37:45.451796Z","shell.execute_reply.started":"2025-05-07T15:37:45.440226Z","shell.execute_reply":"2025-05-07T15:37:45.450897Z"}}},{"cell_type":"code","source":"class EmbeddingDataset(torch.utils.data.Dataset  ):\n    def __init__(self, dataloader , embddinglayer,device= 'cuda'):\n        self.loader  =  dataloader\n        self.embeddinglayer =  embddinglayer\n\n    def __len__(self):return len(self.loader.dataset)\n\n    def __getitem__(self, idx):\n        tokens , _ =  self.loader.dataset[idx]\n        emb =  self.embeddinglayer(tokens.to(device))\n        return emb \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:34:53.304106Z","iopub.execute_input":"2025-05-07T16:34:53.304483Z","iopub.status.idle":"2025-05-07T16:34:53.309823Z","shell.execute_reply.started":"2025-05-07T16:34:53.304453Z","shell.execute_reply":"2025-05-07T16:34:53.308580Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"warm_dataloader = EmbeddingDataset(train_dataloader, model.embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:34:54.544644Z","iopub.execute_input":"2025-05-07T16:34:54.544958Z","iopub.status.idle":"2025-05-07T16:34:54.549160Z","shell.execute_reply.started":"2025-05-07T16:34:54.544935Z","shell.execute_reply":"2025-05-07T16:34:54.547941Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mem  =  model.memory\nfor p in mem.parameters():\n    p.requires_grad =  False \nfor p in mem.compression.parameters():\n    p.requires_grad  = True  \nfor p in mem.W_cell.parameters():\n    p.requires_grad = True  \nfor p in mem.decompression.parameters():\n    p.requires_grad =  True \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:34:54.963774Z","iopub.execute_input":"2025-05-07T16:34:54.964083Z","iopub.status.idle":"2025-05-07T16:34:54.968843Z","shell.execute_reply.started":"2025-05-07T16:34:54.964060Z","shell.execute_reply":"2025-05-07T16:34:54.968008Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"optim = torch.optim.Adam(\n    list(mem.compression.parameters()) + [mem.W_cell.weight] + list(mem.decompression.parameters()), lr =  1e-4\n)\nwarmup_loader =  DataLoader(EmbeddingDataset(train_dataloader , model.embedding))\nmem = mem.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:34:55.189272Z","iopub.execute_input":"2025-05-07T16:34:55.189604Z","iopub.status.idle":"2025-05-07T16:34:55.196056Z","shell.execute_reply.started":"2025-05-07T16:34:55.189577Z","shell.execute_reply":"2025-05-07T16:34:55.195149Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\ndef warmup_memory(\n    memory: EfiBioSemanticMemory_V2,\n    embedding: nn.Embedding,\n    dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    epochs: int = 50,\n    lr: float = 1e-3,\n    cos_coef: float = 0.1\n):\n    # Freeze everything except compression, W_cell, decompression\n    for name, p in memory.named_parameters():\n        if not any(n in name for n in [\"compression\", \"W_cell\", \"decompression\"]):\n            p.requires_grad = False\n\n    optimizer = torch.optim.AdamW(\n        list(memory.compression.parameters()) +\n        list(memory.W_cell.parameters()) +\n        list(memory.decompression.parameters()),\n        lr=lr\n    )\n\n    memory.to(device).train()\n    embedding.to(device).eval()\n\n    for epoch in range(epochs):\n        total_loss, n = 0.0, 0\n        for input_ids, _ in tqdm(dataloader, desc=f\"Warmup epoch {epoch}\"):\n            input_ids = input_ids.to(device)              # [B, T]\n            with torch.no_grad():\n                x = embedding(input_ids)                  # [B, T, D]\n            B, T, D = x.shape\n\n            # 1) compress & W_cell\n            z = memory.compression(x.mean(dim=1))         # [B, C]\n            z = memory.W_cell(z)                          # [B, C]\n\n            # 2) reconstruct back to embedding space\n            recon = memory.decompression(z)               # [B, D]\n            recon = recon.unsqueeze(1).expand(-1, T, -1)  # [B, T, D]\n\n            # 3) losses# after you have `recon` of shape [B, T, D] and x of shape [B, T, D]:\n            B, T, D = recon.shape\n            \n            # reshape safely:\n            recon_flat = recon.reshape(B * T, D)\n            x_flat     = x.reshape(B * T, D)\n            \n            rec_loss = F.mse_loss(recon, x)\n            cos_loss = 1 - F.cosine_similarity(recon_flat, x_flat, dim=-1).mean()\n\n            loss = rec_loss + cos_coef * cos_loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            n += 1\n\n        print(f\">> Warmup epoch {epoch:02d} avg loss: {total_loss/n:.4f}\")\n    \n    # after warmâ€‘up you can save just these heads:\n    torch.save({\n        \"compression\": memory.compression.state_dict(),\n        \"W_cell\":      memory.W_cell.state_dict(),\n        \"decompression\": memory.decompression.state_dict()\n    }, \"memory_warmup_heads.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:41:22.819146Z","iopub.execute_input":"2025-05-07T16:41:22.819475Z","iopub.status.idle":"2025-05-07T16:41:22.828450Z","shell.execute_reply.started":"2025-05-07T16:41:22.819453Z","shell.execute_reply":"2025-05-07T16:41:22.827475Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"warmup_memory(mem , model.embedding, train_dataloader,'cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:41:23.983878Z","iopub.execute_input":"2025-05-07T16:41:23.984158Z","iopub.status.idle":"2025-05-07T16:41:34.844792Z","shell.execute_reply.started":"2025-05-07T16:41:23.984137Z","shell.execute_reply":"2025-05-07T16:41:34.843861Z"}},"outputs":[{"name":"stderr","text":"Warmup epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 245.08it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 00 avg loss: 19.6673\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 241.67it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 01 avg loss: 19.6731\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 260.40it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 02 avg loss: 19.6686\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 273.27it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 03 avg loss: 19.6661\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 278.86it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 04 avg loss: 19.6644\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 270.90it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 05 avg loss: 19.6619\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 272.90it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 06 avg loss: 19.6615\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 273.57it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 07 avg loss: 19.6625\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 244.35it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 08 avg loss: 19.6610\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 262.94it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 09 avg loss: 19.6622\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 285.39it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 10 avg loss: 19.6657\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 278.38it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 11 avg loss: 19.6637\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 276.98it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 12 avg loss: 19.6621\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 279.66it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 13 avg loss: 19.6607\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 248.79it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 14 avg loss: 19.6611\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 262.84it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 15 avg loss: 19.6636\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 272.99it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 16 avg loss: 19.6645\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 270.31it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 17 avg loss: 19.6627\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 245.92it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 18 avg loss: 19.6623\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 229.24it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 19 avg loss: 19.6595\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 245.91it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 20 avg loss: 19.6585\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 250.70it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 21 avg loss: 19.6582\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 269.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 22 avg loss: 19.6590\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 284.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 23 avg loss: 19.6581\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 269.66it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 24 avg loss: 19.6579\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 274.89it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 25 avg loss: 19.6585\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 288.67it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 26 avg loss: 19.6586\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 249.72it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 27 avg loss: 19.6589\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 272.14it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 28 avg loss: 19.6578\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 271.33it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 29 avg loss: 19.6593\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 248.39it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 30 avg loss: 19.6609\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 275.53it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 31 avg loss: 19.6585\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 279.21it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 32 avg loss: 19.6583\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 275.26it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 33 avg loss: 19.6580\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 250.27it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 34 avg loss: 19.6570\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 261.03it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 35 avg loss: 19.6571\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 272.37it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 36 avg loss: 19.6578\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 273.63it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 37 avg loss: 19.6578\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 278.15it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 38 avg loss: 19.6570\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 281.82it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 39 avg loss: 19.6559\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 248.49it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 40 avg loss: 19.6556\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 263.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 41 avg loss: 19.6552\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 273.79it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 42 avg loss: 19.6557\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 280.62it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 43 avg loss: 19.6575\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 275.30it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 44 avg loss: 19.6572\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 281.84it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 45 avg loss: 19.6565\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 268.05it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 46 avg loss: 19.6565\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 254.16it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 47 avg loss: 19.6593\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 272.10it/s]\n","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 48 avg loss: 19.6586\n","output_type":"stream"},{"name":"stderr","text":"Warmup epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 284.04it/s]","output_type":"stream"},{"name":"stdout","text":">> Warmup epoch 49 avg loss: 19.6582\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"cos_loss , rec_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:37:48.324948Z","iopub.execute_input":"2025-05-07T16:37:48.325351Z","iopub.status.idle":"2025-05-07T16:37:48.333356Z","shell.execute_reply.started":"2025-05-07T16:37:48.325319Z","shell.execute_reply":"2025-05-07T16:37:48.332516Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"(tensor(0.6241, device='cuda:0', grad_fn=<RsubBackward1>),\n tensor(22.8748, device='cuda:0', grad_fn=<MseLossBackward0>))"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"\nmem.train()\nfor epoch in range(100):\n    total = 0 \n    for emb_in , emb_target in warmup_loader:\n        emb_in , emb_target =  emb_in.to(device) , emb_target.to(device)\n        z = mem.compression(emb_in.mean(dim=1))\n        q =  mem.W_cell(z)\n\n        recon = mem.decompression(q)\n        recon = recon.unsqueeze(1).expand(-1, emb_in.size(1) , -1)\n        loss = F.mse_loss(recon, emb_target)\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        total+=  loss.item()\n    print(f\"Warmup epoch {epoch}  mse={total/len(warmup_loader):.6f}\")\n    with torch.no_grad():\n        cos_sim = F.cosine_similarity(q, z, dim=-1).mean()\n        print(\"avg cos_sim:\", cos_sim.item())\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:34:57.268950Z","iopub.execute_input":"2025-05-07T16:34:57.269248Z","iopub.status.idle":"2025-05-07T16:34:57.285914Z","shell.execute_reply.started":"2025-05-07T16:34:57.269226Z","shell.execute_reply":"2025-05-07T16:34:57.284885Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-f0a0e7b50105>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0memb_in\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0memb_target\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwarmup_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0memb_in\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0memb_target\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0memb_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0memb_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"],"ename":"ValueError","evalue":"not enough values to unpack (expected 2, got 1)","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"\ndevice =  'cuda' if torch.cuda.is_available() else \"cpu\"\nmodel =  GPTMemoryEnhanced(GPT_CONFIG_124M_Memory).to(device)\n\noptimizer =  torch.optim.AdamW(model.parameters() , lr=0.0004,weight_decay=0.01 )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:24:53.555846Z","iopub.execute_input":"2025-05-07T16:24:53.556176Z","iopub.status.idle":"2025-05-07T16:24:54.043745Z","shell.execute_reply.started":"2025-05-07T16:24:53.556156Z","shell.execute_reply":"2025-05-07T16:24:54.042824Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from collections import defaultdict\n\ndef get_param_group_summary(model):\n    groups = defaultdict(int)\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if \"embedding\" in name:\n            groups[\"embedding\"] += param.numel()\n        elif \"transformer\" in name:\n            groups[\"transformer_blocks\"] += param.numel()\n        elif \"memory\" in name or \"episodic\" in name or \"semantic\" in name:\n            groups[\"memory_modules\"] += param.numel()\n        elif \"norm\" in name:\n            groups[\"normalization\"] += param.numel()\n        elif \"lm_head\" in name or \"projection\" in name:\n            groups[\"output_projection\"] += param.numel()\n        else:\n            groups[\"other\"] += param.numel()\n    total = sum(groups.values())\n    for k, v in groups.items():\n        print(f\"{k:20s}: {v:,} parameters\")\n    print(f\"\\nTotal: {total:,}\")\nget_param_group_summary(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:50:18.469461Z","iopub.execute_input":"2025-05-07T15:50:18.469762Z","iopub.status.idle":"2025-05-07T15:50:18.478049Z","shell.execute_reply.started":"2025-05-07T15:50:18.469723Z","shell.execute_reply":"2025-05-07T15:50:18.477378Z"}},"outputs":[{"name":"stdout","text":"embedding           : 6,433,024 parameters\nmemory_modules      : 681,355 parameters\ntransformer_blocks  : 2,082,048 parameters\nnormalization       : 128 parameters\noutput_projection   : 50,257 parameters\n\nTotal: 9,246,812\n","output_type":"stream"}],"execution_count":205},{"cell_type":"code","source":"\n\nnum_epochs =  1\ntrain_ratio = 0.90\n\nfilename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\nwith open(filename , 'r') as f:\n    text_data =  json.load(f)\ntext_data = text_data[:100]\nsplit = int(train_ratio * len(text_data))\n\ntrain_data =  text_data[:split]\nval_data =  text_data[split:]\n\ntrain_dataloader =  create_dataloader_v1(data=train_data , batch_size=2 , max_length=GPT_CONFIG_124M_Memory['context_length'] , shuffle=True , drop_last= True)\nval_dataloader = create_dataloader_v1(data=val_data , batch_size=2 , max_length=GPT_CONFIG_124M_Memory['context_length']  , shuffle=False , drop_last=False )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T16:32:18.705899Z","iopub.execute_input":"2025-05-07T16:32:18.706241Z","iopub.status.idle":"2025-05-07T16:32:19.963345Z","shell.execute_reply.started":"2025-05-07T16:32:18.706211Z","shell.execute_reply":"2025-05-07T16:32:19.962618Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"\n\n\n\n\nprint('start training')\ntrain_losses , val_losses , token_seen =  train_model(\n    model=model , \n    train_dataloader=train_dataloader, \n    device=device, \n    eval_freq=5 , \n    eval_dataloader=val_dataloader , \n    optimizer=optimizer, \n    eval_iter=3,  \n    num_epochs=1, \n    start_context='Hello '\n)\nprint(model.memory.get_memory_metrics())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T15:54:40.587490Z","iopub.execute_input":"2025-05-07T15:54:40.587766Z","iopub.status.idle":"2025-05-07T15:54:43.194384Z","shell.execute_reply.started":"2025-05-07T15:54:40.587743Z","shell.execute_reply":"2025-05-07T15:54:43.193023Z"}},"outputs":[{"name":"stdout","text":"start training\nðŸš€ Total training steps: 57\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa59c983f0cb40dc9ce757cf762e77de"}},"metadata":{}},{"name":"stdout","text":"   preâ€‘write mean/sd: -9.456602856516838e-05 0.09186528623104095\n   preâ€‘write mean/sd: 0.01767999678850174 0.09592430293560028\n   preâ€‘write mean/sd: 0.009731531143188477 0.11400534212589264\nEpoch: 1 (step 000000): Train Loss: 46.0949, Val Loss: 46.1481\n   preâ€‘write mean/sd: 0.0032993219792842865 0.11148456484079361\n   preâ€‘write mean/sd: -0.003892923705279827 0.09818599373102188\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-211-e17e49131310>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m train_losses , val_losses , token_seen =  train_model(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-199-7c116f97df48>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, device, eval_dataloader, optimizer, eval_freq, eval_iter, start_context, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-195-977f46b62d9e>\u001b[0m in \u001b[0;36mcal_loss_batch\u001b[0;34m(input_batch, target_batch, model, device, mem_cof)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcal_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmem_cof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_batch\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0minput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mx_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_output\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     gpt_loss = F.cross_entropy(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-193-e4401e52ac6f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tokens, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_block\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mx\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmemory_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmem_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-191-e693c37417c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mff_output\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mff_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-187-9378e1c7a3cc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mnorm_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":211},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:23:02.485829Z","iopub.execute_input":"2025-05-03T09:23:02.486130Z","iopub.status.idle":"2025-05-03T09:23:02.490772Z","shell.execute_reply.started":"2025-05-03T09:23:02.486107Z","shell.execute_reply":"2025-05-03T09:23:02.490057Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:47:03.421280Z","iopub.execute_input":"2025-04-24T09:47:03.421571Z","iopub.status.idle":"2025-04-24T09:47:03.430581Z","shell.execute_reply.started":"2025-04-24T09:47:03.421551Z","shell.execute_reply":"2025-04-24T09:47:03.429796Z"}},"outputs":[{"name":"stdout","text":"memory_modules      : 10,305,011 parameters\nembedding           : 38,725,376 parameters\ntransformer_blocks  : 72,061,464 parameters\nnormalization       : 768 parameters\noutput_projection   : 50,257 parameters\n\nTotal: 121,142,876\n","output_type":"stream"}],"execution_count":60},{"cell_type":"raw","source":"def count_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n# your custom GPT, MemoryLLM, etc.\ntotal_params = count_trainable_parameters(model)\nprint(f\"Trainable parameters: {total_params:,}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T09:45:21.381944Z","iopub.execute_input":"2025-04-24T09:45:21.382287Z","iopub.status.idle":"2025-04-24T09:45:21.388203Z","shell.execute_reply.started":"2025-04-24T09:45:21.382238Z","shell.execute_reply":"2025-04-24T09:45:21.387376Z"}}},{"cell_type":"code","source":"from collections import defaultdict\n\ndef modulewise_param_count(model):\n    module_params = defaultdict(int)\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            module = name.split('.')[0]  # or custom parsing\n            module_params[module] += param.numel()\n    \n    for module, count in sorted(module_params.items(), key=lambda x: -x[1]):\n        print(f\"{module:<20} : {count:,} parameters\")\n\nmodulewise_param_count(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:45:21.656375Z","iopub.execute_input":"2025-04-24T09:45:21.656704Z","iopub.status.idle":"2025-04-24T09:45:21.664403Z","shell.execute_reply.started":"2025-04-24T09:45:21.656679Z","shell.execute_reply":"2025-04-24T09:45:21.663432Z"}},"outputs":[{"name":"stdout","text":"transformer_blocks   : 72,061,464 parameters\nembedding            : 38,597,376 parameters\nshared_memory        : 10,433,010 parameters\nprojection           : 50,257 parameters\nfinal_norm           : 768 parameters\nmemory_retention_alpha : 1 parameters\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"def train_model_restart(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    checkpoint_path: str = None\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step, start_epoch = 0, -1, 0\n\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"ðŸš€ Total training steps: {total_steps}\")\n\n    # ðŸ” Load from checkpoint if provided\n    if checkpoint_path is not None:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)\n\n        # â¬‡ï¸ Reduce learning rate by half when resuming\n        for param_group in optimizer.param_groups:\n            old_lr = param_group['lr']\n            param_group['lr'] = old_lr * 0.5\n            print(f\"ðŸ”§ Reduced LR: {old_lr:.6f} âžœ {param_group['lr']:.6f}\")\n\n        print(f\"âœ… Resuming training from Epoch {start_epoch}\")\n\n    # âš™ï¸ Reinitialize scheduler after changing LR\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=500,\n        num_training_steps=total_steps\n    )\n\n    for epoch in tqdm(range(start_epoch, num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n\n        save_model_checkpoint(model, optimizer, epoch + 1, global_step, tokens_seen)\n\n    return train_losses, val_losses, track_tokens_seen\n\ndef save_model_checkpoint(model, optimizer, epoch, global_step=None, tokens_seen=None, path=\"checkpoint_epoch.pt\"):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    if global_step is not None:\n        checkpoint['global_step'] = global_step\n    if tokens_seen is not None:\n        checkpoint['tokens_seen'] = tokens_seen\n\n    torch.save(checkpoint, f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\")\n    print(f\"ðŸ’¾ Saved checkpoint at epoch {epoch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.412648Z","iopub.status.idle":"2025-04-24T09:34:44.413047Z","shell.execute_reply":"2025-04-24T09:34:44.412872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"        checkpoint = torch.load('/kaggle/working/checkpoint_epoch_6.pt', map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.413954Z","iopub.status.idle":"2025-04-24T09:34:44.414329Z","shell.execute_reply":"2025-04-24T09:34:44.414156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses , val_losses  , token_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=2\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.415232Z","iopub.status.idle":"2025-04-24T09:34:44.415568Z","shell.execute_reply":"2025-04-24T09:34:44.415454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nloss_history = {\n    \"train_loss\": train_losses,\n    \"val_loss\": val_losses,\n    \"tokens_seen\": token_seen\n}\n\nwith open(\"loss_history.json\", \"w\") as f:\n    json.dump(loss_history, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.416549Z","iopub.status.idle":"2025-04-24T09:34:44.416821Z","shell.execute_reply":"2025-04-24T09:34:44.416720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# If you loaded from a JSON file\n# with open(\"loss_history.json\", \"r\") as f:\n#     data = json.load(f)\n#     train_losses = data[\"train_loss\"]\n#     val_losses = data[\"val_loss\"]\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\nplt.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\nplt.xlabel(\"Evaluation Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")  # Save the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.417430Z","iopub.status.idle":"2025-04-24T09:34:44.417738Z","shell.execute_reply":"2025-04-24T09:34:44.417597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\n\n# Load model\nmodel = GPTMQModel2(GPT_CONFIG_124M)\n# model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_7.pt\"))\ncheckpoint = torch.load(\"checkpoint_epoch_7.pt\")\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\nmodel.eval().to(device)\n\n# Tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Utility functions\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n# Sampling-based generate function (uses your logic)\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\nf\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n# High-level text generation function\ndef generate_response(prompt, model, tokenizer, max_new_tokens=100, context_size=128, temperature=1.0, top_k=50):\n    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n    generated_ids = generate(\n        model=model,\n        idx=input_ids,\n        max_new_tokens=max_new_tokens,\n        context_size=context_size,\n        temperature=temperature,\n        top_k=top_k\n    )\n    return token_ids_to_text(generated_ids, tokenizer)\n\n# Try it out\n# prompt = \"### Instruction:\\nExplain what is deep learning.\\n\\n### Response:\\n <bot>\"\nprompt = \"\"\"\n\n'### Instruction :Give three tips for staying healthy ### Response:'\n\"\"\".strip()\n\n\noutput = generate_response(prompt, model, tokenizer)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.418499Z","iopub.status.idle":"2025-04-24T09:34:44.418773Z","shell.execute_reply":"2025-04-24T09:34:44.418675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is generated\n        if idx_next.item() == end_token_id:\n            break\n\n    return idx\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\nraw_output = generate_response(prompt, model, tokenizer)\ncleaned_output = truncate_after_n_bullets(raw_output)\nprint(cleaned_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.419352Z","iopub.status.idle":"2025-04-24T09:34:44.419579Z","shell.execute_reply":"2025-04-24T09:34:44.419486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_response(\n    prompt, model, tokenizer,\n    temperature=0.8,  # better balance\n    top_k=40,         # a bit narrower selection\n    max_new_tokens=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.420138Z","iopub.status.idle":"2025-04-24T09:34:44.420420Z","shell.execute_reply":"2025-04-24T09:34:44.420314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device).unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is in the generated output\n        if end_token_id in idx_next:\n            break\n\n    return idx\n\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\n\n# ðŸ” Input prompt\nprompt = \"### Instruction: What are the three primary colors? \\n### Response:\"\n\n# ðŸ” Tokenize input\ninput_ids = text_to_token_ids(prompt, tokenizer).to(device)\n\n# ðŸ” Generate output tokens\noutput_ids = generate(\n    model=model,\n    idx=input_ids,\n    max_new_tokens=100,\n    context_size=128,\n    temperature=0.7,\n    top_k=40\n)\n\n# ðŸ” Decode and postprocess\noutput_text = tokenizer.decode(output_ids[0].tolist())\n\n# âœ‚ï¸ Truncate after 3 bullets (optional)\nfinal_output = truncate_after_n_bullets(output_text)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.421013Z","iopub.status.idle":"2025-04-24T09:34:44.421333Z","shell.execute_reply":"2025-04-24T09:34:44.421177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}