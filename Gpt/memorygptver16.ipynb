{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339249,"sourceType":"datasetVersion","datasetId":7093845},{"sourceId":11378548,"sourceType":"datasetVersion","datasetId":7124129},{"sourceId":11378997,"sourceType":"datasetVersion","datasetId":7124489},{"sourceId":12037038,"sourceType":"datasetVersion","datasetId":7574150},{"sourceId":12046667,"sourceType":"datasetVersion","datasetId":7581007},{"sourceId":12071125,"sourceType":"datasetVersion","datasetId":7598368},{"sourceId":12076822,"sourceType":"datasetVersion","datasetId":7602140},{"sourceId":12078039,"sourceType":"datasetVersion","datasetId":7603050}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math \nimport torch \nimport torch.nn as nn \nimport transformers \nfrom tqdm.notebook import tqdm\n# Memory Network\nimport torch.nn.functional as F \nimport random\nimport json\nfrom typing import Tuple , Optional\nimport torch.bin \nfrom transformers import AutoModelForCausalLM , AutoTokenizer\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:27.613750Z","iopub.execute_input":"2025-06-12T16:39:27.614066Z","iopub.status.idle":"2025-06-12T16:39:40.572093Z","shell.execute_reply.started":"2025-06-12T16:39:27.614035Z","shell.execute_reply":"2025-06-12T16:39:40.571162Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\ntorch.backends.cudnn.benchmark = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.573188Z","iopub.execute_input":"2025-06-12T16:39:40.573549Z","iopub.status.idle":"2025-06-12T16:39:40.577086Z","shell.execute_reply.started":"2025-06-12T16:39:40.573526Z","shell.execute_reply":"2025-06-12T16:39:40.576191Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# RMS NORM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim)) \n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        norm_x = self._norm(x.float()).type_as(x) \n        return norm_x * self.scale \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.578650Z","iopub.execute_input":"2025-06-12T16:39:40.578933Z","iopub.status.idle":"2025-06-12T16:39:40.596977Z","shell.execute_reply.started":"2025-06-12T16:39:40.578912Z","shell.execute_reply":"2025-06-12T16:39:40.596217Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Embedding Layer","metadata":{}},{"cell_type":"code","source":"import torch \n\nclass EmbeddingLayer(torch.nn.Module):\n    def __init__(self , vocab_size , embedding_dim):\n        super().__init__()\n\n        self.embedding_layer= torch.nn.Embedding(vocab_size , embedding_dim)\n\n    def forward(self , input_tokens):\n        return self.embedding_layer(input_tokens)\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.598126Z","iopub.execute_input":"2025-06-12T16:39:40.598338Z","iopub.status.idle":"2025-06-12T16:39:40.622063Z","shell.execute_reply.started":"2025-06-12T16:39:40.598319Z","shell.execute_reply":"2025-06-12T16:39:40.621228Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# FeedForward Layer ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n      \n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n            GELU(),\n            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n        )\n    def forward(self, x ):\n        return self.layers(x)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.622946Z","iopub.execute_input":"2025-06-12T16:39:40.623199Z","iopub.status.idle":"2025-06-12T16:39:40.639252Z","shell.execute_reply.started":"2025-06-12T16:39:40.623172Z","shell.execute_reply":"2025-06-12T16:39:40.638692Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Normalization Layer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch \n\nclass LayerNorm(nn.Module):\n    def __init__(self , emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale  = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self , x):\n        mean = x.mean(dim= -1, keepdim = True)\n        var = x.var(dim =-1, keepdim = True)\n        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n        return self.scale * norm_x + self.shift \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.640002Z","iopub.execute_input":"2025-06-12T16:39:40.640221Z","iopub.status.idle":"2025-06-12T16:39:40.657555Z","shell.execute_reply.started":"2025-06-12T16:39:40.640191Z","shell.execute_reply":"2025-06-12T16:39:40.657030Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# RoPE Embdding ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport torch \nfrom dataclasses import dataclass\n\n\nclass NRopE: # RopE in Numpy \n    def rotate_2d(self,vec , theta_p):\n        cos_theta  , sin_theta  = np.cos(theta_p) , np.sin(theta_p)\n        rotat_vec = np.array([[cos_theta , -sin_theta],\n                    [sin_theta ,cos_theta]])\n        \n        return rotat_vec @ vec\n\n\n    def RoPe(self,x , p , theta = 10000):\n        d = len(x)\n        x_rotate =  np.zeros_like(x)\n        for i in range(0 , d , 2):\n            if i +1< d:\n                theta_p = (theta **(-2*(i//2)))**p \n                roted_pair = self.rotate_2d(x[i:i+1] , theta_p)    \n                x_rotate[i:i+1] = roted_pair\n\n        return x_rotate\n\n\n\n@dataclass\nclass TRopE(torch.nn.Module): # RopE in torch \n    def __init__(self, dim:int ,theta:float = 10000):\n        self.dim = dim \n        self.theta = theta \n        self.freq =  torch.pow(self.theta ,-torch.arange(0 ,dim  , 2)/dim )\n        torch.nn.Parameter('freq' , self.freq)\n\n    def forward(self, x:torch.Tensor , pos:torch.Tensor):\n        batch_size , seq_len, dim = x.shape\n        assert dim ==self.dim ,\"Error dim must be same\"\n        theta_p = torch.einsum(\"n,d->nd\" , pos, self.freq.to(x.device))\n        cos_theta  , sin_theta = torch.cos(theta_p) , torch.sin(theta_p)\n        x_even , x_odd =  x[... , ::2] , x[... , 1::2]\n        x_rotated =  torch.empty_like(x)\n        x_rotated[...,::2] =  x_even * cos_theta - x_odd * sin_theta\n        x_rotated[...,1::2] =  x_even * sin_theta + x_odd * cos_theta\n\n        return x_rotated\n\n\n\n\n\n\n\ndef precompute_freq_cis(  dim:int , end:int , theta:float = 10000.0):\n        \"\"\"dim : dimentions \n        end: end index   \n        \"\"\"\n        freqs =  1/(theta **(torch.arange(0 , dim , 2)[:dim//2].float() / dim))\n        t =  torch.arange(end, device=freqs.device)\n        freqs = torch.outer(t , freqs).float()\n        freqs_cis =  torch.polar(torch.ones_like(freqs), freqs)\n        return freqs_cis \n\n\ndef reshape_for_broadcast(freq_cis  , x):\n        \"\"\" reshape the freqcies to match x dimentions \"\"\"\n        ndim=  x.ndim\n        assert 0<=1<ndim \n        assert freq_cis.shape == (x.shape[1], x.shape[-1]), f\"Expected {(x.shape[1], x.shape[-1])}, got {freq_cis.shape}\" \n        shape = [d if i == 1 or i ==  ndim -1 else 1 for i , d in enumerate(x.shape)]\n        return freq_cis.view(*shape)\n\n\ndef apply_rotary_embedding( xq:torch.Tensor ,xk:torch.Tensor ,  freq_cis:torch.Tensor):\n\n            xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n            xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n\n\n            freq_cies =  reshape_for_broadcast(freq_cis , xq_)\n    \n\n            xq_out = torch.view_as_real(xq_* freq_cies).flatten(3)\n            \n            xk_out = torch.view_as_real(xk_*freq_cies).flatten(3)\n\n\n            return  xq_out.type_as(xq)   ,  xk_out.type_as(xq) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.658103Z","iopub.execute_input":"2025-06-12T16:39:40.658277Z","iopub.status.idle":"2025-06-12T16:39:40.676810Z","shell.execute_reply.started":"2025-06-12T16:39:40.658262Z","shell.execute_reply":"2025-06-12T16:39:40.676067Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# MultiHead & MultiQuery Attention Layer ","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention_V2(nn.Module):\n    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n        super().__init__()\n        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n        self.d_in =d_in\n        self.d_out = d_out\n        self.dropout = nn.Dropout(dropout)\n        self.num_heads  = num_heads\n        self.head_dim = d_out // num_heads\n        self.out_proj  = nn.Linear(d_out , d_out)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens , d_in = x.shape\n        keys = self.w_key(x)\n        queries  = self.w_query(x)\n        values = self.w_value(x)\n        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(2, 3)\n        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n        attn_scores.masked_fill(mask_bool , -torch.inf)\n        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n        attn_weights = self.dropout(attn_weights)\n        context_vector = (attn_weights  @ values).transpose(1, 2)\n        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n        context_vector = self.out_proj(context_vector)\n        return context_vector\n\n\n\n\ndef apply_rotary_embedding(xq:torch.Tensor , xk:torch.Tensor , freq_cies:torch.Tensor):\n\n    assert xq.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n    assert xk.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1] , -1, 2))\n\n    freq_cies = reshape_for_broadcast(freq_cies , xq_)\n\n    xq_out = torch.view_as_real(xq_ * freq_cies ).flatten(3)\n\n    xk_out = torch.view_as_real(xk_ * freq_cies).flatten(3)\n\n    return xq_out.type_as(xq) ,  xk_out.type_as(xk)\n\n\n\n\nclass MultiQueryAttentionBlock(nn.Module):\n    def __init__(self, d_model:int , h:int , dropout:float , seq_len:int , qkv_bias =  False ):\n        super().__init__()\n        self.d_model  = d_model \n\n        self.seq_len=  seq_len\n\n        assert d_model % h == 0, \"d_model is must be divided by th head\"\n        self.dropout = nn.Dropout(dropout)\n\n        self.h = h  \n\n        self.d_k = d_model // h\n\n        self.w_qkv =  nn.Linear(d_model , d_model +2 * self.d_k )\n\n        self.w_o = nn.Linear(d_model  , d_model)\n\n        freq_cies = precompute_freq_cis(dim=self.d_k , end=self.seq_len * 2 )\n\n        self.register_buffer('freq_cies' , freq_cies , persistent= False )\n\n    def generate_causal_mask(self, seq_len, device):\n        # shape: (1, 1, seq_len, seq_len)\n        return torch.tril(torch.ones((1, 1, seq_len, seq_len), device=device)).bool()\n\n    @staticmethod\n    def attention(q, k  , v,mask  , dropout):\n        d_k = q.shape[-1]\n\n        attention_score =  (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n\n        if mask is not None :\n            if mask.dim() == 2:\n                      mask = mask.unsqueeze(1).unsqueeze(2)\n            elif mask.dim() == 3:\n                     mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n        \n        attention_score = attention_score.softmax(dim=-1)\n\n        if dropout is not None :\n            attention_score = dropout(attention_score)\n\n        context_vector =  attention_score @ v\n\n        return context_vector  , attention_score\n    \n\n\n    def forward(self, q, mask= None , return_attn =  False ):\n        if mask is None:\n            mask = self.generate_causal_mask(self.seq_len , device = q.device)\n        qkv =  self.w_qkv(q)\n\n        query , key, value =  torch.split(qkv , [self.d_model  , self.d_k , self.d_k], dim=-1)\n\n        query = query.view(query.shape[0] , -1 , self.h , self.d_k).transpose(1, 2)\n\n        key =  key.unsqueeze(1)\n\n        value =  value.unsqueeze(1)\n\n\n        seq_len =  q.size(1)\n\n        freq_cies = self.freq_cies[:query.shape[1]].to(q.device)\n\n        # freq_cies =  self.freq_cies[:seq_len].to(q.device)\n\n        query , key = apply_rotary_embedding(query , key , freq_cies)\n\n        x , attention_score = MultiQueryAttentionBlock.attention(q = query,k =  key,v= value ,mask=mask , dropout= self.dropout)\n\n        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h* self.d_k)\n\n        x = self.w_o(x)\n        if return_attn:\n            return x , attention_score \n        else:\n            return x , None\n        \n\n     \n    \n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.679306Z","iopub.execute_input":"2025-06-12T16:39:40.679516Z","iopub.status.idle":"2025-06-12T16:39:40.699398Z","shell.execute_reply.started":"2025-06-12T16:39:40.679498Z","shell.execute_reply":"2025-06-12T16:39:40.698732Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Memory Network ","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn \nimport torch.nn.functional as F \nimport random\n\n\nclass EfiBioSemanticMemory_V2(nn.Module):\n    def __init__(self, input_dim:int ,semantic_memory_dim , max_slots:int = 1000 , compress_dim:int =  128 , top_k:int = 5 , num_heads:int =  4 ):\n        super().__init__()\n\n        self.input_dim = input_dim \n        self.max_slots =  max_slots \n        self.compress_dim =  compress_dim \n        self.top_k =  top_k \n        self.num_heads =  num_heads \n        self.semantic_memory_dim = semantic_memory_dim\n        # self.memory_size =  semantic_memory_dim \n\n\n\n        self.key_memory =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.value_memory = nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.cell_state =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.register_buffer('active_mask' , torch.zeros(max_slots , dtype= torch.bool))\n        self.active_mask[:semantic_memory_dim] = True  \n\n        \n        # Meta data parameter \n        self.register_buffer('age', torch.zeros(max_slots))\n        self.register_buffer('usage', torch.zeros(max_slots))\n        self.register_buffer('concept_energy', torch.ones(max_slots))\n        self.register_buffer('memory_age', torch.zeros(max_slots))\n        self.register_buffer('access_count', torch.zeros(max_slots))\n        self.register_buffer(\"_memory_version\", torch.tensor(0))\n        self.concept_energy[:semantic_memory_dim] =  0.2\n        # self.new_slot_mask  = torch.zeros(self.memory_size).bool()\n\n        #stats params\n        self.register_buffer(\"step_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer('replay_count', torch.zeros(1 , dtype= torch.long))\n        self.register_buffer(\"query_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"novel_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"write_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"hit_count\", torch.zeros(1, dtype=torch.long)) \n        self.register_buffer('merge_count' , torch.zeros(1  , dtype= torch.long))\n        self.register_buffer('neuroslot_count' , torch.zeros(1, dtype= torch.long))\n        self.register_buffer('prune_count' , torch.zeros(1, dtype=torch.long))\n        self.register_buffer('consalidate_count', torch.zeros(1,dtype=torch.long))\n        self.register_buffer('update_count', torch.zeros(1,dtype=torch.long))\n\n\n        self.initial_write_step  = 300\n        \n        # Threshold Parameter \n        self.consolidation_threshold = nn.Parameter(torch.tensor(100.0))\n        self.energy_threshold = nn.Parameter(torch.tensor(0.3))\n        self.decay_rate = nn.Parameter(torch.tensor(0.99))\n        # self.novelty_threshold = nn.Parameter(torch.tensor(0.2))\n        # self.novelty_threshold = 0.2 * (1 - (self.memory_size / self.max_slots))\n        self.novelty_threshold = 0.1\n\n        self.register_buffer(\"prune_age_threshold\", torch.tensor(50))\n        self.register_buffer(\"neurogenesis_threshold\", torch.tensor(0.15))\n        self.register_buffer(\"new_slot_maturation_steps\", torch.tensor(10)) \n        self.synaptic_scale = nn.Parameter(torch.tensor(0.5))\n        self.sparsity = nn.Parameter(torch.tensor(0.5))\n        self.sim_thershold =  nn.Parameter(torch.tensor(0.7))\n        self.confidence_threshold_att = 0.15 \n\n        # Concept queue Params\n        self.register_buffer('queue_max_size' , torch.tensor(1000))\n        self.register_buffer('concept_queue' ,  torch.zeros(self.queue_max_size , self.compress_dim))\n        self.queue_ptr = 0\n        self.queue_count = 0 \n\n        # Networks \n        self.important_net = nn.Sequential(\n            nn.Linear(compress_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        \n\n        self.update_gate = nn.Sequential(\n            nn.Linear(3 * compress_dim, 1),\n            nn.Hardsigmoid()\n        )\n\n        for layer in self.update_gate:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n                nn.init.constant_(layer.bias, 0.1) \n        self.forgot_gate = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 3),\n            nn.Sigmoid()\n        )\n        self.forget_gate_net = nn.Linear(compress_dim * 2, compress_dim)\n\n        self.compression = nn.Sequential(\n            nn.Linear(input_dim, semantic_memory_dim),\n            nn.RMSNorm(semantic_memory_dim),\n            nn.GELU(),\n            nn.Linear(semantic_memory_dim, self.compress_dim)\n        )\n\n        self.decompression = nn.Sequential(\n            nn.Linear(self.compress_dim, input_dim),\n        )\n\n        self.W_cell = nn.Linear(self.compress_dim, compress_dim, bias=False)\n        self.memory_projection = nn.Linear(self.compress_dim, self.input_dim)\n        # self.query_proj =  nn.Linear(self.semantic_memory_dim  , self.compress_dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=compress_dim,\n            num_heads=num_heads,\n            batch_first=False\n        )\n        self.mem_key_proj   = nn.Linear(compress_dim, semantic_memory_dim, bias=False)\n        self.mem_value_proj = nn.Linear(compress_dim, semantic_memory_dim, bias=False)\n        self.no_memory_embedding = nn.Parameter(torch.randn(1, 1, self.input_dim))\n\n        nn.init.kaiming_uniform_(self.key_memory, mode='fan_out')\n        nn.init.xavier_normal_(self.value_memory)\n        nn.init.xavier_normal_(self.cell_state)\n        # centroids =  centroids.to(self.key_memory.device)\n        # self.key_memory.data = F.normalize(centroids.clone(), dim=-1)\n        # self.value_memory.data = F.normalize(centroids.clone(), dim=-1)\n        # self.cell_state.data = F.normalize(centroids.clone(), dim=-1)\n\n    def _get_active_memory(self):\n\n        \"\"\"\n            Get the active memries slot \n        \"\"\"\n        idx = torch.nonzero(self.active_mask, as_tuple=False).squeeze(1)\n        assert idx.numel() > 0, \"No active memory slots\"\n       \n        return  (\n            self.key_memory[idx] , \n            self.value_memory[idx], \n            self.cell_state[idx]\n\n        )\n        \n    @property \n    def active_capacity(self):\n        return torch.sum(self.active_mask).item()/ self.max_slots\n\n\n    def _adaptive_decay(self, memory_idx):\n        energy = self.concept_energy[memory_idx]\n        access = self.usage[memory_idx]\n\n        decay = torch.exp((1-self.decay_rate) * (1-energy) * (1-access))\n        return decay  \n    \n    def _boost_energy_on_access(self,indices , sims):\n        weights =  F.softmax(sims , dim=-1)\n        boost =  weights * 0.1 \n        self.concept_energy[indices] = torch.clamp(\n            self.concept_energy[indices] * 0.95 + boost, 0, 1)\n        \n        \n    def _update_memory(self, topk_idx: torch.LongTensor, cells: torch.Tensor,\n                   projected: torch.Tensor, update_gates: torch.Tensor):\n        projected_exp = projected.unsqueeze(1).expand(-1, cells.size(1), -1)\n    \n        cell_input = torch.sigmoid(cells + projected_exp)  \n    \n        decay_factor = self._adaptive_decay(topk_idx)     \n        decay_factor = decay_factor.unsqueeze(-1)         \n        cell_updates = decay_factor * cell_input\n    \n        # apply update_gates: [B, top_k, D]\n        delta = update_gates.unsqueeze(-1) * cell_updates\n    \n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0]\n        new_slot_mask = torch.isin(topk_idx, active_indices[-10:])\n        if new_slot_mask.any():\n            cell_updates = cell_updates.clone()  \n            cell_updates[new_slot_mask] *= 0.1\n            delta = delta * torch.where(new_slot_mask.unsqueeze(-1), 0.5, 1.0)\n    \n        delta = delta * self.synaptic_scale   \n    \n        batch_size, top_k, dim = delta.shape\n        flat_topk_idx = topk_idx.view(-1)             \n        flat_delta    = delta.view(-1, dim)         \n    \n        flat_cells = F.normalize(cells, dim=-1).view(-1, dim)  \n    \n        self.age.data += 1\n        self.update_count += 1\n        assert flat_topk_idx.max() < self.key_memory.size(0), (\n            f\"Index {flat_topk_idx.max()} >= {self.key_memory.size(0)}\"\n        )\n        max_slots, _ = self.cell_state.shape \n        device = self.cell_state.device\n        dtype = self.cell_state.dtype\n    \n        delta_buffer = torch.zeros((max_slots, dim), device=device, dtype=dtype)\n        key_buffer   = torch.zeros_like(delta_buffer)\n        val_buffer   = torch.zeros_like(delta_buffer)\n    \n        \n        idx_expanded = flat_topk_idx.unsqueeze(-1).expand(-1, dim) \n        delta_buffer.scatter_add_(0, idx_expanded, flat_delta)\n    \n        key_buffer.scatter_add_(0, idx_expanded, flat_cells)\n        val_buffer.scatter_add_(0, idx_expanded, flat_cells)\n    \n        with torch.no_grad():\n            self.cell_state.data.add_(delta_buffer)\n            self.key_memory.data.add_(key_buffer)\n            self.value_memory.data.add_(val_buffer)\n    \n            updated_idx = flat_topk_idx.unique()\n            self.age.data[updated_idx] = 0\n    \n            decay = self.decay_rate ** self.age.unsqueeze(-1)\n            self.cell_state.data.mul_(decay)\n            self.key_memory.data.mul_(decay)\n            self.value_memory.data.mul_(decay)\n    \n            k_norm = F.normalize(self.key_memory.data[updated_idx], dim=-1)\n            v_norm = F.normalize(self.value_memory.data[updated_idx], dim=-1)\n            c_norm = F.normalize(self.cell_state.data[updated_idx], dim=-1)\n    \n            self.key_memory.data[updated_idx]   = torch.tanh(k_norm)\n            self.value_memory.data[updated_idx] = torch.tanh(v_norm)\n            self.cell_state.data[updated_idx]   = torch.tanh(c_norm)\n\n\n\n    def _get_low_energy_slots(self, candidate_indices):\n        if len(candidate_indices) == 0:\n            return candidate_indices \n        candidate_energy =  self.concept_energy[candidate_indices]\n\n        sorted_indices   =  torch.argsort(candidate_energy)\n        return candidate_indices[sorted_indices]\n       \n\n    @property\n    def memory_size(self):\n        return int(self.active_mask.sum().item())\n    \n    @property\n    def utilization(self):\n        return    self.memory_size /  self.max_slots\n\n        \n    # @torch.no_grad()\n    def _batch_update_with_old(self, indices: torch.LongTensor, new_data: torch.Tensor):\n     \n        old_keys = self.key_memory[indices]      \n        old_vals = self.value_memory[indices]     \n        old_cells = self.cell_state[indices]     \n    \n        gate_input = torch.cat([old_cells, old_keys, new_data], dim=-1)\n    \n        pre = self.update_gate(gate_input)            \n        gate = self.hard_sigmoid(pre)                \n    \n\n    \n        with torch.no_grad():\n            blended_key = gate * new_data + (1.0 - gate) * old_keys   # [N, D]\n            blended_val = (0.35 * old_vals) + (0.65 * new_data * (1.0 - gate))  # [N, D]\n            blended_cell = gate * new_data + (1.0 - gate) * old_cells  # [N, D]\n    \n            new_key_norm = F.normalize(blended_key, dim=-1)\n            new_val_norm = F.normalize(blended_val, dim=-1)\n            new_cell_norm = F.normalize(blended_cell, dim=-1)\n    \n            self.key_memory.data[indices]   = new_key_norm\n            self.value_memory.data[indices] = new_val_norm\n            self.cell_state.data[indices]   = new_cell_norm\n    \n            self.usage[indices] = 0.0\n            self.age[indices] *= 0.25\n            self.memory_age[indices] = 0\n            self.concept_energy[indices] = 0.5\n            self.access_count[indices] = 0\n    \n\n\n    def _batch_update_with_old(self , indices , new_data, importance_score = None):\n        if importance_score is None:\n            importance_score =  torch.ones_like(indices, dtype = torch.float32) * 0.5\n\n        old_keys =  self.key_memory[indices]\n        old_vals =  self.value_memory[indices]\n        old_cell =  self.cell_state[indices]\n        concept_energy =  self.concept_energy[indices]\n\n        gate_input =  torch.cat([old_keys, new_data]  , dim=-1)\n        forget_gate =  torch.sigmoid(self.forget_gate_net(gate_input))\n        inactive_mask = (concept_energy < 0.15).float().unsqueeze(-1)\n        write_gate = torch.clamp(importance_score.unsqueeze(-1) + inactive_mask, 0.0, 1.0)\n    \n        # Update memory with gated blend\n        updated_keys = F.normalize((1 - forget_gate) * old_keys + forget_gate * new_data, dim=-1)\n        updated_vals = F.normalize(0.35 * old_vals + 0.65 * new_data, dim=-1)\n    \n        updated_cell = F.normalize(\n            self.update_gate(torch.cat([old_cell, old_keys, new_data], dim=-1)) * new_data\n            + (1 - self.update_gate(torch.cat([old_cell, old_keys, new_data], dim=-1))) * old_cell,\n            dim=-1\n        )\n    \n        # Final gated write\n        with torch.no_grad():\n            self.key_memory[indices] = (1 - write_gate) * old_keys + write_gate * updated_keys\n            self.value_memory[indices] = (1 - write_gate) * old_vals + write_gate * updated_vals\n            self.cell_state[indices] = (1 - write_gate) * old_cell + write_gate * updated_cell\n    \n            # Reset metadata\n            self.usage[indices] = 0.0\n            self.age[indices] *= 0.25\n            self.memory_age[indices] = 0\n            self.concept_energy[indices] = 0.5\n            self.access_count[indices] = 0\n            \n\n    @torch.no_grad()\n    def _batch_update_with_new(self, new_idx, new_data):\n        self.key_memory[new_idx] = new_data\n        self.value_memory[new_idx] = new_data\n        self.cell_state[new_idx] = new_data\n        self.usage[new_idx] = 0.0\n        self.age[new_idx] = 0\n        self.access_count[new_idx] = 0\n        self.memory_age[new_idx] = 0\n        self.concept_energy[new_idx] = 0.5\n        self.active_mask[new_idx] = True\n\n    \n    def _write_memory_update(self, new_concepts: torch.Tensor, retry_count=0):\n        # if retry_count ==  0 and self.query_count > 0:\n        #     self.flush_concept_queue()\n        # print('Writing Happen')\n        if retry_count > 2 or new_concepts.size(0) == 0:\n            return\n        importance = self.important_net(new_concepts).squeeze(-1)\n        keep_mask = importance > 0.10\n        if not keep_mask.any():\n            return\n\n        new_concepts = new_concepts[keep_mask]\n        remaining = new_concepts.size(0)\n\n        # 1. Update low-energy active slots\n        low_energy_candidate = torch.where(self.active_mask & (self.energy_threshold > self.concept_energy))[0]\n        if low_energy_candidate.numel() > 0:\n            candidate = self._get_low_energy_slots(low_energy_candidate)\n            num_reuse = min(len(candidate), remaining)\n            if num_reuse > 0:\n                self._batch_update_with_old(indices=candidate[:num_reuse], new_data=new_concepts[:num_reuse])\n                new_concepts = new_concepts[num_reuse:]\n                remaining = new_concepts.size(0)\n\n        # 2. Write to new slots\n        if remaining > 0 and self.memory_size < self.max_slots:\n            add = min(remaining, self.max_slots - self.memory_size)\n            new_idx = self.neurogenesis(return_index=True, required_slots=add)\n            if new_idx is not None:\n                self._batch_update_with_new(new_idx, new_concepts[:add])\n            # self.memory_size += add\n                new_concepts = new_concepts[add:]\n                remaining = new_concepts.size(0)\n\n        if remaining == 0:\n            return\n        prev_active = self.active_mask.sum()\n        self._optimize_memory()\n        if self.active_mask.sum() > prev_active and retry_count < 2:\n            \n            self._write_memory_update(new_concepts, retry_count + 1)\n        else:\n            self._enqueue_to_queue_buffer(new_concepts)\n\n        assert self.memory_size <= self.max_slots\n        assert torch.all(self.active_mask[:self.memory_size])\n\n    @torch.no_grad()\n    def flush_concept_queue(self):\n        if self.query_count > 0:\n            concepts = self.concept_queue[:self.queue_count]\n            self._write_memory_update(concepts)\n            self.queue_count = 0\n            self.queue_ptr = 0\n           \n\n    @torch.no_grad()\n    def _enqueue(self, data):\n        if data.size(0) == 0:\n            return\n\n        capacity = self.concept_queue.size(0)\n        avail = capacity - self.queue_count\n        to_add = data[:avail]\n        if to_add.size(0) == 0:\n            return\n\n        start = self.queue_ptr\n        end = (start + to_add.size(0)) % self.queue_max_size  \n        if end <= capacity:\n\n            self.concept_queue[start:end] = to_add\n        else:\n            split = capacity - start\n            self.concept_queue[start:] = to_add[:split]\n            self.concept_queue[:end % capacity] = to_add[split:]\n\n        self.queue_ptr = end % capacity\n        self.queue_count = min(self.queue_count + to_add.size(0), capacity)\n\n    @torch.no_grad()\n    def _enqueue_to_queue_buffer(self, new_concepts):\n        to_enqueue = new_concepts.size(0)\n        if to_enqueue == 0:\n            return\n\n        capacity = self.queue_max_size.item()\n        current = self.queue_count\n        overflow = max(0, current + to_enqueue - capacity)\n        important =  self.important_net(new_concepts).squeeze(-1)\n        keep_mask = important > 0.65\n        new_concepts = new_concepts[keep_mask]\n        if new_concepts.size(0) == 0:\n            return \n        if overflow >= new_concepts.size(0):\n            self._enqueue(new_concepts[-capacity:])\n            return  \n\n        kept_new = new_concepts[-(to_enqueue - overflow):]\n        self._enqueue(kept_new)\n\n    @torch.no_grad()\n    def _optimize_memory(self , aggressive = False):\n        if aggressive:\n            self._consolidate_important_memories()\n            self._merge_similar_slots()\n            self._prune_memories()\n            self._prune_slots()\n            self.neurogenesis()\n        else:\n            self._consolidate_important_memories()\n            self._merge_similar_slots()\n            self._prune_memories()\n            self.neurogenesis()\n\n\n        \n\n    def _update_energy_level(self):\n        # key_memory = self.mem_key_proj(self.key_memory)\n        importance = self.important_net(self.key_memory).squeeze()\n        assert torch.all(self.concept_energy >= 0)\n        assert torch.all(self.concept_energy <= 1.01)  \n        new_energy =  (\n            # self.decay_rate * \n            0.3 * self.concept_energy + 0.1 * self.usage + 0.6 * importance *(1-self.concept_energy)\n                    )\n        \n        deactivated  = ~self.active_mask \n\n        valid_deactivate =  deactivated.nonzero().squeeze()\n        valid_deactivate = valid_deactivate[(valid_deactivate>=0)&(valid_deactivate < self.memory_size)]\n        with torch.no_grad():\n            self.concept_energy.data =  torch.clamp(new_energy  , 0, 1)\n\n            if valid_deactivate.numel() >0:\n                self.key_memory.data[valid_deactivate] *=0.01\n                self.value_memory.data[valid_deactivate]*=0.01\n                self.cell_state.data[valid_deactivate]*=0.1\n\n        self.active_concepts =  torch.sum(self.active_mask).clamp(min=0 , max=self.memory_size)\n    \n    def forward(self ,x:torch.Tensor , training:bool = True):\n        if training:\n           x=  self.replay_consolidation(x=x)\n    \n        batch_size  , seq_len , _  = x.shape \n        self.step_count += 1\n        self.query_count += batch_size \n        compressed = self.compression(x.mean(dim=1))\n        query = self.W_cell(compressed)\n        k_active , v_active , c_active =  self._get_active_memory()\n        k  = k_active.unsqueeze(1).expand(-1 , batch_size ,-1)\n        v = v_active.unsqueeze(1).expand(-1, batch_size , -1)\n        assert k.size(1) == batch_size\n        # attn_output , attn_weights = self.attn(\n        #         query.unsqueeze(0), k ,v , need_weights =  True \n        #     )\n\n        # Cosine similarity instead of MHA\n        sims =  F.cosine_similarity(query.unsqueeze(1),\n                                     k_active.unsqueeze(0) , dim=-1)\n        # print(\"   pre‑write mean/sd:\", sims.mean().item(), sims.std().item())\n        top_vals , local_topk = sims.topk(self.top_k , dim=-1)\n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0] \n        topk_idx = active_indices[local_topk]\n        attn_weights = torch.zeros(1, batch_size, k_active.size(0), device=query.device)\n        attn_weights[0].scatter_(1, local_topk, 1.0)\n        # indices = torch.nonzero(self.active_mask, as_tuple=True)[0][topk_idx]\n\n        # self._boost_energy_on_access(indices.view(-1), sims.view(-1, k.size(0)))\n\n        v_exp    = v_active.unsqueeze(0).expand(batch_size, -1, -1)  \n        gathered = torch.gather(\n            v_exp,\n            1,\n            local_topk.unsqueeze(-1).expand(-1, -1, self.compress_dim)\n        )                                                             \n        retrieved = gathered.mean(dim=1)                            \n\n        attn_output = retrieved.unsqueeze(0)                        \n\n         \n\n        max_scores , _  = sims.max(dim = -1)\n        # if self.query_count < self.initial_write_step:\n        #     hit_thershold = 0.79\n        # else:\n        #     hit_thershold=  0.50\n        # hit_thershold = 0.5 + 0.2 * (self.memory_size / self.max_slots)\n        hit_thershold =  0.61\n        # print('hit_threshold', hit_thershold)\n        # query_proj = self.query_proj(query)    \n        # hit_threshold = 0.3 + 0.2 * (self.memory_size / self.max_slots)\n        max_scores = max_scores.squeeze(-1)\n        # print('max scores ',max_scores)\n        hits = (max_scores>hit_thershold).sum()\n        self.hit_count +=  hits\n        # print('hit count', self.hit_count)\n        # # self.novelty_threshold = torch.clamp(\n        #     torch.tensor(0.4 - 0.3 * (self.memory_size / self.max_slots)), \n        #     min=0.1, \n        #     max=0.5\n        # )     \n        #   \n        novel_mask =  max_scores <  hit_thershold\n        \n        \n        self.novel_count+= novel_mask.sum()\n        if novel_mask.any():\n                novel_projection =  query[novel_mask]\n            # sim_scores =  F.cosine_similarity(novel_projection.unsqueeze(1) , k_active.unsqueeze(0) , dim=-1)\n            # similarity_threshold = self.sim_thershold - 0.2 * (self.memory_size / self.max_slots)\n            # is_novel =  sim_scores.max(dim=-1).values< (self.sim_thershold - 0.2 * (self.memory_size/self.max_slots))\n            # write_mask = is_novel\n            # write_mask = sim_scores.max(dim=-1).values < similarity_threshold\n            # if write_mask.any():\n                if self.query_count >  0:\n                    self.flush_concept_queue()\n                # new_concepts =  novel_projection[write_mask]\n                new_concepts = novel_projection\n                self.write_count += new_concepts.size(0)\n                assert new_concepts.size(0) <= self.max_slots - self.memory_size \n                \"Exceeding maximum memory capacity\"\n                self._write_memory_update(new_concepts=new_concepts)\n                no_memory_out =  self.no_memory_embedding.repeat(batch_size , seq_len, 1)\n                return no_memory_out  , self.no_memory_embedding.squeeze(0), torch.tensor([], dtype= torch.long) , None \n        # attn_output = attn_output + torch.randn_like(attn_output) * 0.1\n        \n\n        with torch.no_grad():\n            self.usage *= 0.95\n            self.usage[topk_idx] +=0.2\n            self.usage.clamp(0,1)\n            self.usage.mul_(0.9)\n            self.usage.scatter_add_(0, topk_idx.flatten(), torch.ones_like(topk_idx, dtype=torch.float).flatten())\n            self.usage.clamp_(max=1.0)\n            # self.concept_energy[topk_idx] += 0.15 * max_scores.squeeze()\n            # self.concept_energy.clamp_(max=1.0)\n       \n        keys= self.key_memory[topk_idx]\n        value = self.value_memory[topk_idx]\n        cells = self.cell_state[topk_idx]\n        if training and self.query_count % 31 == 0:\n            self._update_energy_level()\n            self._update_thersholds()\n             \n        gate_input = torch.cat([\n            keys, cells, query.unsqueeze(1).expand(-1, self.top_k , -1)\n        ], dim= -1) \n        update_gates =  self.update_gate(gate_input.view(-1, 3 *self.compress_dim))\n        update_gates = update_gates.view(batch_size, self.top_k)\n        self._update_memory(topk_idx=topk_idx, cells=cells ,projected=query, update_gates=update_gates)\n        \n        # Project the output to the out \n        out =  self.memory_projection(retrieved)\n        out = out.unsqueeze(1).repeat(1, seq_len, 1)\n        self._memory_version +=1 \n        self._update_memory_metadata(topk_idx)\n        return  out , retrieved , topk_idx , attn_weights\n\n\n    @torch.no_grad()\n    def _gradual_influence_increase(self):\n        \"\"\"\n        Gradually increase the influence of newly added memory slots based on their age and access.\n        \"\"\"\n        new_slots_mask  = (self.age <= self.new_slot_maturation_steps) & self.active_mask \n        if not torch.any(new_slots_mask):\n            return  \n        \n        age_normalized = self.age[new_slots_mask] / self.new_slot_maturation_steps\n        usage_normalized =  self.usage[new_slots_mask]\n\n        growth_factor =torch.sigmoid((age_normalized + usage_normalized) * 3 ).unsqueeze(-1)\n\n        self.key_memory[new_slots_mask] = F.normalize(self.key_memory[new_slots_mask] * (1 + growth_factor * 0.5),\n        dim=-1)\n        self.value_memory[new_slots_mask] =  F.normalize(self.value_memory[new_slots_mask]* (1+growth_factor * 0.3) ,dim=-1 )\n\n        energy_boost = torch.clamp(0.1 * growth_factor.squeeze(), max=0.15)\n        self.concept_energy.data[new_slots_mask] = torch.clamp(\n        self.concept_energy[new_slots_mask] + energy_boost,\n        min=0.3,\n        max=0.7\n    )\n\n        self.age.data[new_slots_mask] += 1 \n\n        \n    def _consolidate_important_memories(self):\n        key_memory =  self.mem_key_proj(self.key_memory)\n        importance =  self.important_net(self.key_memory).squeeze()\n        consolidate_mask  = importance > 0.1\n        if consolidate_mask.any():\n          with torch.no_grad():\n            self.key_memory[consolidate_mask] = F.normalize(\n                self.key_memory[consolidate_mask] , dim=-1\n            )\n            mean_value = self.value_memory[consolidate_mask].mean(dim=0)\n           \n            self.value_memory[consolidate_mask] = (\n                    0.9 * self.value_memory[consolidate_mask] +\n                    0.1 * mean_value\n                )\n            self.concept_energy[consolidate_mask] = torch.clamp(self.concept_energy[consolidate_mask] + 0.05, 0, 1)\n            self.concept_energy[~consolidate_mask] *= 0.85\n            self.consalidate_count +=1 \n\n    # @torch.no_grad()\n    # def _prune_memories(self):\n    #     prune_condidate =  ((self.age > self.prune_age_threshold * 0.5 ) & (self.usage < 0.05) & (self.concept_energy < self.energy_threshold))\n    #     if prune_condidate.any():\n    #         self.key_memory.data[prune_condidate] *=  0.1\n    #         self.value_memory.data[prune_condidate]*=0.01\n    #         self.cell_state.data[prune_condidate] *= 0.01\n    #         self.age.data[prune_condidate] = 0 \n    #         self.usage[prune_condidate] = 0 \n    #         self.concept_energy.data[prune_condidate] = 0.1\n    #         self.prune_count +=1 \n    #         self._memory_version += prune_condidate.sum().item()\n\n    def _prune_memories(self):\n        # prune_mask: [max_slots]\n        prune_mask = torch.sigmoid((self.age - 100) / 20) * (1 - self.usage)\n        prune_mask *= torch.sigmoid(-self.concept_energy * 5)\n\n        # no need to unsqueeze for concept_energy\n        with torch.no_grad():\n            self.key_memory.data *= (1 - prune_mask.unsqueeze(-1) * 0.5)   # keeps shape [max_slots, dim]\n            self.concept_energy.data *= (1 - prune_mask * 0.3)             # shape [max_slots]\n\n\n    @torch.no_grad()\n    def _prune_slots(self):\n            mask = self.age > self.prune_age_threshold\n            if mask.any():\n                self.key_memory.data[mask] *= 0.01\n                self.value_memory.data[mask] *= 0.01\n                self.cell_state.data[mask] *= 0.01\n                self.concept_energy.data[mask] = 0\n                self.usage[mask]= 0 \n                self.age.data[mask] = 0 \n                self.prune_count +=1 \n                self.active_slots[mask] = False \n    \n\n\n    def replay_consolidation(self, x: torch.Tensor):\n\n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0] \n        active_key , active_value, _ = self._get_active_memory()\n        if  self.training and random.random() < 0.2: \n            high_energy_mask = self.concept_energy[active_indices] > 0.8\n            if high_energy_mask.sum() == 0:\n                return x \n            if high_energy_mask.sum() > 0:\n                replay_keys = active_key[high_energy_mask]\n                replay_values = active_value[high_energy_mask]\n                \n                replay_input = self.decompression(replay_values.mean(dim=0, keepdim=True))\n                B, T , D =  x.shape\n                self.replay_count+= 1\n                return replay_input.unsqueeze(1).expand(B,T,D)\n        return x\n\n    def get_reusable_slots(self ,num_needed:int):\n       \n\n        age_score =  1-torch.sigmoid(self.age / 100) # old age \n        energy_score = (1-  self.concept_energy ) *2 \n        usage_score = 1 - self.usage \n        reuse_scores = (0.4 * energy_score  + 0.3 * age_score + 0.3 * usage_score \n                       )\n\n        mask =(self.concept_energy  < self.energy_threshold) & (self.age< 100)\n        reuse_scores[~mask]= -float('inf')\n\n        topk_scores  , candidates = torch.topk(reuse_scores, min(num_needed, self.memory_size))\n        return candidates \n\n            \n    def _reinitialize_slot(self, idx):\n        \"\"\"Reset a slot to initial state\"\"\"\n        with torch.no_grad():\n            scale = 0.1 + 0.05 * torch.rand(1, device=idx.device)\n            self.key_memory[idx] = torch.randn_like(self.key_memory[idx]) * scale\n            self.value_memory[idx] = torch.randn_like(self.value_memory[idx]) * scale\n            self.cell_state[idx] =  0.2 * self.cell_state.data[idx].mean(dim=0)\n            \n            # Reset metadata\n            # neighbor_energy = self.concept_energy[idx±5].mean()   \n            self.concept_energy[idx] = 0.3 + 0.2 * torch.rand_like(self.concept_energy[idx])\n            # self.usage[idx] =  0.1 * torch.rand_like(self.usage[idx])\n            self.usage[idx] = 0.05\n            self.age[idx] = 0\n            self.memory_age[idx] = 0\n            self.access_count[idx] = 0\n            self.active_mask[idx] = True \n\n\n\n\n\n    def _consalidate_new_slots(self):\n        new_slot_indices =  torch.arange(self.memory_size - 10 , self.memory_size )\n        new_slot_energy = self.concept_energy[new_slot_indices]\n        with torch.no_grad():\n            self.concept_energy[new_slot_indices] = torch.clamp(\n                new_slot_energy + 0.1 * self.age[new_slot_indices] , 0 ,1\n            )\n            self.key_memory[new_slot_indices] *=  0.1\n            self.value_memory[new_slot_indices] *= 0.1\n            self.age[new_slot_indices] +=1\n            self.access_count[new_slot_indices] +=1 \n\n    def _update_memory_metadata(self, used_indices):\n       \n        self.access_count[used_indices] += 1\n        \n        self.memory_age += 1\n        self.memory_age[used_indices] = 0\n        with torch.no_grad():\n            self.concept_energy[used_indices] += 0.1\n            # self.concept_energy= torch.clamp(self.concept_energy * 0.95, 0, 1)\n            self.concept_energy.mul_(0.95).clamp_(0,1)\n\n            # self.age[used_indices] -= 5 \n    @torch.no_grad()\n    def neurogenesis(self, required_slots:int= 10 , return_index = False):\n        device = self.key_memory.device\n        if self.max_slots > self.memory_size:\n\n            usage_rate =  (self.usage > 0.1).float().mean()\n            if usage_rate > self.neurogenesis_threshold:\n                reusable =  self.get_reusable_slots()\n                num_reuse =  min(reusable.numel() , required_slots)\n                reused_indices = reusable[:num_reuse]\n                if num_reuse > 0:\n                    with torch.no_grad():\n                        device =  self.key_memory.device \n                        self._reinitialize_slot(idx=reused_indices)\n                        \n                new_slots =  min(max(0 ,    required_slots -  num_reuse ), self.max_slots- self.memory_size)\n                if new_slots > 0:\n                    start_idx =self.memory_size\n                    end_idx =  start_idx  + new_slots \n                    new_indices  = torch.arange(start_idx , end_idx, device = device )\n                  \n                   \n                    self.key_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.value_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.cell_state.data[new_indices] = 0\n                    self.concept_energy.data[new_indices] = 0.5\n                    self.usage.data[new_indices] = 0.0\n                    self.age.data[new_indices] = 0.0\n                    self.access_count.data[new_indices] = 0.0\n                    self.active_mask.data[new_indices] = True\n                    self.neuroslot_count +=1 \n                    self._gradual_influence_increase()\n                    self._memory_version += new_slots\n                    # self.new_slot_mask[new_indices] =  True \n\n\n                \n                    \n    \n       \n                if return_index:\n                    return torch.cat([reused_indices, new_indices]) if new_indices.numel() > 0 else reused_indices\n            \n        elif return_index:\n            return  torch.empty(0, dtype=torch.long, device=self.key_memory.device) \n\n    def emergency_recovery(self):\n        # Reset unstable memories\n        unstable = self.concept_energy < 0.2\n        self._reinitialize_slot(unstable)\n        \n        self._optimize_memory(aggressive=True)\n    def _protect_critical_memories(self):\n            # Protect top 10% of important memories\n            importance = self.important_net(self.key_memory).squeeze()\n            topk = importance.topk(int(self.max_slots * 0.1)).indices\n            self.concept_energy[topk] = 1.0\n            self.age[topk] -= 10\n    @torch.no_grad()\n    def _merge_similar_slots(self, top_k: int = 32):\n        device = self.key_memory.device\n        active_idx = torch.nonzero(self.active_mask, as_tuple=True)[0]\n        N = active_idx.size(0)\n        if N < 2:\n            return\n\n        # 1. Normalized vectors\n        keys = F.normalize(self.key_memory[active_idx], dim=-1)\n        values = F.normalize(self.value_memory[active_idx], dim=-1)\n        D = keys.size(-1)\n\n        # 2. Similarity search\n        K = min(top_k, N-1)\n        sims, nbrs = torch.topk(keys @ keys.T, k=K+1, dim=-1)\n        sims, nbrs = sims[:, 1:], nbrs[:, 1:]  # Remove self\n\n        # 3. Dynamic threshold\n        # pressure = torch.tensor(N / self.max_slots, device=device)\n        # threshold = (0.9 - 0.4 * pressure).clamp(0.65, 0.9)\n        # energy_factor = torch.sigmoid((self.concept_energy.mean() - 0.5) * 5)\n        confidence_factor = sims.mean()\n        utilization_factor = (self.concept_energy < 0.9).float().mean()\n        \n        # adaptive_threshold = 0.4 + 0.2 * self.concept_energy.mean() + 0.2 * confidence_factor + 0.2 * utilization_factor\n        # adaptive_threshold = 0.28\n        \n        adaptive_threshold = 0.25 \\\n            + 0.2 * mean_energy \\\n            + 0.2 * confidence_factor \\\n            + 0.2 * utilization_factor\n\n        high_energy_mask = self.concept_energy[active_idx] > 0.3\n        sims[~high_energy_mask.unsqueeze(1) | ~high_energy_mask.unsqueeze(0)] = 0\n        # mask = sims > threshold\n        mask = sims >  adaptive_threshold \n\n        # 4. Graph construction\n        row = torch.arange(N, device=device).unsqueeze(1).expand(-1, K)[mask]\n        col = nbrs[mask]\n        edges = torch.stack([\n            torch.cat([row, col]),\n            torch.cat([col, row])\n        ])\n\n        # 5. Label propagation\n        labels = torch.arange(N, device=device)\n        for _ in range(3):\n            neighbor_labels = labels[edges[1]]\n            updates = torch.minimum(labels[edges[0]], neighbor_labels)\n            labels.scatter_reduce_(0, edges[0], updates, reduce='amin')  # Fixed reduction\n\n        # 6. Cluster analysis\n        uniq, inv, counts = torch.unique(labels, return_inverse=True, return_counts=True)\n        cluster_mask = counts >= 2\n        big_clusters = uniq[cluster_mask]\n        big_counts = counts[cluster_mask]\n        num_clust = big_clusters.size(0)\n        if num_clust == 0:\n            return\n\n        # 7. Cluster mapping\n        cluster_id_map = torch.zeros(uniq.max()+1, dtype=torch.long, device=device)\n        cluster_id_map[big_clusters] = torch.arange(num_clust, device=device)\n        member_mask = torch.isin(inv, big_clusters)\n        global_idx = active_idx[member_mask]\n        cluster_ids = cluster_id_map[inv[member_mask]]  # Proper mapping\n        # 8. Energy aggregation\n        energy = self.concept_energy[global_idx]\n        weights = (energy / big_counts[cluster_ids].float()).unsqueeze(-1)\n        expanded_ids = cluster_ids.unsqueeze(-1).expand(-1, D)\n        # 8.1 Weighted sum of the seleceted slots datat \n        new_keys = torch.zeros((num_clust, D), device=device)\n        new_vals = torch.zeros_like(new_keys)\n        new_keys.scatter_add_(0, expanded_ids, keys[member_mask] * weights)\n        new_vals.scatter_add_(0, expanded_ids, values[member_mask] * weights)\n\n        # 9. Representative selection\n        cluster_ages = self.age[global_idx]\n        min_ages = torch.zeros(num_clust, device=device)\n        min_ages.scatter_reduce_(0, cluster_ids, cluster_ages, reduce='amin', include_self=False)\n        \n        # Find first occurrence of min age\n        _, sorted_idx = torch.sort(cluster_ids)\n        cluster_ids_sorted = cluster_ids[sorted_idx]\n        age_mask = (cluster_ages[sorted_idx] == min_ages[cluster_ids_sorted])\n        _, first_occurrence = torch.unique_consecutive(cluster_ids_sorted, return_inverse=True)\n        rep_mask = age_mask & (first_occurrence == 0)\n        rep_cluster_ids = cluster_ids_sorted[rep_mask]\n        representatives = global_idx[sorted_idx][rep_mask]\n    \n        # 10. Memory updates\n        self.key_memory[representatives] = F.normalize(new_keys[cluster_ids_sorted[rep_mask]], dim=-1)\n        self.value_memory[representatives] = F.normalize(new_vals[cluster_ids_sorted[rep_mask]], dim=-1)\n        \n        clust_energy = torch.bincount(cluster_ids, weights=energy, minlength=num_clust)\n        # self.concept_energy[representatives] = clust_energy[rep_cluster_ids].clamp(min=1e-5, max=1.0)\n        self.concept_energy[representatives] = torch.clamp(\n            clust_energy[rep_cluster_ids] * 1.2,  \n            min=0.7, \n            max=1.0\n        )\n     \n        self.merge_count+= 1 \n        self._memory_version += num_clust\n        # 11. Usage update\n        per_cluster_usage = torch.bincount(\n            cluster_ids,\n            weights=self.usage[global_idx],\n            minlength=num_clust\n        ).float() / big_counts.float() \n        # self.usage[representatives] = per_cluster_usage[rep_cluster_ids]\n        self.usage[representatives] = torch.clamp(\n            per_cluster_usage[rep_cluster_ids] * 1.5,\n            min=0.3,\n            max=1.0\n        )\n\n        # 12. Deactivation\n        active_mask_modified = torch.zeros_like(self.active_mask)\n        active_mask_modified[representatives] = True\n        deactivate_idx = member_mask & ~active_mask_modified[active_idx]\n        \n        if deactivate_idx.any():\n            to_deactivate = active_idx[deactivate_idx]\n            self.concept_energy[to_deactivate] *= 0.1\n            self.key_memory[to_deactivate] *= 0.1\n            self.value_memory[to_deactivate] *= 0.1\n            self.usage[to_deactivate] *= 0.25\n\n        self._consolidate_important_memories()\n        self._update_memory_metadata(representatives)\n            \n    @torch.no_grad()\n    def _update_thersholds(self , momentum:float=0.9 ):\n        hit_rate =  float(self.hit_count / max(self.query_count, 1))\n        write_rate =  float(self.write_count  / max(self.query_count , 1))\n        novely_rate =  float(self.novel_count / max(self.query_count  , 1))\n\n        util =  float(self.active_capacity)\n\n        new_nov =   (0.2 * (1-util) + 0.1 * novely_rate)\n        self.novelty_threshold =    momentum * self.novelty_threshold + (1-momentum) * new_nov\n\n        new_enger_thr = 0.3 + 0.3 *(1-hit_rate)\n        self.energy_threshold.data  = momentum * self.energy_threshold + (1-momentum) * new_enger_thr \n        \n        new_consal = 50.0 +50.0 * write_rate \n        self.consolidation_threshold.data.mul_(momentum).add_(new_consal * (1-momentum))\n\n        new_decay = 0.995 + 0.003 * (1-util)\n        self.decay_rate.data.mul_(momentum).add_(new_decay *(1-momentum))\n\n\n        new_prune_age = 100 * (1- util ) + 20 * util \n        self.prune_age_threshold.fill_(momentum * self.prune_age_threshold+(1-momentum) * new_prune_age)\n\n        new_neuro = 0.8 + 0.1 * write_rate - 0.1 * util\n        self.neurogenesis_threshold.fill_(momentum * self.neurogenesis_threshold + (1-momentum) * new_neuro)\n\n        new_mat = 50 + 50 * write_rate\n        self.new_slot_maturation_steps.fill_(momentum * self.new_slot_maturation_steps + (1-momentum) * new_mat)\n\n        new_scale = 0.05 + 0.2 * (1 - hit_rate)\n        self.synaptic_scale.data.mul_(momentum).add_(new_scale * (1-momentum))\n\n        new_sp = 0.5 + 0.3 * util\n        self.sparsity.data.mul_(momentum).add_(new_sp * (1-momentum))\n\n        new_sim = 0.5 - 0.2 * novely_rate\n        self.sim_thershold.data.mul_(momentum).add_(new_sim * (1-momentum))\n\n\n    \n\n        \n    def get_memory_metrics(self):\n\n        \"Return memory health and retivel param details\"\n        active_mask  =  self.concept_energy > self.energy_threshold\n        energy  =  self.concept_energy \n        usage =  self.usage \n        access =  self.access_count\n\n        age_hist   = torch.histc(self.memory_age.float(), bins=10, min=0, max=float(self.memory_age.max()))\n        usage_hist = torch.histc(usage, bins=10, min=0, max=1.0)\n        access_hist= torch.histc(access.float(), bins=10, min=0, max=float(access.max()))\n        active_concepts = self.active_mask &(self.concept_energy > 0.70)\n\n        return  {\n\n            #________Memory Health _______________________________\n            \n            'memory_size': self.memory_size  , \n            'total_memory_size':self.max_slots,\n            'active_concepts_with_high_energy':active_concepts.sum().item(),\n            'utilization':self.utilization , \n            'energy_mean':energy.mean().item(), \n            'energy_std':energy.std().item(), \n            'age_histogram': age_hist, \n            'usage_histogram':usage_hist, \n            'access_histogram':access_hist, \n            # 'merge_rate':(energy < 0.3).sum().item() / self.memory_size ,\n            'merge_rate':self.merge_count.item() / max(1 , self.step_count.item()),\n            # \"prune_rate\":((self.age > self.prune_age_threshold) & (usage < 0.01)).float().mean().item(),            \n            'prune_rate':self.prune_count.item() / max(1 , self.step_count.item()),\n            'neuro_rate':(self.memory_age < 10).float().mean().item(), \n            \"reuse_efficiency\":      access[energy > 0.5].float().mean().item(),\n\n             # —— retrieval/write stats ———————————————————————\n            \"steps\":                 self.step_count.item(),\n            \"queries\":               self.query_count.item(),\n            \"novelty_rate\":          self.novel_count.item() / max(1, self.query_count.item()),\n            \"write_rate\":            self.write_count.item() / max(1, self.query_count.item()),\n            \"hit_rate\":              self.hit_count.item() / max(1, self.query_count.item()),\n            'merge_count':self.merge_count.item() , \n            'neuroslot_count':self.neuroslot_count.item(), \n            'prune_count':self.prune_count.item(), \n            'consalidate_count':self.consalidate_count.item(),\n            'hit_count':self.hit_count.item() , \n            'write_count':self.write_count.item(),\n            'memory_version':self._memory_version.item(),\n            'access_count':self.access_count[self.active_mask].tolist(), \n            'replay_count':self.replay_count.item(),\n            'access_count_sum': self.access_count[self.active_mask].sum().item(),\n\n                'access_count_mean': self.access_count[self.active_mask].mean().item(),\n            'update_count':self.update_count.item()\n            \n        }\n\n\n        \n    def model_save(self , path):\n            torch.save({\n\n                'key_memory':self.key_memory.data.cpu(), \n                'value_memory':self.value_memory.data.cpu() , \n                'cell_state':self.cell_state.data.cpu(), \n                'active_mask':self.active_mask.cpu(),\n                'age':self.age.data.cpu() , \n                'usage':self.usage.data.cpu(), \n                'access_count':self.access_count.data.cpu(), \n                'memory_version':self._memory_version.data.cpu(), \n                'memory_age':self.memory_age.data.cpu(), \n                'concept_queue':self.concept_queue.data.cpu(), \n                'queue_ptr':self.queue_ptr , \n                'queue_count':self.queue_count , \n                'query_count':self.query_count , \n                'step_count':self.step_count , \n                'novel_count':self.novel_count , \n                'write_count':self.write_count , \n                'hit_count':self.hit_count , \n                'merge_count':self.merge_count , \n                'neuroslot_count':self.neuroslot_count , \n                'prune_count':self.prune_count , \n                'consalidate_count':self.consalidate_count ,\n                'memory_size':self.memory_size,\n                'replay_count':self.replay_count\n\n             } , path)\n            \n    def model_load(self, path, map_location=None):\n        state = torch.load(path, map_location=map_location)\n        self.key_memory.data.copy_(state['key_memory'])\n        self.value_memory.data.copy_(state['value_memory'])\n        self.cell_state.data.copy_(state['cell_state'])\n        self.active_mask.data.copy_(state['active_mask'])\n        self.age.data.copy_(state['age'])\n        self.usage.data.copy_(state['usage'])\n        self.access_count.data.copy_(state['access_count'])\n        self.memory_age.data.copy_(state['memory_age'])\n        self.concept_queue.data.copy_(state['concept_queue'])\n        self.queue_ptr = state.get('queue_ptr', 0)\n        self.queue_count = state.get('queue_count', 0)\n        self._memory_version.data.copy_(\n            state.get('memory_version', torch.tensor(0))\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.700903Z","iopub.execute_input":"2025-06-12T16:39:40.701090Z","iopub.status.idle":"2025-06-12T16:39:40.797211Z","shell.execute_reply.started":"2025-06-12T16:39:40.701073Z","shell.execute_reply":"2025-06-12T16:39:40.796538Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Transformer Block","metadata":{}},{"cell_type":"code","source":"\n\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention_V2(\n        d_in=cfg[\"emb_dim\"],\n        d_out=cfg[\"emb_dim\"],\n        context_length=cfg[\"context_length\"],\n        num_heads=cfg[\"n_heads\"],\n        dropout=cfg[\"drop_rate\"],\n        qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n    #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut # Add the original input back\n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut #C\n        return x\n\n\n\nclass TransformerBlock_v2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention =  MultiQueryAttentionBlock(d_model=cfg['emb_dim'], h=cfg['n_heads'] , dropout=cfg['drop_rate'], seq_len=  cfg['context_length'] ,qkv_bias=cfg['qkv_bias'])\n\n        self.feed_forward = FeedForward(cfg)\n\n        self.layernorm1 =  LayerNorm(cfg['emb_dim'])\n    \n        self.layernorm2 =  LayerNorm(cfg['emb_dim'])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x , mask= None , output_attentions =  False ):\n\n        attention_output , attention_weights =  self.attention(self.layernorm1(x) , mask =  mask, return_attn =  output_attentions)\n\n        ff_output =  self.feed_forward(self.layernorm2(x))\n\n        x =  x + self.drop_out(ff_output) + self.drop_out(attention_output) \n\n        if output_attentions:\n            return x , attention_weights\n        else:\n            return x , None \n\n        \nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.memory = MemorySystem(cfg=cfg)\n        self.feed_forward = FeedForward(cfg=cfg)\n        \n        self.norm1 = LayerNorm(cfg['emb_dim'])\n        self.norm2 = LayerNorm(cfg['emb_dim'])\n        self.norm3 = LayerNorm(cfg['emb_dim'])\n        \n        # Memory gate\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] * 2, cfg['emb_dim']),\n            nn.Sigmoid()\n        )\n        \n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        print('initial input shape ' , x.shape )\n        attn_out = self.attention(self.norm1(x), mask=mask)\n        x = x + self.dropout(attn_out)\n        \n        norm_x = self.norm2(x)\n        epic_out , semantic_out , memory_out = self.memory(norm_x)\n        \n        gate_input = torch.cat([norm_x, memory_out], dim=-1)\n        memory_gate = self.memory_gate(gate_input)\n        x = x + memory_gate * memory_out\n        \n        ff_out = self.feed_forward(self.norm3(x))\n        x = x + self.dropout(ff_out)\n        \n        return x\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg, shared_memory=None):\n        super().__init__()\n        # Core components\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.ffn = FeedForward(cfg=cfg)\n        \n        # Memory system (shared across blocks)\n        self.memory = shared_memory or MemorySystem(cfg=cfg)\n        \n        # Normalization layers\n        self.pre_ln_attn = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_mem = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_ffn = RMSNorm(cfg['emb_dim'])\n        \n        # Adaptive memory gating\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'], 1),\n            nn.Sigmoid()\n        )\n        \n        # Memory residual weights\n        self.mem_alpha = nn.Parameter(torch.tensor(0.5))\n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        print('initail input shape ' , x.shape)\n        # Attention phase\n        resid = x\n        x = self.pre_ln_attn(x)\n        x = resid + self.dropout(self.attention(x, mask=mask))\n        \n        # Memory phase\n        resid_mem = x\n        x_mem = self.pre_ln_mem(x)\n        # print('x shape ', x.shape)\n        _, _, memory_out = self.memory(x_mem)\n        \n        # Adaptive gating\n        gate = self.memory_gate(x_mem)\n        x = resid_mem + self.mem_alpha * gate * memory_out\n        \n        # FFN phase\n        resid_ffn = x\n        x = self.pre_ln_ffn(x)\n        x = resid_ffn + self.dropout(self.ffn(x))\n        \n        return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.797886Z","iopub.execute_input":"2025-06-12T16:39:40.798204Z","iopub.status.idle":"2025-06-12T16:39:40.812699Z","shell.execute_reply.started":"2025-06-12T16:39:40.798176Z","shell.execute_reply":"2025-06-12T16:39:40.812042Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# GPTQModel","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int, embdding_layer: nn.Embedding):\n        super().__init__()\n        self.weight = embdding_layer.weight  # share weights with input embedding\n        self.bias = nn.Parameter(torch.zeros(vocab_size))  # learnable bias\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\n\n\n\nclass GPTMQModel2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim = cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel1(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks =  nn.ModuleList([\n            TransformerBlockWithMemory(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n        \n        # Shared memory system across layers\n        self.shared_memory = MemorySystem(cfg=cfg)\n        \n        # Transformer blocks with shared memory\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlockWithMemory(\n                cfg=cfg,\n                shared_memory=self.shared_memory if cfg['share_memory'] else None\n            ) for _ in range(cfg['n_layers'])\n        ])\n        \n        # Final projections\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.projection = ProjectionLayer(\n            cfg['emb_dim'], \n            cfg['vocab_size'], \n            self.embedding.embeddings\n        )\n        self.memory_retention_alpha = nn.Parameter(torch.tensor(0.9))\n\n        # Memory loss coefficient\n        self.mem_loss_coef = cfg.get('mem_loss_coef', 0.3)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n        \n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)\n            x = self.memory_retention_alpha * x + (1 - self.memory_retention_alpha) * x.detach()\n            \n        x = self.final_norm(x)\n        logits = self.projection(x)\n        \n        return logits\n    \n    def get_memory_loss(self):\n        \"\"\"Get combined memory regularization loss\"\"\"\n        return self.mem_loss_coef * self.shared_memory.memory_loss()\n    \n    def transformer_parameters(self):\n        return [p for n, p in self.named_parameters() if 'transformer_blocks' in n and p.requires_grad]\n    \n    def memory_parameters(self):\n        return [p for n, p in self.named_parameters() if 'memory_modules' in n and p.requires_grad]\n    \n    def embedding_parameters(self):\n        return [p for n, p in self.named_parameters() if 'embedding' in n and p.requires_grad]\n    \n    def norm_parameters(self):\n        return [p for n, p in self.named_parameters() if 'normalization' in n and p.requires_grad]\n    \n    def output_parameters(self):\n        return [p for n, p in self.named_parameters() if 'output_projection' in n and p.requires_grad]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.813399Z","iopub.execute_input":"2025-06-12T16:39:40.813597Z","iopub.status.idle":"2025-06-12T16:39:40.841273Z","shell.execute_reply.started":"2025-06-12T16:39:40.813570Z","shell.execute_reply":"2025-06-12T16:39:40.840714Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Memort Enhanced GPTQ Model","metadata":{}},{"cell_type":"code","source":"\nclass GPTMemoryEnhanced(nn.Module):\n    def __init__(self,cfg ):\n        super().__init__()\n        self.embedding =  InputEmbedding(cfg['vocab_size'] , cfg['emb_dim'])\n        self.memory_proj = nn.Linear(cfg['emb_dim'], cfg['memory_dim'])\n        self.memory_expander = nn.Linear(cfg['memory_dim'], cfg['emb_dim'] )\n        self.transformer_block = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n        self.dropout =  nn.Dropout(cfg['drop_rate'])\n        self.memory =  EfiBioSemanticMemory_V2(input_dim=cfg['memory_dim'] ,semantic_memory_dim=cfg['memory_dim'],num_heads=2)\n\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.fusion_gate = nn.Sequential(\n                nn.Linear(2*cfg['emb_dim'], cfg['emb_dim']),\n                nn.Sigmoid()\n            )\n        \n\n        self.projection =  ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'] , self.embedding.embeddings)\n\n    def forward(self,input_tokens:torch.Tensor , mask =None , output_hidden_state =  False , output_attentions =  False ):\n        x =  self.embedding(input_tokens)\n        x_emb = x \n        hidden_states = [x] if output_hidden_state else None\n        all_attentions = [] if output_attentions else None\n        for block in self.transformer_block:\n            x , attn = block(x, mask = mask ,output_attentions =  output_attentions)\n            if output_hidden_state:\n                hidden_states.append(x)\n            if output_attentions:\n                all_attentions.append(attn)\n                \n        memory_query = self.memory_proj(x) \n        mem_out, retrieved, topk_idx, attn_w = self.memory(memory_query)\n        # memory_out, _ ,_ =  self.memory(x.las_hidden_state.mean(1))\n        mem_out = self.memory_expander(mem_out)\n        gate = self.fusion_gate(torch.cat([x, mem_out], -1))\n        fused = gate * x + (1 - gate) * mem_out\n        fused =  self.final_norm(fused)\n\n        logits =  self.projection(fused)\n        # return logits, {\n        #     \"memory_topk\": topk_idx, \n        #     \"memory_attn\": attn_w,\n        #     \"retrieved\":  retrieved\n        # }\n        # return  logits ,x_emb ,  mem_out\n\n        return {\n            \"logits\": logits,\n            'x_emb':x_emb ,\n            'memory_output':mem_out, \n            \"hidden_states\": hidden_states,\n            \"attentions\": all_attentions,\n            \"retrieved_memory\": retrieved,\n            # 'attention_weights':all_attentions\n        }\n\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.842052Z","iopub.execute_input":"2025-06-12T16:39:40.842308Z","iopub.status.idle":"2025-06-12T16:39:40.866112Z","shell.execute_reply.started":"2025-06-12T16:39:40.842287Z","shell.execute_reply":"2025-06-12T16:39:40.865554Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Dataset and DataLoader ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef generate_prompt(sample):\n    # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n        return f\"User:\\n{sample['instruction']} \\nAssistant:{sample['output']} <|endoftext|>\"\n    \n\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer =  tokenizer\n\n        all_tokens = []\n        allowed = {'<|endoftext|>'}\n        for sample in data:\n            prompt = generate_prompt(sample)\n            # tokens = tokenizer.encode(prompt , allowed_special=allowed)\n            tokens =  tokenizer.encode(prompt)\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n    return inputs, targets\n\ndef create_dataloader_v1(data,tokenizer , batch_size=4,\n    max_length=256, stride=128, shuffle=True, drop_last=True ):\n    tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n    dataset = Dataset_V1(data, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=collate_fn)\n    return dataloader\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-12T16:39:40.866853Z","iopub.execute_input":"2025-06-12T16:39:40.867048Z","iopub.status.idle":"2025-06-12T16:39:40.949033Z","shell.execute_reply.started":"2025-06-12T16:39:40.867030Z","shell.execute_reply":"2025-06-12T16:39:40.948481Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef generate_prompt(sample):\n\n    return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n    # return f\"User:\\n{sample['instruction']} \\nAssistant:{sample['output']} <|endoftext|>\"\n \n    # return (\n    #     \"<|system|>\\n\"\n    #     \"You are a helpful assistant.\\n\"\n    #     \"<|user|>\\n\"\n    #     f\"{sample['instruction'] }\\n {sample['input']}\"\n        \n    #     \"<|assistant|>\\n\"\n    #     f\"{sample['output']}<|endoftext|>\"\n    # )\n\n\n\n    \n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer = tokenizer\n\n        all_tokens = []\n        for sample in data:\n            prompt = generate_prompt(sample)\n            tokens = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\nclass Dataset_KD(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.samples = []\n\n        for sample in data:\n            prompt = generate_prompt(sample)\n            enc = tokenizer(prompt, truncation=True, max_length=max_length,\n                            padding=\"max_length\", \n                            return_tensors=\"pt\",\n                            add_special_tokens=False ,padding_side = 'right')\n\n            input_tensor = enc[\"input_ids\"].squeeze(0)\n            self.samples.append(input_tensor)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx]\n        return input_ids, input_ids.clone()\nclass KDPretrainDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = []\n        for sample in data:\n            prompt = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n\"\n            target = sample['output']\n            full_text = prompt + target + \" <|endoftext|>\"\n\n            tokenized = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n\n            self.data.append({\n                \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n                \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n                \"prompt_len\": len(tokenizer(prompt)[\"input_ids\"]),  # optional\n            })\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=teacher_tokenizer.pad_token_id)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)\n    return inputs, targets\n\ndef create_dataloader_v1(data, tokenizer, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n    dataset = KDPretrainDataset(data, tokenizer, max_length)#(data, tokenizer, max_length, stride)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n                            drop_last=drop_last, collate_fn=collate_fn,pin_memory=True, num_workers=2 )\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.949803Z","iopub.execute_input":"2025-06-12T16:39:40.950095Z","iopub.status.idle":"2025-06-12T16:39:40.961942Z","shell.execute_reply.started":"2025-06-12T16:39:40.950067Z","shell.execute_reply":"2025-06-12T16:39:40.961237Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Psycology Dataloader \n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport tiktoken\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass Dataset_v2(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n        self.input_ids = []\n\n        all_tokens = []\n        for sample in data:\n            tokens = tokenizer.encode(sample)  \n            all_tokens.extend(tokens)\n\n        # Split the tokens into chunks of size max_length with stride\n        for i in range(0, len(all_tokens) - self.max_length, self.stride):\n            input_chunk = all_tokens[i:i + self.max_length]\n            target_chunk = all_tokens[i + 1:i + self.max_length + 1]\n            self.input_ids.append((torch.tensor(input_chunk), torch.tensor(target_chunk)))\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, index):\n        return self.input_ids[index]\n\ndef collect_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0) \n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  \n    return inputs, targets\n\ndef create_dataloader_v2(data, batch_size=4, max_length=1024, stride=128, shuffle=True, drop_last=True):\n    # tokenizer = tiktoken.get_encoding(\"gpt2\")  \n    tokenizer =  teacher_tokenizer\n    dataset = Dataset_v2(data, tokenizer, max_length, stride) \n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=collect_fn)\n    return dataloader\n\ndef load_txt_file(filepath):\n    with open(filepath, 'r') as f:\n        text = f.read()\n    return text\n\ndef split_into_chunks(text, chunk_size=1024, overlap=200):\n\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        chunks.append(chunk)\n\n    return chunks\n\n\n\nfile =  '/kaggle/input/datasetcleaned/cleaned_books.txt'\nload_text =  load_txt_file(file)\nchunk = split_into_chunks(load_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:40.962663Z","iopub.execute_input":"2025-06-12T16:39:40.962921Z","iopub.status.idle":"2025-06-12T16:39:43.263570Z","shell.execute_reply.started":"2025-06-12T16:39:40.962901Z","shell.execute_reply":"2025-06-12T16:39:43.262920Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nclass LMPretrainDataset(Dataset):\n    def __init__(self, text: str, tokenizer, max_length: int, stride: int):\n        self.pad_id = tokenizer.pad_token_id\n        all_ids = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n        self.examples = []\n\n        # slide a window of size max_length+1\n        for i in range(0, len(all_ids) - 1, stride):\n            window = all_ids[i : i + max_length + 1]\n            inp = window[:-1]\n            tgt = window[1:]\n\n            # pad both to exactly max_length\n            if len(inp) < max_length:\n                pad_len = max_length - len(inp)\n                inp = inp + [self.pad_id] * pad_len\n                tgt = tgt + [-100] * pad_len  # -100 will be ignored in loss\n\n            self.examples.append((\n                torch.tensor(inp, dtype=torch.long),\n                torch.tensor(tgt, dtype=torch.long),\n            ))\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        return self.examples[idx]\n\ndef lm_collate_fn(batch):\n    inputs, targets = zip(*batch)\n    return torch.stack(inputs), torch.stack(targets)\n\ndef create_lm_dataloader(\n    text: str,\n    tokenizer,\n    batch_size: int = 4,\n    max_length: int = 1024,\n    stride: int = 512,\n    shuffle: bool = True,\n    drop_last: bool = True,\n):\n    ds = LMPretrainDataset(text, tokenizer, max_length, stride)\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        collate_fn=lm_collate_fn,\n        pin_memory=True,\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:43.264234Z","iopub.execute_input":"2025-06-12T16:39:43.264477Z","iopub.status.idle":"2025-06-12T16:39:43.272141Z","shell.execute_reply.started":"2025-06-12T16:39:43.264456Z","shell.execute_reply":"2025-06-12T16:39:43.271247Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nfrom torch.nn.utils.rnn import pad_sequence\ndef load_txt_file(filepath):\n    with open(filepath, 'r') as f:\n        text = f.read()\n    return text\n\nclass LMPretrainDataset(Dataset):\n    def __init__(self, text: str, tokenizer, max_length: int, stride: int):\n        \"\"\"\n        text: one big string to train on\n        tokenizer: a deepseek base model tokenizer\n        max_length: chunk length\n        stride: how far to slide window\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n\n        # 1) tokenize entire book (no special tokens)\n        tokens = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n\n        # 2) slide a window of size max_length+1, stride steps\n        self.inputs = []\n        self.targets = []\n        for i in range(0, len(tokens) - max_length, stride):\n            chunk = tokens[i : i + max_length + 1]\n            self.inputs.append(torch.tensor(chunk[:-1], dtype=torch.long))\n            self.targets.append(torch.tensor(chunk[1:], dtype=torch.long))\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]\n\n\ndef lm_collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=teacher_tokenizer.pad_token_id)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)\n    return inputs, targets\n\n\ndef create_lm_dataloader(\n    text: str,\n    tokenizer,\n    batch_size: int = 4,\n    max_length: int = 1024,\n    stride: int = 128,\n    shuffle: bool = True,\n    drop_last: bool = True,\n):\n    ds = LMPretrainDataset(text, tokenizer, max_length, stride)\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        collate_fn=lm_collate_fn,\n        pin_memory=True,\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:43.273089Z","iopub.execute_input":"2025-06-12T16:39:43.273525Z","iopub.status.idle":"2025-06-12T16:39:43.354092Z","shell.execute_reply.started":"2025-06-12T16:39:43.273493Z","shell.execute_reply":"2025-06-12T16:39:43.353446Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# books = load_txt_file(\"/kaggle/input/datasetcleaned/cleaned_books.txt\")\n\n# lm_dl = create_lm_dataloader(books, teacher_tokenizer , batch_size=6, max_length=256, stride=128)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:43.354909Z","iopub.execute_input":"2025-06-12T16:39:43.355190Z","iopub.status.idle":"2025-06-12T16:39:43.377307Z","shell.execute_reply.started":"2025-06-12T16:39:43.355161Z","shell.execute_reply":"2025-06-12T16:39:43.376669Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport unicodedata\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef load_txt_file(filepath, chunk_size=10000):\n    \"\"\"Load text in manageable chunks\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n            yield chunk\n\nclass EfficientLMDataset(Dataset):\n    def __init__(self, text_generator, tokenizer, max_length=1024, stride=256):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n        self.samples = []\n        \n        for text_chunk in text_generator:\n            text_chunk  = self.clean_text(text_chunk)\n            tokens = tokenizer(\n                text_chunk,\n                add_special_tokens=False,\n                truncation=False,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                return_offsets_mapping=False\n            )['input_ids']\n            \n            # Create training samples\n            for token_list in tokens:\n                for i in range(0, len(token_list) - self.max_length, self.stride):\n                    input_chunk = token_list[i : i + self.max_length]\n                    target_chunk = token_list[i + 1 : i + self.max_length + 1]\n            \n                    # truncate target_chunk if too long\n                    if len(target_chunk) != len(input_chunk):\n                        target_chunk = target_chunk[:len(input_chunk)]\n            \n                    self.samples.append((\n                        torch.tensor(input_chunk, dtype=torch.long),\n                        torch.tensor(target_chunk, dtype=torch.long)\n                    ))\n    \n\n    def clean_text(self,text):\n        text =  text.lower()\n        text = unicodedata.normalize(\"NFKC\", text)\n        text =  text.replace('\\n' ,' ')\n        text = text.replace(\"\\u00a0\", \" \")  # non-breaking space\n        text = text.replace(\"\\u2013\", \"-\")  # en-dash to dash\n        text = text.replace(\"\\ufeff\", \"\")   # remove BOM\n        text = text.strip()\n        return text\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n    def save(self):\n        torch.save(self.samples, 'tokenized_samples.pt')\n        print('Tokenizer samples save in  tokenized_samples.pt  ')\n\ndef lm_collate_fn(batch, pad_token_id):\n    inputs, targets = zip(*batch)\n    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=pad_token_id)\n    targets_padded = pad_sequence(targets, batch_first=True, padding_value=-100)\n    return inputs_padded, targets_padded\ndef collate_with_pad(batch, pad_token_id):\n    return lm_collate_fn(batch, pad_token_id)\n\ndef create_lm_dataloader(\n    file_path,\n    tokenizer,\n    batch_size=4,\n    max_length=1024,\n    stride=256,\n    shuffle=True,\n    drop_last=True,\n    text_chunk_size=10000\n):\n    text_generator = load_txt_file(file_path, chunk_size=text_chunk_size)\n    \n    dataset = EfficientLMDataset(\n        text_generator,\n        tokenizer,\n        max_length=max_length,\n        stride=stride\n    )\n    \n    # collate_fn = lambda batch: lm_collate_fn(batch, tokenizer.pad_token_id)\n    from functools import partial\n    collate_fn = partial(collate_with_pad, pad_token_id=tokenizer.pad_token_id)\n    torch.save(dataset , 'phycology_dataset.pt')\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        collate_fn=collate_fn,\n        pin_memory=True,\n        num_workers=2\n    )\n\n# if __name__ == \"__main__\":\n\n#     books_path = \"/kaggle/input/datasetcleaned/cleaned_books.txt\"\n#     lm_dl = create_lm_dataloader(\n#         books_path,\n#         teacher_tokenizer,\n#         batch_size=6,\n#         max_length=256,\n#         stride=128\n#     )\n    \n#     for inputs, targets in lm_dl:\n#         print(\"Input shape:\", inputs.shape)\n#         print(\"Target shape:\", targets.shape)\n#         break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:54.303364Z","iopub.execute_input":"2025-06-12T16:39:54.303738Z","iopub.status.idle":"2025-06-12T16:39:54.315655Z","shell.execute_reply.started":"2025-06-12T16:39:54.303705Z","shell.execute_reply":"2025-06-12T16:39:54.314724Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# torch.save(lm_dl, 'phycology_dataloader.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:43.403222Z","iopub.execute_input":"2025-06-12T16:39:43.403464Z","iopub.status.idle":"2025-06-12T16:39:43.423797Z","shell.execute_reply.started":"2025-06-12T16:39:43.403444Z","shell.execute_reply":"2025-06-12T16:39:43.423072Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_loader) , len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:43.553584Z","iopub.status.idle":"2025-06-12T16:39:43.553815Z","shell.execute_reply":"2025-06-12T16:39:43.553720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = next(iter(val_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:15:25.033604Z","iopub.execute_input":"2025-06-12T16:15:25.033960Z","iopub.status.idle":"2025-06-12T16:15:25.252118Z","shell.execute_reply.started":"2025-06-12T16:15:25.033927Z","shell.execute_reply":"2025-06-12T16:15:25.250920Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"\n# len(lm_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T06:12:00.773540Z","iopub.execute_input":"2025-06-11T06:12:00.773904Z","iopub.status.idle":"2025-06-11T06:12:00.779981Z","shell.execute_reply.started":"2025-06-11T06:12:00.773873Z","shell.execute_reply":"2025-06-11T06:12:00.778919Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"26687"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# teacher_model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T06:22:18.661299Z","iopub.execute_input":"2025-06-10T06:22:18.661694Z","iopub.status.idle":"2025-06-10T06:22:18.666637Z","shell.execute_reply.started":"2025-06-10T06:22:18.661663Z","shell.execute_reply":"2025-06-10T06:22:18.665902Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"print('input :',teacher_tokenizer.decode(sample[0][0]))\nprint('\\ntarget :', teacher_tokenizer.decode(sample[1][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:15:45.373097Z","iopub.execute_input":"2025-06-12T16:15:45.373392Z","iopub.status.idle":"2025-06-12T16:15:45.379939Z","shell.execute_reply.started":"2025-06-12T16:15:45.373369Z","shell.execute_reply":"2025-06-12T16:15:45.379133Z"}},"outputs":[{"name":"stdout","text":"input :  your guests so you won’t be tempted to eat them. 48. if someone in your dining party orders fries for the table, make sure you have a glass of water or green tea to sip on while they munch. it will keep you occupied and help prevent you from reaching for a fry, or two, or twenty. 49. at the grocery store checkout stand, keep your eyes focused on the checker so you don’t have to look at the candy and other impulse-buy items calling out to you. 50. make it a rule never to take free food samples anywhere! get smart to get thinner “i used to be around people who told me i was fat. i have taken charge of that and don’t have critical people around me anymore. now i’ve surrounded myself with people who express appreciation and encouragement. i didn’t realize how others’ negativity was affecting me.” —laura hold the bread before meals why do restaurants serve baskets of bread before each meal for free? why not cheese? why not almonds or chunks of beef or chicken? the reason is that bread makes you hungrier and encourages you to eat more. bread, especially white bread made from bleached and processed flour, spikes\n\ntarget :  guests so you won’t be tempted to eat them. 48. if someone in your dining party orders fries for the table, make sure you have a glass of water or green tea to sip on while they munch. it will keep you occupied and help prevent you from reaching for a fry, or two, or twenty. 49. at the grocery store checkout stand, keep your eyes focused on the checker so you don’t have to look at the candy and other impulse-buy items calling out to you. 50. make it a rule never to take free food samples anywhere! get smart to get thinner “i used to be around people who told me i was fat. i have taken charge of that and don’t have critical people around me anymore. now i’ve surrounded myself with people who express appreciation and encouragement. i didn’t realize how others’ negativity was affecting me.” —laura hold the bread before meals why do restaurants serve baskets of bread before each meal for free? why not cheese? why not almonds or chunks of beef or chicken? the reason is that bread makes you hungrier and encourages you to eat more. bread, especially white bread made from bleached and processed flour, spikes your\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"teacher_tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T11:33:44.620484Z","iopub.execute_input":"2025-06-09T11:33:44.620783Z","iopub.status.idle":"2025-06-09T11:33:44.625844Z","shell.execute_reply.started":"2025-06-09T11:33:44.620758Z","shell.execute_reply":"2025-06-09T11:33:44.624961Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"100000"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"sample[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T11:52:18.737707Z","iopub.execute_input":"2025-06-09T11:52:18.738182Z","iopub.status.idle":"2025-06-09T11:52:18.743316Z","shell.execute_reply.started":"2025-06-09T11:52:18.738146Z","shell.execute_reply":"2025-06-09T11:52:18.742597Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"torch.Size([6, 256])"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"prompt = \"User: What is the capital of France?\\nAssistant:\"\ninput_ids = teacher_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n\n# Generate output\noutput = teacher_model.generate(input_ids, max_length=100, num_return_sequences=1)\nresponse = teacher_tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T06:52:56.459903Z","iopub.execute_input":"2025-06-10T06:52:56.460198Z","iopub.status.idle":"2025-06-10T06:53:08.617778Z","shell.execute_reply.started":"2025-06-10T06:52:56.460176Z","shell.execute_reply":"2025-06-10T06:53:08.616850Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"User: What is the capital of France?\nAssistant: Paris.\nUser: What is the capital of Germany?\nAssistant: Berlin.\nUser: What is the capital of Italy?\nAssistant: Rome.\nUser: What is the capital of Spain?\nAssistant: Madrid.\nUser: What is the capital of the United States?\nAssistant: Washington, D.C.\nUser: What is the capital of the United Kingdom?\nAssistant: London.\nUser\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"sample = sample[0].to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T07:41:30.284785Z","iopub.execute_input":"2025-06-11T07:41:30.285111Z","iopub.status.idle":"2025-06-11T07:41:30.289665Z","shell.execute_reply.started":"2025-06-11T07:41:30.285087Z","shell.execute_reply":"2025-06-11T07:41:30.288833Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:55:49.314225Z","iopub.execute_input":"2025-06-09T13:55:49.314543Z","iopub.status.idle":"2025-06-09T13:55:49.564799Z","shell.execute_reply.started":"2025-06-09T13:55:49.314520Z","shell.execute_reply":"2025-06-09T13:55:49.563535Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mSignature:\u001b[0m     \n \u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding_side\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mType:\u001b[0m           LlamaTokenizerFast\n\u001b[0;31mString form:\u001b[0m   \nLlamaTokenizerFast(name_or_path='deepseek-ai/deepseek-llm-7b-base', vocab_size=100000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t100000: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t100001: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t100002: AddedToken(\"ø\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100003: AddedToken(\"ö\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100004: AddedToken(\"ú\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100005: AddedToken(\"ÿ\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100006: AddedToken(\"õ\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100007: AddedToken(\"÷\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100008: AddedToken(\"û\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100009: AddedToken(\"ý\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100010: AddedToken(\"À\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100011: AddedToken(\"ù\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100012: AddedToken(\"Á\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100013: AddedToken(\"þ\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n\t100014: AddedToken(\"ü\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n}\n)\n\u001b[0;31mLength:\u001b[0m         100015\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.10/dist-packages/transformers/models/llama/tokenization_llama_fast.py\n\u001b[0;31mSource:\u001b[0m        \n\u001b[0;32mclass\u001b[0m \u001b[0mLlamaTokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n\u001b[0;34m    Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    This uses notably ByteFallback and no normalization.\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    ```python\u001b[0m\n\u001b[0;34m    >>> from transformers import LlamaTokenizerFast\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\u001b[0m\n\u001b[0;34m    >>> tokenizer.encode(\"Hello this is a test\")\u001b[0m\n\u001b[0;34m    [1, 15043, 445, 338, 263, 1243]\u001b[0m\n\u001b[0;34m    ```\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    If you want to change the `bos_token` or the `eos_token`, make sure to specify them when initializing the model, or\u001b[0m\n\u001b[0;34m    call `tokenizer.update_post_processor()` to make sure that the post-processing is correctly done (otherwise the\u001b[0m\n\u001b[0;34m    values of the first token and final token of an encoded sequence will not be correct). For more details, checkout\u001b[0m\n\u001b[0;34m    [post-processors] (https://huggingface.co/docs/tokenizers/api/post-processors) documentation.\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\u001b[0m\n\u001b[0;34m    refer to this superclass for more information regarding those methods.\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    Args:\u001b[0m\n\u001b[0;34m        vocab_file (`str`, *optional*):\u001b[0m\n\u001b[0;34m            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\u001b[0m\n\u001b[0;34m            contains the vocabulary necessary to instantiate a tokenizer.\u001b[0m\n\u001b[0;34m        tokenizer_file (`str`, *optional*):\u001b[0m\n\u001b[0;34m            [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\u001b[0m\n\u001b[0;34m            contains everything needed to load the tokenizer.\u001b[0m\n\u001b[0;34m        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\u001b[0m\n\u001b[0;34m            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\u001b[0m\n\u001b[0;34m            extra spaces.\u001b[0m\n\u001b[0;34m        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\u001b[0m\n\u001b[0;34m            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\u001b[0m\n\u001b[0;34m            token instead.\u001b[0m\n\u001b[0;34m        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<s>\"`):\u001b[0m\n\u001b[0;34m            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\u001b[0m\n\u001b[0;34m        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"</s>\"`):\u001b[0m\n\u001b[0;34m            The end of sequence token.\u001b[0m\n\u001b[0;34m        add_bos_token (`bool`, *optional*, defaults to `True`):\u001b[0m\n\u001b[0;34m            Whether or not to add an `bos_token` at the start of sequences.\u001b[0m\n\u001b[0;34m        add_eos_token (`bool`, *optional*, defaults to `False`):\u001b[0m\n\u001b[0;34m            Whether or not to add an `eos_token` at the end of sequences.\u001b[0m\n\u001b[0;34m        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\u001b[0m\n\u001b[0;34m            Whether or not the default system prompt for Llama should be used\u001b[0m\n\u001b[0;34m        legacy (`bool`, *optional*):\u001b[0m\n\u001b[0;34m            Whether or not the `legacy` behavior of the tokenizer should be used. Legacy is before the merge of #24622\u001b[0m\n\u001b[0;34m            and #25224 which includes fixes to properly handle tokens that appear after special tokens.\u001b[0m\n\u001b[0;34m            Make sure to also set `from_slow` to `True`.\u001b[0m\n\u001b[0;34m            A simple example:\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m            - `legacy=True`:\u001b[0m\n\u001b[0;34m            ```python\u001b[0m\n\u001b[0;34m            >>> from transformers import LlamaTokenizerFast\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=True, from_slow=True)\u001b[0m\n\u001b[0;34m            >>> tokenizer.encode(\"Hello <s>.\") # 869 is '▁.'\u001b[0m\n\u001b[0;34m            [1, 15043, 29871, 1, 869]\u001b[0m\n\u001b[0;34m            ```\u001b[0m\n\u001b[0;34m            - `legacy=False`:\u001b[0m\n\u001b[0;34m            ```python\u001b[0m\n\u001b[0;34m            >>> from transformers import LlamaTokenizerFast\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=False, from_slow=True)\u001b[0m\n\u001b[0;34m            >>> tokenizer.encode(\"Hello <s>.\")  # 29889 is '.'\u001b[0m\n\u001b[0;34m            [1, 15043, 29871, 1, 29889]\u001b[0m\n\u001b[0;34m            ```\u001b[0m\n\u001b[0;34m            Checkout the [pull request](https://github.com/huggingface/transformers/pull/24565) for more details.\u001b[0m\n\u001b[0;34m        add_prefix_space (`bool`, *optional*):\u001b[0m\n\u001b[0;34m            Whether or not the tokenizer should automatically add a prefix space\u001b[0m\n\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvocab_files_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCAB_FILES_NAMES\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mslow_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding_side\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmodel_input_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mbos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0madd_bos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0madd_eos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0muse_default_system_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mlegacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlegacy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34mf\"You are using the default legacy behaviour of the {self.__class__}. This is\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34m\" expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you.\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34m\" If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34m\" means, and thoroughly read the reason why this was added as explained in\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34m\" https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34m\" you can ignore this message.\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mlegacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0madd_prefix_space\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from_slow\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mbos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbos_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0madd_bos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_bos_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0madd_eos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_eos_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0muse_default_system_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_default_system_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_prefix_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mlegacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_bos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_bos_token\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_eos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_eos_token\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_post_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_default_system_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_default_system_prompt\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mcan_save_slow_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_post_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n\u001b[0;34m        Updates the underlying post processor with the current `bos_token` and `eos_token`.\u001b[0m\n\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mbos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mbos_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bos_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"add_bos_token = True but bos_token = None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0meos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0meos_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0meos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_eos_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"add_eos_token = True but eos_token = None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0msingle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{(bos+':0 ') if self.add_bos_token else ''}$A:0{(' '+eos+':0') if self.add_eos_token else ''}\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{single}{(' '+bos+':1') if self.add_bos_token else ''} $B:1{(' '+eos+':1') if self.add_eos_token else ''}\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mspecial_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bos_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_eos_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemplateProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0msingle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_eos_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_eos_token\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_bos_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_bos_token\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0madd_eos_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_eos_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_eos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_post_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0madd_bos_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_bos_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_bos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_post_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_save_slow_tokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34m\"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m                \u001b[0;34m\"tokenizer.\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Vocabulary path ({save_directory}) should be a directory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mout_vocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename_prefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVOCAB_FILES_NAMES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_vocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_vocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;31m# TODO ArthurZ let's rely on the template processor instead, refactor all fast tokenizers\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;31m# Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.build_inputs_with_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuild_inputs_with_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0mbos_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bos_token\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0meos_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_eos_token\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbos_token_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_ids_0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtoken_ids_1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbos_token_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_ids_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mCall docstring:\u001b[0m\nMain method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\nsequences.\n\nArgs:\n    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n\n    add_special_tokens (`bool`, *optional*, defaults to `True`):\n        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n        automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n        automatically.\n    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n        Activates and controls padding. Accepts the following values:\n\n        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n          sequence if provided).\n        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n          acceptable input length for the model if that argument is not provided.\n        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n          lengths).\n    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n        Activates and controls truncation. Accepts the following values:\n\n        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n          to the maximum acceptable input length for the model if that argument is not provided. This will\n          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n          sequences (or a batch of pairs) is provided.\n        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n          maximum acceptable input length for the model if that argument is not provided. This will only\n          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n          maximum acceptable input length for the model if that argument is not provided. This will only\n          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n          greater than the model maximum admissible input size).\n    max_length (`int`, *optional*):\n        Controls the maximum length to use by one of the truncation/padding parameters.\n\n        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n    stride (`int`, *optional*, defaults to 0):\n        If set to a number along with `max_length`, the overflowing tokens returned when\n        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n        returned to provide some overlap between truncated and overflowing sequences. The value of this\n        argument defines the number of overlapping tokens.\n    is_split_into_words (`bool`, *optional*, defaults to `False`):\n        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n        which it will tokenize. This is useful for NER or token classification.\n    pad_to_multiple_of (`int`, *optional*):\n        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n        `>= 7.5` (Volta).\n    padding_side (`str`, *optional*):\n        The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n        Default value is picked from the class attribute of the same name.\n    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n        If set, will return tensors instead of list of python integers. Acceptable values are:\n\n        - `'tf'`: Return TensorFlow `tf.constant` objects.\n        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n        - `'np'`: Return Numpy `np.ndarray` objects.\n\n    return_token_type_ids (`bool`, *optional*):\n        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n        the specific tokenizer's default, defined by the `return_outputs` attribute.\n\n        [What are token type IDs?](../glossary#token-type-ids)\n    return_attention_mask (`bool`, *optional*):\n        Whether to return the attention mask. If left to the default, will return the attention mask according\n        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n\n        [What are attention masks?](../glossary#attention-mask)\n    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n        of returning overflowing tokens.\n    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n        Whether or not to return special tokens mask information.\n    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n        Whether or not to return `(char_start, char_end)` for each token.\n\n        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n        Python's tokenizer, this method will raise `NotImplementedError`.\n    return_length  (`bool`, *optional*, defaults to `False`):\n        Whether or not to return the lengths of the encoded inputs.\n    verbose (`bool`, *optional*, defaults to `True`):\n        Whether or not to print more information and warnings.\n    **kwargs: passed to the `self.tokenize()` method\n\nReturn:\n    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n\n    - **input_ids** -- List of token ids to be fed to a model.\n\n      [What are input IDs?](../glossary#input-ids)\n\n    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n      if *\"token_type_ids\"* is in `self.model_input_names`).\n\n      [What are token type IDs?](../glossary#token-type-ids)\n\n    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n\n      [What are attention masks?](../glossary#attention-mask)\n\n    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n      `return_overflowing_tokens=True`).\n    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n      `return_overflowing_tokens=True`).\n    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n    - **length** -- The length of the inputs (when `return_length=True`)\n"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"input_ids = teacher_tokenizer('captial of france'  , return_tensors =  'pt')['input_ids']\ndevice =  'cuda:0' if torch.cuda.is_available() else 'cpu'\ninput_ids = input_ids.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:59:04.978221Z","iopub.execute_input":"2025-06-09T13:59:04.978581Z","iopub.status.idle":"2025-06-09T13:59:04.984941Z","shell.execute_reply.started":"2025-06-09T13:59:04.978547Z","shell.execute_reply":"2025-06-09T13:59:04.984163Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"teacher_model = teacher_model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:59:06.507534Z","iopub.execute_input":"2025-06-09T13:59:06.507839Z","iopub.status.idle":"2025-06-09T13:59:06.516873Z","shell.execute_reply.started":"2025-06-09T13:59:06.507814Z","shell.execute_reply":"2025-06-09T13:59:06.516167Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"teacher_model = teacher_model.to(device)\nfor name, buffer in teacher_model.named_buffers():\n    if buffer.device != device:\n        setattr(teacher_model, name, buffer.to(device))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T06:55:15.361075Z","iopub.execute_input":"2025-06-10T06:55:15.361524Z","iopub.status.idle":"2025-06-10T06:55:15.484831Z","shell.execute_reply.started":"2025-06-10T06:55:15.361466Z","shell.execute_reply":"2025-06-10T06:55:15.483428Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-82-d227ade8aa93>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mteacher_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't move a model that has some modules offloaded to cpu or disk.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3162\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m                 )\n\u001b[0;32m-> 3164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 3558 has 14.69 GiB memory in use. Of the allocated memory 14.35 GiB is allocated by PyTorch, and 220.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 52.12 MiB is free. Process 3558 has 14.69 GiB memory in use. Of the allocated memory 14.35 GiB is allocated by PyTorch, and 220.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":82},{"cell_type":"code","source":"from accelerate import Accelerator\naccelerator = Accelerator()\nteacher_model, input_ids = accelerator.prepare(teacher_model, input_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:59:25.193230Z","iopub.execute_input":"2025-06-09T13:59:25.193531Z","iopub.status.idle":"2025-06-09T13:59:25.232104Z","shell.execute_reply.started":"2025-06-09T13:59:25.193507Z","shell.execute_reply":"2025-06-09T13:59:25.231448Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"print(\"Model device:\", next(teacher_model.parameters()).device)\nprint(\"Input IDs device:\", input_ids.device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:59:37.803227Z","iopub.execute_input":"2025-06-09T13:59:37.803567Z","iopub.status.idle":"2025-06-09T13:59:37.808951Z","shell.execute_reply.started":"2025-06-09T13:59:37.803539Z","shell.execute_reply":"2025-06-09T13:59:37.808144Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Model device: cuda:0\nInput IDs device: cuda:0\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"teacher_model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T06:54:54.752759Z","iopub.execute_input":"2025-06-10T06:54:54.753077Z","iopub.status.idle":"2025-06-10T06:54:54.757926Z","shell.execute_reply.started":"2025-06-10T06:54:54.753052Z","shell.execute_reply":"2025-06-10T06:54:54.757216Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":78},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\nwith torch.no_grad():\n    output , _  = teacher_model(sample[0].to(teacher_model.device) , return_attentions =  True )\n    # teacher_outputs = teacher_model(\n    #     input_ids=sample[0].to(teacher_model.device),\n    #     output_hidden_states=True,\n    #     output_attentions=False,  # Don't request attention maps\n    #     return_dict=True\n    # )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T06:15:11.536356Z","iopub.execute_input":"2025-06-11T06:15:11.536671Z","iopub.status.idle":"2025-06-11T06:15:12.634386Z","shell.execute_reply.started":"2025-06-11T06:15:11.536650Z","shell.execute_reply":"2025-06-11T06:15:12.633500Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"with torch.no_grad():\n    output = teacher_model(sample[0].to(teacher_model.device), output_attentions=True)\n    attentions = output.attentions  # attentions is a list: [layer0_attn, layer1_attn, ..., layerN_attn]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T06:16:06.777909Z","iopub.execute_input":"2025-06-11T06:16:06.778229Z","iopub.status.idle":"2025-06-11T06:16:08.032011Z","shell.execute_reply.started":"2025-06-11T06:16:06.778207Z","shell.execute_reply":"2025-06-11T06:16:08.031340Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"attentions[0].shape ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T06:16:34.166018Z","iopub.execute_input":"2025-06-11T06:16:34.166322Z","iopub.status.idle":"2025-06-11T06:16:34.171519Z","shell.execute_reply.started":"2025-06-11T06:16:34.166297Z","shell.execute_reply":"2025-06-11T06:16:34.170613Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"torch.Size([6, 32, 256, 256])"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"teacher_output.hidden_states ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text And Generation Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\n\n\n\ndef text_to_token_ids(text,  tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n    \ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n    return encoded  # Already 2D [1, seq_len]\n\n\n\ndef token_ids_to_text(tokens , tokenizer):\n    flat  = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decoded = tokenizer.decode(flat.tolist(), skip_special_tokens=True)\n    return decoded\n\n    \ndef generate_and_sample(model  , idx , context_size ,max_new_tokens ):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits  , _ , _ = model(idx_cond).logits\n        logits  = logits[:, -1  , :]\n        probs  = torch.softmax(logits  , dim=-1)\n        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx \n\n#\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]  # shape: [1, current_seq_len]\n\n        # Create causal mask dynamically\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)  # [1, seq_len, seq_len]\n\n        with torch.no_grad():\n            # logits , _ , _= model(idx_cond, mask=causal_mask) \n            logits = model(idx_cond , mask =causal_mask).logits\n\n        logits = logits[:, -1, :]  # only take the last token logits\n\n        # Apply top-k sampling if needed\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        # Temperature sampling\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n        \n\n    return idx\ndef real_time_generation(model, initial_input, context_size, temperature, top_k=None, device=\"cpu\"):\n    # Tokenize the initial input and prepare the model context\n    idx = torch.tensor(initial_input).unsqueeze(0).to(device)  # Assuming initial_input is tokenized\n    \n    print(\"Starting real-time generation...\")\n    \n    # Start generating tokens in real-time\n    for new_token in generate(model, idx, max_new_tokens=50, context_size=context_size, temperature=temperature, top_k=top_k, device=device):\n        print(f\"Generated token: {new_token.item()}\")  # Or decode it back to a word\n        \n        # You can check for user input here and update idx with the new input\n        # For instance, wait for the user to input a prompt to append to the context\n        user_input = input(\"Enter new input (or press enter to continue generation): \")\n        \n        if user_input:\n            # Tokenize the new user input and append it to the context\n            user_input_tokens = torch.tensor(tokenize(user_input)).unsqueeze(0).to(device)\n            idx = torch.cat((idx, user_input_tokens), dim=1)  # Append the new tokens to the context\n        else:\n            # Continue generating if no new user input\n            continue\n\n# Function to tokenize input (adjust depending on your tokenizer)\ndef tokenize(text):\n    # Assuming you have a tokenizer function available\n    return [ord(c) for c in text]  # Dummy example: ord() converts char to token id\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:07.716250Z","iopub.execute_input":"2025-06-12T16:40:07.716566Z","iopub.status.idle":"2025-06-12T16:40:07.733084Z","shell.execute_reply.started":"2025-06-12T16:40:07.716539Z","shell.execute_reply":"2025-06-12T16:40:07.732197Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"markdown","source":"Memery losses\n","metadata":{}},{"cell_type":"code","source":"\n\n# Memory Reconstruction Loss\n# Ensures stored information preserves input patterns\ndef reconstruction_loss(inputs, memory_output):\n    return F.mse_loss(memory_output, inputs)\n\n\n\n# Task-Specific Loss\n# Drives memory to store task-relevant information\ndef task_loss(predictions, targets):\n    return F.cross_entropy(predictions, targets)  # For classification\n\n# Memory Sparsity Loss\n# Encourages efficient slot usage\ndef sparsity_loss(concept_energy):\n    return torch.mean(concept_energy**2)  # L2 penalty on energy levels\n\n# Memory Diversity Loss\n# Prevents slot redundancy\ndef diversity_loss(key_memory):\n    normalized_keys = F.normalize(key_memory, dim=1)\n    similarity = torch.mm(normalized_keys, normalized_keys.T)\n    return torch.mean(similarity**2) - 1/torch.numel(similarity)\n\n\n\n# Energy Maintenance Loss\n# Maintains healthy energy distribution\ndef energy_loss(concept_energy):\n    energy_mean = torch.mean(concept_energy)\n    return F.mse_loss(energy_mean, torch.tensor(0.5,device = concept_energy.device))\n\n\n\n# Pruning Incentive Loss\n# Encourages proper slot turnover\ndef pruning_loss(age, usage):\n    old_unused = (age > 100) & (usage < 0.01)\n    return torch.mean(old_unused.float())\n\n\n\n# Anti-Collapse Loss\n# Prevents memory dependency on few slots\ndef anti_collapse_loss(usage_counts):\n    return -torch.sum(usage_counts * torch.log(usage_counts + 1e-7))\n\n\n\ndef novelty_loss(new_slots, existing_memory):\n    sim = F.cosine_similarity(new_slots.unsqueeze(1), \n                            existing_memory.unsqueeze(0), dim=-1)\n    return torch.mean(sim)\n\n\n\n\ndef total_loss(inputs, outputs, targets, memory):\n    # Base losses\n    rec_loss = reconstruction_loss(inputs, outputs)\n    # t_loss = task_loss(outputs, targets)\n    \n    # Memory regularization\n    sp_loss = sparsity_loss(memory.concept_energy)\n    div_loss = diversity_loss(memory.key_memory)\n    en_loss = energy_loss(memory.concept_energy)\n    \n    # Stability terms\n    prun_loss = pruning_loss(memory.age, memory.usage)\n    anti_loss = anti_collapse_loss(F.softmax(memory.access_count, dim=0))\n    \n    # Weighted combination\n    return (1.0 * rec_loss + \n            # 0.5 * t_loss + \n            0.3 * sp_loss + \n            0.2 * div_loss +\n            0.1 * en_loss +\n            0.05 * prun_loss +\n            0.02 * anti_loss)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:37:28.963020Z","iopub.execute_input":"2025-06-12T14:37:28.963330Z","iopub.status.idle":"2025-06-12T14:37:28.971404Z","shell.execute_reply.started":"2025-06-12T14:37:28.963304Z","shell.execute_reply":"2025-06-12T14:37:28.970296Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Losss","metadata":{}},{"cell_type":"code","source":"\n# Old Loss Loaders \n\n\ndef cal_loss_batch(input_batch , target_batch , model:torch.nn.Module , device:torch.device ):\n    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n    util_loss = -torch.log(model.memory.utilization + 1e-8)\n    \n    return loss + 0.1 * util_loss\ndef cal_loss_batch(input_batch , target_batch ,model:nn.Module, device:torch.device , mem_cof:float= 0.1):\n    input_batch  , target_batch =  input_batch.to(device) , target_batch.to(device)\n    logits ,x_emb, mem_output  =  model(input_batch)\n    B,T,V = logits.shape \n    gpt_loss = F.cross_entropy(\n            logits.view(B * T, V),\n            target_batch.view(B * T),\n            ignore_index=-100,                       # if you pad with -100\n        )   \n    # utilization_loss = -torch.log(model.memory.utilization + 1e-8)\n    memory_loss =  total_loss(inputs=x_emb , memory= model.memory , outputs=mem_output , targets= target_batch.float())\n    return gpt_loss +mem_cof * memory_loss \n\n\ndef calc_loss_loader(data_loader , model , device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:37:40.607931Z","iopub.execute_input":"2025-06-09T13:37:40.608244Z","iopub.status.idle":"2025-06-09T13:37:40.615097Z","shell.execute_reply.started":"2025-06-09T13:37:40.608217Z","shell.execute_reply":"2025-06-09T13:37:40.614252Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Train Script ","metadata":{}},{"cell_type":"code","source":"\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef evaluate_model(model, train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,   # Increase generation length if needed\n            context_size=256,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n\n        # Trim everything before the generation\n        generated_only = decoded_text[len(start_context):].strip()\n\n        # Stop at endoftext token if present\n        end_marker = \"<|endoftext|>\"\n        if end_marker in generated_only:\n            generated_only = generated_only.split(end_marker)[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\ndef save_model_checkpoint(model, optimizer, epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch  , \n        'memory_model_state_dict': model.memory.state_dict()   ,\n        \n    }\n    torch.save(checkpoint, path.format(epoch))\ndef after_save_load():\n    checkpoint = torch.load(\"checkpoint_epoch_7.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n\n\ndef train_model(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n    # scheduler = get_cosine_schedule_with_warmup(optimizer,\n    #                                         num_warmup_steps=500,\n    #                                         num_training_steps=total_steps)\n    for p in model.parameters():p.requires_grad= True \n    # for p in model.memory.compression.parameters():p.requires_grad =  False \n    # for p in model.memory.W_cell.parameters():p.requires_grad = False \n    ckpt = torch.load(\"memory_encoder.pth\")\n    model.memory.compression.load_state_dict(ckpt[\"compression\"])\n    model.memory.W_cell.load_state_dict(ckpt[\"W_cell\"])\n    print(\"Encoder weights are loaded \")\n    with torch.no_grad():\n        all_concepts =  []\n        \n    \n        for  input_batch , target_batch in train_dataloader:\n            input_batch = input_batch.to(device)\n            x = model.embedding(input_batch)\n            x =  model.memory.compression(x.mean(dim=1))\n            x = model.memory.W_cell(x)\n            all_concepts.append(x)\n\n    concept_pool=  torch.cat(all_concepts , dim=0)\n    k = min(model.memory.memory_size , concept_pool.shape[0])\n    kmeans = KMeans(n_clusters = k , random_state = 42)\n    kmeans.fit(concept_pool.cpu().numpy())\n    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n    print(centroids.shape , model.memory.key_memory.shape)\n    centroids = F.normalize(centroids, dim=-1)\n\n    n_centroids = centroids.size(0)  # 114\n    \n    with torch.no_grad():\n        model.memory.key_memory.data[:n_centroids] = centroids\n        model.memory.value_memory.data[:n_centroids] = centroids\n        model.memory.cell_state.data[:n_centroids] = centroids\n\n    print(centroids.shape , model.memory.key_memory.shape)\n\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n        save_model_checkpoint(model , optimizer , epoch+1)\n        \n\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:37:41.193576Z","iopub.execute_input":"2025-06-09T13:37:41.193853Z","iopub.status.idle":"2025-06-09T13:37:41.526318Z","shell.execute_reply.started":"2025-06-09T13:37:41.193828Z","shell.execute_reply":"2025-06-09T13:37:41.525452Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# KNOWLEDGE DISTILLATION ","metadata":{}},{"cell_type":"markdown","source":"        1.teacher model   and align the studen model hidden state and structure of the student and teacher model \n        2.align the dataset samples with the teacher dataset tokenizer and encoding and context \n        \n        3.define loss function \n            3.1 - kd loss  \n            3.2 - attention map loss \n            3.3 - feature loss \n        4.training method \n        5.generation method \n        ","metadata":{}},{"cell_type":"markdown","source":"# Train Scripts (Knowledge Distillation)","metadata":{}},{"cell_type":"markdown","source":"Loss Funcition","metadata":{}},{"cell_type":"code","source":"# Pretraining Loss function \n\n# Laywe mapping using the non_linear layer to match the teacher student hidden dim \n\n\ndef get_teacher_layer(student_layer_idx, total_student_layers, total_teacher_layers ,rat:float =  0.7):\n    ratio = (student_layer_idx / total_student_layers) ** rat\n    return min(int(ratio * total_teacher_layers), total_teacher_layers - 1)\n# mapping =  {i:get_teacher_layer(i , student_layer_count , teacher_layer_count) for i in range(student_layer_count)}\n\n# KD loss function for hidden state \nclass LayerKDLoss(nn.Module):\n    def __init__(self,\n                 student_hidden_dim: int,\n                 teacher_hidden_dim: int,\n                 mapping_dict: dict,\n                 teacher_layer_count: int = 30,\n                 student_layer_count: int = 12):\n        super().__init__()\n        self.mapping_dict = mapping_dict\n        self.student_n_layers = student_layer_count\n        self.teacher_n_layers = teacher_layer_count \n\n        self.adaptors = nn.ModuleDict({\n            f\"layer_{s}\": nn.Sequential(\n                nn.Linear(teacher_hidden_dim, student_hidden_dim),\n                nn.GELU(),\n                nn.LayerNorm(student_hidden_dim)  \n            )\n            for s in range(student_layer_count)\n        })\n\n    def forward(self,teacher_hidden_outputs,  student_hidden_outputs ):\n\n\n        loss = 0.0\n        for s_idx, t_idx in self.mapping_dict.items():\n            t_h = teacher_hidden_outpust[t_idx].detach()   \n            s_h = student_hidden_outputs[s_idx]          \n            adapted = self.adaptors[f\"layer_{s_idx}\"](t_h)  \n            \n            loss += F.mse_loss(s_h, adapted, reduction=\"mean\")\n\n        return loss / len(self.mapping_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:04.306367Z","iopub.execute_input":"2025-06-12T16:40:04.306673Z","iopub.status.idle":"2025-06-12T16:40:04.313066Z","shell.execute_reply.started":"2025-06-12T16:40:04.306649Z","shell.execute_reply":"2025-06-12T16:40:04.312261Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nclass EfficientAttentionKDMapper:\n    def __init__(self, teacher_model, student_model, tokenizer, device='cuda'):\n        self.teacher_model = teacher_model\n        self.student_model = student_model.to(device).eval()\n        self.tokenizer = tokenizer\n        self.device = device\n    \n    def compute_mapping(self, dataloader, max_batches=100):\n        student_layers = 12\n        teacher_layers = self.teacher_model.config.num_hidden_layers\n    \n        sim_matrix = torch.zeros(student_layers, teacher_layers)\n    \n        pbar = tqdm(dataloader, total=min(len(dataloader), max_batches), desc=\"Building attention mapping\")\n    \n        for i, batch in enumerate(pbar):\n            if i >= max_batches:\n                break\n    \n            input_ids = batch[0]\n    \n            with torch.no_grad():\n                s_out = self.student_model(input_ids.to(device), output_attentions=True)\n                t_out = self.teacher_model(input_ids.to(teacher_model.device), output_attentions=True)\n                \n                s_out  =  s_out['attentions']\n            for s_idx in range(student_layers):\n                s_attn = s_out[s_idx]  # [B, Hs, N, N]\n                s_mean = s_attn.mean(dim=1).reshape(s_attn.size(0), -1)  # [B, N*N]\n    \n                for t_idx in range(teacher_layers):\n                    t_attn = t_out.attentions[t_idx]\n                    t_mean = t_attn.mean(dim=1).reshape(t_attn.size(0), -1)\n    \n                    sim = F.cosine_similarity(s_mean, t_mean, dim=-1).mean()\n                    sim_matrix[s_idx, t_idx] += sim.item()\n    \n        sim_matrix /= max_batches\n    \n        mapping_dict = {\n            s_idx: torch.argmax(sim_matrix[s_idx]).item()\n            for s_idx in range(student_layers)\n        }\n    \n        return mapping_dict\n    \n    \n\nmapper = EfficientAttentionKDMapper(\n    teacher_model=teacher_model,\n    student_model=model,\n    tokenizer=teacher_tokenizer,\n    device='cuda'\n)\n\nattn_mapping_dict = mapper.compute_mapping(lm_dl, max_batches=50)\nprint(attn_mapping_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:22:59.343224Z","iopub.execute_input":"2025-06-12T16:22:59.343546Z","iopub.status.idle":"2025-06-12T16:24:13.508721Z","shell.execute_reply.started":"2025-06-12T16:22:59.343519Z","shell.execute_reply":"2025-06-12T16:24:13.507672Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Building attention mapping:   0%|          | 0/50 [00:00<?, ?it/s]LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\nBuilding attention mapping: 100%|██████████| 50/50 [01:14<00:00,  1.48s/it]","output_type":"stream"},{"name":"stdout","text":"{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"class AttentionKDLoss(nn.Module):\n    def __init__(self,mapping_dict:dict ,  student_num_layers:int =  12 , teacher_num_layers:int =  30  ):\n        super().__init__()\n        self.mapping_dict =  mapping_dict\n        self.student_n_layers = student_num_layers\n        self.teacher_n_layers =  teacher_num_layers \n\n    def forward(self, teacher_attentions , student_attentions):\n        total_loss =  0.0\n        count =  0\n        for s_layer in range(self.student_n_layers):\n            t_layer =  self.mapping_dict[s_layer]\n            student_attn  =  student_attentions[s_layer]\n            teacher_attn  = teacher_attentions[t_layer]\n\n            student_attn_mean =  student_attn.mean(dim=1)\n            teacher_attn_mean  = teacher_attn.mean(dim=1)\n\n            student_attn_mean = F.softmax(student_attn_mean, dim =-1)\n            teacher_attn_mean = F.softmax(teacher_attn_mean, dim =-1)\n\n            layer_loss = F.kl_div(\n                student_attn_mean.log(), teacher_attn_mean, reduction='batchmean'\n            )  \n            total_loss += layer_loss \n            count +=1 \n\n        return total_loss / count \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:12.095332Z","iopub.execute_input":"2025-06-12T16:40:12.095692Z","iopub.status.idle":"2025-06-12T16:40:12.101353Z","shell.execute_reply.started":"2025-06-12T16:40:12.095651Z","shell.execute_reply":"2025-06-12T16:40:12.100591Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LogitsKDLoss(nn.Module):\n    def __init__(self, temp=0.5):\n        super().__init__()\n        self.temp = temp\n\n    def forward(self, teacher_logits, student_logits, target_batch):\n        B, T, V = student_logits.shape\n\n        assert student_logits.shape == teacher_logits.shape, \\\n            f\"Shape mismatch: student={student_logits.shape}, teacher={teacher_logits.shape}\"\n\n        student_soft = F.log_softmax(student_logits.float() / self.temp, dim=-1)\n        teacher_soft = F.softmax(teacher_logits.float() / self.temp, dim=-1).detach()\n\n        teacher_soft = torch.clamp(teacher_soft, min=1e-9)\n        distillation_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (self.temp ** 2)\n        # distillation_loss = torch.clamp(distillation_loss, max=50.0)\n        ce_loss = F.cross_entropy(\n            student_logits.view(B * T, V),\n            target_batch.view(B * T),\n            ignore_index=-100,\n        )\n\n        return distillation_loss, ce_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:12.568322Z","iopub.execute_input":"2025-06-12T16:40:12.568601Z","iopub.status.idle":"2025-06-12T16:40:12.574295Z","shell.execute_reply.started":"2025-06-12T16:40:12.568577Z","shell.execute_reply":"2025-06-12T16:40:12.573391Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"class AnnealingScheduler:\n    def __init__(self, tau_max):\n        # self.T_max = T_max\n        self.tau_max = tau_max\n\n    def get_phi(self, epoch):\n        T = max(1, self.tau_max - int(epoch))\n        phi = 1 - ((T - 1) / self.tau_max)\n        return phi\nclass AnnealingKDLoss(nn.Module):\n    def __init__(self, annealing_scheduler, temp=6.0):\n        super().__init__()\n        self.annealing_scheduler = annealing_scheduler\n        self.temp = temp\n\n    def forward(self, teacher_logits, student_logits, epoch):\n        phi = self.annealing_scheduler.get_phi(epoch)\n\n        teacher_scaled = teacher_logits * phi\n        student_scaled = student_logits * phi\n\n        return F.mse_loss(student_scaled, teacher_scaled)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:12.999859Z","iopub.execute_input":"2025-06-12T16:40:13.000140Z","iopub.status.idle":"2025-06-12T16:40:13.005748Z","shell.execute_reply.started":"2025-06-12T16:40:13.000117Z","shell.execute_reply":"2025-06-12T16:40:13.004759Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class cal_loss_batch(nn.Module):\n    def __init__(self, teacher_model: nn.Module,\n                 student_model: nn.Module, \n                 hidden_mapping_dict: dict, \n                 attn_mapping_dict: dict,\n                 student_hidden_dim: int, \n                 teacher_hidden_dim: int, \n                 annealing_scheduler,\n                 student_num_layers: int = 12, \n                 teacher_num_layers: int = 30, \n                 device: torch.device = torch.device('cuda'), \n                 \n                 mem_cof: float = 0.2, \n                 temp: float = 6.0, \n                 alpha: float = 0.8, \n                 lambda_hidden: float = 1.0,\n                 lambda_attention: float = 1.0,\n                 lambda_logits: float = 1.0,\n                 lambda_ce: float = 1.0):\n        super().__init__()\n\n        self.teacher_model = teacher_model\n        self.student_model = student_model\n        self.device = device\n        self.mem_cof = mem_cof\n        self.temp = temp\n        self.alpha = alpha\n        self.lambda_hidden = lambda_hidden\n        self.lambda_attention = lambda_attention\n        self.lambda_logits = lambda_logits\n        self.lambda_ce = lambda_ce\n\n        self.logit_loss = LogitsKDLoss(temp=temp)\n        self.attn_loss = AttentionKDLoss(mapping_dict=attn_mapping_dict,\n                                         student_num_layers=student_num_layers,\n                                         teacher_num_layers=teacher_num_layers)\n        self.hidden_loss = LayerKDLoss(student_hidden_dim=student_hidden_dim,\n                                       teacher_hidden_dim=teacher_hidden_dim,\n                                       mapping_dict=hidden_mapping_dict)\n        self.annealing_kd_loss = AnnealingKDLoss(annealing_scheduler)\n\n    def forward(self, input_batch, target_batch , epoch):\n        # input_batch = input_batch.to(self.device)\n\n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(input_batch.to(teacher_model.device), output_hidden_states=True, output_attentions=True)\n            teacher_logits = teacher_outputs.logits\n            teacher_attentions = teacher_outputs.attentions\n            teacher_hidden = teacher_outputs.hidden_states\n            \n        student_outputs = self.student_model(input_batch.to(self.device), output_hidden_state=True, output_attentions=True)\n        student_logits = student_outputs['logits']\n        student_attentions = student_outputs['attentions']\n        student_hidden = student_outputs['hidden_states']\n\n        kd_logits_loss, ce_loss = self.logit_loss(teacher_logits, student_logits, target_batch)\n        kd_attn_loss = self.attn_loss(teacher_attentions, student_attentions)\n        kd_hidden_loss = self.hidden_loss(student_hidden, teacher_hidden)\n        anneal_loss = self.annealing_kd_loss(teacher_logits, student_logits, epoch)\n        total_loss = (\n            self.lambda_hidden * kd_hidden_loss +\n            self.lambda_attention * kd_attn_loss +\n            self.lambda_logits * kd_logits_loss +\n            self.lambda_ce * ce_loss\n        )\n        total_loss +=  anneal_loss \n\n        del teacher_outputs, teacher_logits, teacher_attentions, teacher_hidden\n        del student_outputs, student_attentions, student_hidden, student_logits\n        torch.cuda.empty_cache()\n\n        return total_loss , anneal_loss , kd_logits_loss , kd_attn_loss , kd_hidden_loss , ce_loss \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:13.871323Z","iopub.execute_input":"2025-06-12T16:40:13.871641Z","iopub.status.idle":"2025-06-12T16:40:13.880292Z","shell.execute_reply.started":"2025-06-12T16:40:13.871612Z","shell.execute_reply":"2025-06-12T16:40:13.879276Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n\nclass EMA:\n    def __init__(self, model, decay=0.999):\n        self.model = model\n        self.shadow = {}\n        self.decay = decay\n\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.clone().detach()\n\n    def update(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = self.decay * self.shadow[name] + (1.0 - self.decay) * param.detach()\n\n    def apply_shadow(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data.copy_(self.shadow[name])\n\n    def restore(self, backup):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                param.data.copy_(backup[name])\n\n    def backup_model(self):\n        return {name: param.clone() for name, param in self.model.named_parameters()}\n    def to(self, device):\n        self.model.to(device)\n\n    def eval(self):\n        self.model.eval()\n\n    def state_dict(self):\n        return self.model.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.model.load_state_dict(state_dict)\n\n\ndef get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n    \n    return LambdaLR(optimizer, lr_lambda)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:17.048640Z","iopub.execute_input":"2025-06-12T16:40:17.049118Z","iopub.status.idle":"2025-06-12T16:40:17.061288Z","shell.execute_reply.started":"2025-06-12T16:40:17.049082Z","shell.execute_reply":"2025-06-12T16:40:17.060112Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import math\nimport gc\nfrom copy import deepcopy\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef calc_loss_loader(dataloader, student_model, teacher_model, device, num_batches, loss_fn , epoch):\n    total_loss = total_ce = total_distill = total_hidden = total_attn = total_anneal_loss =  0.0\n    for i, (inputs, targets) in enumerate(dataloader):\n        if i >= num_batches:\n            break\n        loss,anneal_loss ,  ce_loss, distill_loss, attn_loss, hidden_loss = loss_fn(inputs.to(device), targets.to(device) , epoch)\n        total_loss += loss.item()\n        total_ce += ce_loss.item()\n        total_anneal_loss += anneal_loss.item()\n        total_distill += distill_loss.item()\n        total_hidden += hidden_loss.item()\n        total_attn += attn_loss.item()\n\n    return (total_loss/num_batches,total_anneal_loss /num_batches , total_ce/num_batches, total_distill/num_batches, total_hidden/num_batches, total_attn/num_batches)\n\ndef evaluate_model(student_model, teacher_model, train_dataloader, eval_dataloader, device, eval_iter, loss_fn, epoch , ema):\n    backup = ema.backup_model()  \n    ema.apply_shadow()\n    student_model.eval()\n    with torch.no_grad():\n         train_loss, anneal_loss , ce_loss, distill_loss, hidden_loss, attn_loss = calc_loss_loader(train_dataloader, student_model, teacher_model, device, eval_iter, loss_fn , epoch )\n         val_loss,val_anneal_loss ,  val_ce_loss, val_distill_loss, val_hidden_loss, val_attn_loss = calc_loss_loader(eval_dataloader, student_model, teacher_model, device, eval_iter, loss_fn , epoch)\n    ema.restore(backup)\n    student_model.train()\n    return train_loss, val_loss,anneal_loss , val_anneal_loss , ce_loss, distill_loss, hidden_loss, attn_loss, val_ce_loss, val_distill_loss, val_hidden_loss, val_attn_loss\n        \ndef save_model_checkpoint(model, optimizer,ema ,  epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch  , \n        'ema_state_dict': ema.shadow,\n        'memory_model_state_dict': model.memory.state_dict()   ,\n        \n    }\n    torch.save(checkpoint, path.format(epoch))\ndef load_checkpoint(model, ema, optimizer, filepath):\n    checkpoint = torch.load(filepath)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Load EMA parameters if available\n    if 'ema_state_dict' in checkpoint:\n        for name, param in model.named_parameters():\n            if param.requires_grad and name in checkpoint['ema_state_dict']:\n                ema.shadow[name] = checkpoint['ema_state_dict'][name].clone()\n    \n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\ndef train_KD_advanced(\n        teacher_model,\n        student_model,\n        train_dataloader,\n        val_dataloader,\n        optimizer,\n        annealing_scheduler,\n        hidden_mapping_dict,\n        attn_mapping_dict,\n        student_hidden_dim,\n        teacher_hidden_dim,\n        student_num_layers,\n        teacher_num_layers,\n        device,\n        start_context:str ,\n        eval_freq = 5,\n        total_epochs=10,\n        grad_accum_steps=1,\n        ema_decay=0.999,\n        warmup_ratio=0.05\n    ):\n\n    loss_fn = cal_loss_batch(\n        teacher_model=teacher_model,\n        student_model=student_model,\n        hidden_mapping_dict=hidden_mapping_dict,\n        attn_mapping_dict=attn_mapping_dict,\n        student_hidden_dim=student_hidden_dim,\n        teacher_hidden_dim=teacher_hidden_dim,\n        student_num_layers=student_num_layers,\n        teacher_num_layers=teacher_num_layers,\n        annealing_scheduler = annealing_scheduler,\n        device=device\n    )\n\n    # annealing_kd_loss = AnnealingKDLoss(annealing_scheduler)\n\n    scaler = GradScaler()\n    ema = EMA(student_model, ema_decay)\n\n    total_steps = total_epochs * len(train_dataloader)\n    warmup_steps = int(total_steps * warmup_ratio)\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\n    global_step = 0\n    student_model.train()\n    teacher_model.eval()\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    for epoch in range(total_epochs):\n        loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n\n        for step, (input_batch, target_batch) in enumerate(loop):\n            # input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n\n            with autocast():\n                total_loss,anneal_loss ,  kd_logits_loss, kd_attn_loss, kd_hidden_loss, ce_loss = loss_fn(input_batch, target_batch , epoch)\n\n            scaler.scale(total_loss).backward()\n\n            if (step + 1) % grad_accum_steps == 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n                ema.update()\n            # token_seen += input_batch.numel()\n            token_seen += input_batch.size(0) * input_batch.size(1)\n\n            global_step += 1\n            if  global_step % eval_freq  == 0:\n                train_loss, val_loss,train_anneal_loss , val_anneal_loss  , ce, ds, hidden, attn, val_ce, val_ds, val_hidden, val_attn=  evaluate_model(student_model, teacher_model, train_dataloader, val_dataloader, device, eval_freq, loss_fn, epoch)\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                \n                \n    \n                loop.set_postfix({\n                    \"total_loss\": total_loss.item(),\n                    \"anneal_loss\": f\"{anneal_loss:.4f}\",\n                    \"ce\": f\"{ce_loss.item():.4f}\",\n                    \"attn\": kd_attn_loss.item(),\n                    \"hidden\": kd_hidden_loss.item() , \n                    'train_loss': f\"{train_loss:.4f}\",\n                    'val_loss': f\"{val_loss:.4f}\",\n                     'token_seen':token_seen , \n                    \n                })\n                \n        generate_and_print_sample(student_model, train_dataloader.dataset.tokenizer, device, start_context)\n        save_model_checkpoint(student_model, optimizer, epoch + 1,ema,  path= \"student_checkpoint_epoch_{}.pth\" )\n        gc.collect()\n        torch.cuda.empty_cache()\n\n\n\n       \n\n    print(\"Training Complete.\")\n    return_dict = {\n        'model':student_model , \n        'train_loss':train_losses , \n        'val_loss':val_losses ,\n        'track_tokens_seen':track_tokens_seen , \n    }\n    return return_dict  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:18.169373Z","iopub.execute_input":"2025-06-12T16:40:18.169692Z","iopub.status.idle":"2025-06-12T16:40:18.184420Z","shell.execute_reply.started":"2025-06-12T16:40:18.169666Z","shell.execute_reply":"2025-06-12T16:40:18.183714Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef cal_loss_batch(input_batch, target_batch,\n                   teacher_model: nn.Module,\n                   student_model: nn.Module,\n                   device: torch.device,\n                   mem_cof: float = 0.2,\n                   distil_temp: float = 5.0,\n                   alpha: float = 0.9):\n\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    with torch.no_grad():\n        teacher_output = teacher_model(input_batch)\n        teacher_logits = teacher_output.logits  # [B, T, V]\n\n    student_output =  student_model(input_batch)\n    student_logits, x_emb, mem_output =   student_output['logits'] , studen_output['x_emb'] , student_output['memory_output']\n    B, T, V = student_logits.shape\n  # \"logits\": logits,\n  #           'x_emb':x_emb ,\n  #           'memory_output':mem_out, \n  #           \"hidden_states\": hidden_states,\n  #           \"attentions\": all_attentions,\n  #           \"retrieved_memory\": retrieved,\n  #           'attention_weights':all_attentions\n    assert student_logits.shape == teacher_logits.shape, \\\n        f\"Shape mismatch: student={student_logits.shape}, teacher={teacher_logits.shape}\"\n\n    # Knowledge distillation loss\n    student_soft = F.log_softmax(student_logits.float() / distil_temp, dim=-1)\n    teacher_soft = F.softmax(teacher_logits.float() / distil_temp, dim=-1).detach()\n    teacher_soft = torch.clamp(teacher_soft, min=1e-9)\n\n    distillation_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (distil_temp ** 2)\n    distillation_loss = torch.clamp(distillation_loss, max=50.0)\n\n    # Cross-entropy loss\n    targets = target_batch if appropriate else target_batch.float()\n\n    ce_loss = F.cross_entropy(\n        student_logits.view(B * T, V),\n        target_batch.view(B * T),\n        ignore_index=-100,\n    )\n\n    # Memory loss\n    memory_loss = total_loss(inputs=x_emb, memory=student_model.memory,\n                             outputs=mem_output, targets=target_batch.float())\n\n    total = alpha * ce_loss + (1 - alpha) * distillation_loss + mem_cof * memory_loss\n\n\n\n    return total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:30:11.357817Z","iopub.execute_input":"2025-06-09T04:30:11.358154Z","iopub.status.idle":"2025-06-09T04:30:11.366108Z","shell.execute_reply.started":"2025-06-09T04:30:11.358127Z","shell.execute_reply":"2025-06-09T04:30:11.365131Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch import nn\n\ndef cal_loss_batch(input_batch, target_batch,\n                   teacher_model: nn.Module,\n                   student_model: nn.Module,\n                   device: torch.device,\n                   mem_cof: float = 0.2,\n                   distil_temp: float = 6.0,\n                   alpha: float = 1.0):\n\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    # 1. Teacher output\n    with torch.no_grad():\n        teacher_output = teacher_model(input_batch)\n        teacher_logits = teacher_output.logits  # [B, T, V]\n\n    # 2. Student output\n    # student_logits, x_emb, mem_output = student_model(input_batch)  # [B, T, V]\n    student_output =  student_model(input_batch)\n    student_logits, x_emb, mem_output =   student_output['logits'] , student_output['x_emb'] , student_output['memory_output']\n\n    B, T, V = student_logits.shape\n\n    assert student_logits.shape == teacher_logits.shape, \\\n        f\"Shape mismatch: student={student_logits.shape}, teacher={teacher_logits.shape}\"\n\n    # 3. Knowledge Distillation Loss\n    student_soft = F.log_softmax(student_logits.float() / distil_temp, dim=-1)\n    teacher_soft = F.softmax(teacher_logits.float() / distil_temp, dim=-1).detach()\n    teacher_soft = torch.clamp(teacher_soft, min=1e-9)\n\n    distillation_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (distil_temp ** 2)\n    distillation_loss = torch.clamp(distillation_loss, max=50.0)\n\n    ce_loss = F.cross_entropy(\n        student_logits.view(B * T, V),\n        target_batch.view(B * T),\n        ignore_index=-100,\n    )\n\n    memory_loss = total_loss(\n        inputs=x_emb,\n        memory=student_model.memory,\n        outputs=mem_output,\n        targets=target_batch.float()  \n    )\n\n    # total = alpha * ce_loss + (1 - alpha) * distillation_loss + mem_cof * memory_loss\n    total = alpha * ce_loss + (1 - alpha) * distillation_loss\n    del teacher_logits\n    del student_logits\n    torch.cuda.empty_cache()\n\n\n    return total, ce_loss.item(), distillation_loss.item(), memory_loss.item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:30:13.310250Z","iopub.execute_input":"2025-06-09T04:30:13.310709Z","iopub.status.idle":"2025-06-09T04:30:13.318431Z","shell.execute_reply.started":"2025-06-09T04:30:13.310676Z","shell.execute_reply":"2025-06-09T04:30:13.317347Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def evaluate_model(student_model, teacher_model,train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss , ce_loss , distillation_loss , memory_loss  = calc_loss_loader(train_dataloader, student_model,teacher_model ,  device, num_batches=eval_iter)\n        val_loss  , val_ce_loss , val_distillation_loss , val_memory_loss  = calc_loss_loader(eval_dataloader, student_model,teacher_model ,  device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss , ce_loss , distillation_loss, memory_loss ,val_ce_loss , val_distillation_loss , val_memory_loss\ndef calc_loss_loader(data_loader , student_model , teacher_model ,device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss , ce_loss , distillation_loss , memory_loss  =  cal_loss_batch(inputs , target , teacher_model , student_model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches , ce_loss , distillation_loss , memory_loss\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:30:15.248208Z","iopub.execute_input":"2025-06-09T04:30:15.248563Z","iopub.status.idle":"2025-06-09T04:30:15.255199Z","shell.execute_reply.started":"2025-06-09T04:30:15.248535Z","shell.execute_reply":"2025-06-09T04:30:15.254250Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":23},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n\n        with torch.no_grad():\n            outputs = model(idx_cond)['logits']\n            logits = outputs.logits\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1].unsqueeze(-1)\n            logits = torch.where(logits < min_val, torch.full_like(logits, float('-inf')), logits)\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    encoded = text_to_token_ids(start_context, tokenizer).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,\n            context_size=256,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n        generated_only = decoded_text[len(start_context):].strip()\n\n        if \"<|endoftext|>\" in generated_only:\n            generated_only = generated_only.split(\"<|endoftext|>\")[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn.functional as F\n\ndef train_model(\n    teacher_model: nn.Module,\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    scaler = GradScaler()\n    torch.autograd.set_detect_anomaly(True)\n\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # Ensure all parameters require grad\n    for p in model.parameters():\n        p.requires_grad = True\n\n    # Load pretrained encoder weights into the memory module\n    ckpt = torch.load('/kaggle/working/memory_encoder.pth')\n    model.memory.compression.load_state_dict(ckpt[\"compression\"])\n    model.memory.W_cell.load_state_dict(ckpt[\"W_cell\"])\n    print(\"Encoder weights are loaded \")\n\n    # Build initial memory centroids via KMeans\n    with torch.no_grad():\n        all_concepts = []\n        for input_batch, _ in train_dataloader:\n            input_batch = input_batch.to(device)\n            x = model.embedding(input_batch)\n            x = model.memory.compression(x.mean(dim=1))\n            x = model.memory.W_cell(x)\n            all_concepts.append(x)\n        concept_pool = torch.cat(all_concepts, dim=0)\n\n        k = min(model.memory.memory_size, concept_pool.shape[0])\n        from sklearn.cluster import KMeans\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(concept_pool.cpu().numpy())\n        centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n        centroids = F.normalize(centroids, dim=-1)\n        n_centroids = centroids.size(0)\n\n        model.memory.key_memory.data[:n_centroids] = centroids\n        model.memory.value_memory.data[:n_centroids] = centroids\n        model.memory.cell_state.data[:n_centroids] = centroids\n\n    # Training loop\n    for epoch in range(num_epochs):\n        loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        model.train()\n\n        for inputs_batch, target_batch in loop:\n            inputs_batch = inputs_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            optimizer.zero_grad()\n\n            # 1) Compute loss under autocast\n            with autocast():\n                loss , ce_loss , distillation_loss , memory_loss  = cal_loss_batch(\n                    input_batch=inputs_batch,\n                    target_batch=target_batch,\n                    device=device,\n                    student_model=model,\n                    teacher_model=teacher_model\n                )\n\n    \n            # 3) Backward + gradient clipping + optimizer step\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n\n\n\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            # 5) Periodic evaluation\n            if global_step % eval_freq == 0:\n                train_loss, val_loss ,train_ce_loss , train_distillation_loss, train_memory_loss ,val_ce_loss , val_distillation_loss , val_memory_loss = evaluate_model(\n                    model,teacher_model ,  train_dataloader, eval_dataloader, device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                loop.set_postfix({\n    'loss': f\"{loss.item():.4f}\",\n    'step': global_step,\n    'ce_loss': f\"{ce_loss:.4f}\",\n    'ds_loss': f\"{distillation_loss:.4f}\",\n    'memory_loss': f\"{memory_loss:.4f}\",\n    'train_loss': f\"{train_loss:.4f}\",\n    'val_loss': f\"{val_loss:.4f}\",\n    'train_ce': f\"{train_ce_loss:.4f}\",\n    'train_ds': f\"{train_distillation_loss:.4f}\",\n    'train_mem': f\"{train_memory_loss:.4f}\",\n    'val_ce': f\"{val_ce_loss:.4f}\",\n    'val_ds': f\"{val_distillation_loss:.4f}\",\n    'val_mem': f\"{val_memory_loss:.4f}\"\n})\n\n        # 6) Generate a sample and save checkpoint at end of epoch\n        generate_and_print_sample(model, train_dataloader.dataset.tokenizer, device, start_context)\n        save_model_checkpoint(model, optimizer, epoch + 1)\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:30:18.410912Z","iopub.execute_input":"2025-06-09T04:30:18.411199Z","iopub.status.idle":"2025-06-09T04:30:18.423571Z","shell.execute_reply.started":"2025-06-09T04:30:18.411177Z","shell.execute_reply":"2025-06-09T04:30:18.422730Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Knowledge Distillation Model","metadata":{}},{"cell_type":"markdown","source":"# Teacher Model ","metadata":{}},{"cell_type":"code","source":"# Pretrain model \nteacher_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-llm-7b-base\" , device_map=\"auto\" ,torch_dtype=torch.float16).half()\nteacher_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-base\")\nteacher_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:40:23.453429Z","iopub.execute_input":"2025-06-12T16:40:23.453737Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/584 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8dd38b6edf445db5573cdd6285126e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/22.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e8d0f486cc418892306ab0252dfb25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4172add5e3924638b07d4268883097e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f8003d9802496ebbcd154a67db90da"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fine tuning \n# teacher_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n# teacher_model = AutoModelForCausalLM.from_pretrained(teacher_name, device_map=\"auto\", torch_dtype=torch.float16)\n# teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-11T07:10:33.675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"teacher_model.config","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-11T07:10:33.676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss for Knowledge distillation\ndef distillation_loss(student_logits , teacher_logits  , labels , T = 2.0 , alpha = 0.7):\n    loss_kd =  F.kl_div(\n        F.log_softmax(stident_logits / T , dim=-1) , \n        F.softmax(teacher_logits / T , dim=-1),\n         reduction=\"batchmean\"\n    ) *(T**2 )\n\n    loss_ce = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n    return alpha * loss_kd +(1-alpha) * loss_ce ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-07T14:13:33.098Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"teacher_model.config.vocab_size ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T10:44:38.618919Z","iopub.execute_input":"2025-06-09T10:44:38.619471Z","iopub.status.idle":"2025-06-09T10:44:38.624422Z","shell.execute_reply.started":"2025-06-09T10:44:38.619445Z","shell.execute_reply":"2025-06-09T10:44:38.623604Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"102400"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T10:45:02.210207Z","iopub.execute_input":"2025-06-09T10:45:02.210494Z","iopub.status.idle":"2025-06-09T10:45:02.215414Z","shell.execute_reply.started":"2025-06-09T10:45:02.210470Z","shell.execute_reply":"2025-06-09T10:45:02.214557Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"102400"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"teacher_tokenizer.vocab_size ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T10:44:39.431860Z","iopub.execute_input":"2025-06-09T10:44:39.432130Z","iopub.status.idle":"2025-06-09T10:44:39.437093Z","shell.execute_reply.started":"2025-06-09T10:44:39.432111Z","shell.execute_reply":"2025-06-09T10:44:39.436271Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"100000"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"prompt = \"User: What is the capital of France?\\nAssistant:\"\ninput_ids = teacher_tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n# print(input_ids)\nteacher_model  = teacher_model.cuda()\n# output = teacher_model.generate(input_ids, max_length=100, num_return_sequences=1)\noutput =  teacher_model.generate(input_ids , output_hidden_states=True, output_attentions=True )\nwith torch.no_grad():\n    outputs = teacher_model(input_ids, output_hidden_states=True, output_attentions=True )\n\nlogits = outputs.logits  # shape: [1, seq_len, vocab_size]\npred_token_ids = torch.argmax(logits, dim=-1)  # shape: [1, seq_len]\n\ndecoded_tokens = teacher_tokenizer.batch_decode(pred_token_ids, skip_special_tokens=True)\nprint(decoded_tokens[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T11:31:12.185699Z","iopub.execute_input":"2025-06-09T11:31:12.186061Z","iopub.status.idle":"2025-06-09T11:31:14.293637Z","shell.execute_reply.started":"2025-06-09T11:31:12.186030Z","shell.execute_reply":"2025-06-09T11:31:14.292899Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n:\n is the difference of the?\n-: Paris\n","output_type":"stream"}],"execution_count":81},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"teacher_tokenizer.decode(output[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T11:31:16.492041Z","iopub.execute_input":"2025-06-09T11:31:16.492340Z","iopub.status.idle":"2025-06-09T11:31:16.497890Z","shell.execute_reply.started":"2025-06-09T11:31:16.492316Z","shell.execute_reply":"2025-06-09T11:31:16.497224Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"'<｜begin▁of▁sentence｜>User: What is the capital of France?\\nAssistant: Paris.\\nUser: What is the capital of Germany?\\nAssistant: Berlin.\\nUser:'"},"metadata":{}}],"execution_count":82},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# MemoryGPT Model training  \n","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPT Config ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nGPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,#126 \n# Context lengt\n\"emb_dim\": 768,\n# Embedding dimension\n\"n_heads\": 12,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False\n# Query-Key-Value bias\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:46:32.894841Z","iopub.execute_input":"2025-06-09T04:46:32.895122Z","iopub.status.idle":"2025-06-09T04:46:32.911238Z","shell.execute_reply.started":"2025-06-09T04:46:32.895100Z","shell.execute_reply":"2025-06-09T04:46:32.910409Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"GPT_CONFIG_124M_Memory = {\n\"vocab_size\": teacher_model.config.vocab_size ,#100000,# 50257\n    # Vocabulary size\n\"context_length\": 126,\n# Context length\n\"emb_dim\": 128,\n# Embedding dimension\n\"n_heads\": 4,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False,\n'memory_dim':128, \n'max_slots' :1000,\n'memory_heads':2 ,\n\n# Query-Key-Value bias\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:42:30.940324Z","iopub.execute_input":"2025-06-10T08:42:30.940662Z","iopub.status.idle":"2025-06-10T08:42:30.944942Z","shell.execute_reply.started":"2025-06-10T08:42:30.940638Z","shell.execute_reply":"2025-06-10T08:42:30.944054Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GPT_CONFIG_360M_Memory = {\n    # 1) Vocabulary size should exactly match the teacher’s tokenizer:\n    \"vocab_size\": teacher_model.config.vocab_size,   # e.g. ~102 400\n\n    # 2) Context length increased from 126→256 so you can fit longer instructions:\n    \"context_length\": 256,\n\n    # 3) Embedding dimension: bump from 128→512. \n    #    A 512-dim hidden size with 12 layers/8 heads is roughly a 360 M-parameter model—\n    #    which a single 16 GB GPU can train in mixed precision at batch sizes ≈4–8.\n    \"emb_dim\": 512,\n\n    # 4) Number of attention heads: raise from 4→8 so that head_dim (=512/8=64) stays a common multiple.\n    \"n_heads\": 8,\n\n    # 5) Number of transformer blocks: keep at 12. \n    #    (12×512×512 layers ≈ 360 M total parameters.)  \n    \"n_layers\": 12,\n\n    # 6) Dropout can remain at 0.1 (standard for GPT-style training):\n    \"drop_rate\": 0.1,\n\n    # 7) Keep QKV bias disabled (matches many open-source GPT variants):\n    \"qkv_bias\": False,\n\n    # — Memory-specific fields below —\n\n    # 8) Memory projection / compression dimension: raise from 128→512 so memory slots live in the same\n    #    512-dim space as the rest of the network. This makes retrieved vectors richer (but costs more).\n    \"memory_dim\": 512,\n\n    # 9) Maximum number of memory slots: 1000 is fine, but you could lower to 512 if VRAM is tight.\n    #    We’ll keep it at 1000 for maximal capacity, but be aware that 1000×512 float vectors take ~2 MB each.\n    \"max_slots\": 1000,\n\n    # 10) When attending to memory, use 4 “memory heads” instead of 2, to match the larger hidden size.\n    \"memory_heads\": 4,\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_encoder(model , train_dataloader , device = 'cuda'):\n    memory = model.memory \n    for p in model.parameters():p.requires_grad = False \n    for p in model.memory.compression.parameters(): p.requires_grad = True\n    for p in model.memory.W_cell.parameters():p.requires_grad =  True \n    optimizer = torch.optim.Adam(\n    list(memory.compression.parameters()) + list(memory.W_cell.parameters()),\n    lr=1e-3\n)\n    global_step= -1 \n    print('tatal step ' , len(train_dataloader) * 5)\n    for _ in tqdm(range(5)):\n        \n        for input_batch , _  in train_dataloader:\n            input_batch =  input_batch.to(device)\n            x1 = model.embedding(input_batch)\n            B , _ , _  =  x1.shape\n            x2 =  model.embedding(input_batch)\n\n            q1 = memory.W_cell(memory.compression(x1.mean(dim=1)))\n            q2 =  memory.W_cell(memory.compression(x2.mean(dim=1)))\n\n            q1, q2 = F.normalize(q1, dim=-1), F.normalize(q2, dim=-1)\n\n            sim = torch.matmul(q1,q2.T)\n            loss = F.cross_entropy(sim/0.1  , torch.arange(B, device = sim.device))\n\n            # print(loss)\n            optimizer.zero_grad();loss.backward();optimizer.step()\n            global_step +=1 \n             # print(\n             #        f\"Epoch: {_+1} (step {global_step:06d}):\",\n    x, _ = next(iter(train_dataloader))\n    x =  x.to(device)\n    x = model.embedding(x)\n\n    q =  memory.W_cell(memory.compression(x.mean(1)))\n    q1 =  memory.W_cell(memory.compression(x.mean(1)))\n    avg_sim = F.cosine_similarity(q, q1, dim=-1).mean().item()\n    # print(avg_sim)\n    assert avg_sim > 0.95 , 'Encoder still drift too much'\n\n    ckpt = {\n        \"compression\": memory.compression.state_dict(),\n        \"W_cell\":      memory.W_cell.state_dict()\n    }\n    torch.save(ckpt, \"memory_encoder.pth\")\n    print(\"saved memory_encoder.pth\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:42:33.782856Z","iopub.execute_input":"2025-06-10T08:42:33.783184Z","iopub.status.idle":"2025-06-10T08:42:33.791901Z","shell.execute_reply.started":"2025-06-10T08:42:33.783154Z","shell.execute_reply":"2025-06-10T08:42:33.791073Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndevice =  'cuda' if torch.cuda.is_available() else \"cpu\"\n# model =  GPTMemoryEnhanced(GPT_CONFIG_124M_Memory).to(device)\nmodel =  GPTMemoryEnhanced(GPT_CONFIG_360M_Memory).to(device)\n\noptimizer =  torch.optim.AdamW(model.parameters() , lr=0.0004,weight_decay=0.01 )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:24:00.315088Z","iopub.execute_input":"2025-06-12T15:24:00.315510Z","iopub.status.idle":"2025-06-12T15:24:01.241987Z","shell.execute_reply.started":"2025-06-12T15:24:00.315469Z","shell.execute_reply":"2025-06-12T15:24:01.241052Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:24:02.434622Z","iopub.execute_input":"2025-06-12T15:24:02.434979Z","iopub.status.idle":"2025-06-12T15:24:02.442347Z","shell.execute_reply.started":"2025-06-12T15:24:02.434948Z","shell.execute_reply":"2025-06-12T15:24:02.441442Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"GPTMemoryEnhanced(\n  (embedding): InputEmbedding(\n    (embeddings): Embedding(102400, 512)\n  )\n  (memory_proj): Linear(in_features=512, out_features=512, bias=True)\n  (memory_expander): Linear(in_features=512, out_features=512, bias=True)\n  (transformer_block): ModuleList(\n    (0-11): 12 x TransformerBlock_v2(\n      (attention): MultiQueryAttentionBlock(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (w_qkv): Linear(in_features=512, out_features=640, bias=True)\n        (w_o): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (feed_forward): FeedForward(\n        (layers): Sequential(\n          (0): Linear(in_features=512, out_features=2048, bias=True)\n          (1): GELU()\n          (2): Linear(in_features=2048, out_features=512, bias=True)\n        )\n      )\n      (layernorm1): LayerNorm()\n      (layernorm2): LayerNorm()\n      (drop_out): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (memory): EfiBioSemanticMemory_V2(\n    (important_net): Sequential(\n      (0): Linear(in_features=128, out_features=128, bias=True)\n      (1): ReLU(inplace=True)\n      (2): Linear(in_features=128, out_features=1, bias=True)\n      (3): Sigmoid()\n    )\n    (update_gate): Sequential(\n      (0): Linear(in_features=384, out_features=1, bias=True)\n      (1): Hardsigmoid()\n    )\n    (forgot_gate): Sequential(\n      (0): Linear(in_features=512, out_features=128, bias=True)\n      (1): ReLU(inplace=True)\n      (2): Linear(in_features=128, out_features=3, bias=True)\n      (3): Sigmoid()\n    )\n    (forget_gate_net): Linear(in_features=256, out_features=128, bias=True)\n    (compression): Sequential(\n      (0): Linear(in_features=512, out_features=512, bias=True)\n      (1): RMSNorm((512,), eps=None, elementwise_affine=True)\n      (2): GELU(approximate='none')\n      (3): Linear(in_features=512, out_features=128, bias=True)\n    )\n    (decompression): Sequential(\n      (0): Linear(in_features=128, out_features=512, bias=True)\n    )\n    (W_cell): Linear(in_features=128, out_features=128, bias=False)\n    (memory_projection): Linear(in_features=128, out_features=512, bias=True)\n    (attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n    )\n    (mem_key_proj): Linear(in_features=128, out_features=512, bias=False)\n    (mem_value_proj): Linear(in_features=128, out_features=512, bias=False)\n  )\n  (final_norm): RMSNorm()\n  (fusion_gate): Sequential(\n    (0): Linear(in_features=1024, out_features=512, bias=True)\n    (1): Sigmoid()\n  )\n  (projection): ProjectionLayer()\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T08:45:11.674058Z","iopub.execute_input":"2025-06-10T08:45:11.674338Z","iopub.status.idle":"2025-06-10T08:45:11.679304Z","shell.execute_reply.started":"2025-06-10T08:45:11.674318Z","shell.execute_reply":"2025-06-10T08:45:11.678435Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 9])"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\ndef get_param_group_summary(model):\n    groups = defaultdict(int)\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if \"embedding\" in name:\n            groups[\"embedding\"] += param.numel()\n        elif \"transformer\" in name:\n            groups[\"transformer_blocks\"] += param.numel()\n        elif \"memory\" in name or \"episodic\" in name or \"semantic\" in name:\n            groups[\"memory_modules\"] += param.numel()\n        elif \"norm\" in name:\n            groups[\"normalization\"] += param.numel()\n        elif \"lm_head\" in name or \"projection\" in name:\n            groups[\"output_projection\"] += param.numel()\n        else:\n            groups[\"other\"] += param.numel()\n    total = sum(groups.values())\n    for k, v in groups.items():\n        print(f\"{k:20s}: {v:,} parameters\")\n    print(f\"\\nTotal: {total:,}\")\nget_param_group_summary(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:16:46.043036Z","iopub.execute_input":"2025-06-12T16:16:46.043359Z","iopub.status.idle":"2025-06-12T16:16:46.051974Z","shell.execute_reply.started":"2025-06-12T16:16:46.043336Z","shell.execute_reply":"2025-06-12T16:16:46.051165Z"}},"outputs":[{"name":"stdout","text":"embedding           : 52,429,312 parameters\nmemory_modules      : 1,699,723 parameters\ntransformer_blocks  : 32,312,832 parameters\nnormalization       : 512 parameters\nother               : 524,800 parameters\noutput_projection   : 102,400 parameters\n\nTotal: 87,069,579\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"\n\ntrain_ratio = 0.90\n\nfilename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\nwith open(filename , 'r') as f:\n    text_data =  json.load(f)\ntext_data = text_data[:1000]\n\nsplit = int(train_ratio * len(text_data))\n\ntrain_data =  text_data[:split]\nval_data =  text_data[split:]\n\ntrain_dataloader =  create_dataloader_v1(data=train_data , batch_size=6 , max_length=GPT_CONFIG_360M_Memory['context_length'] ,tokenizer = teacher_tokenizer ,  shuffle=True , drop_last= True)\nval_dataloader = create_dataloader_v1(data=val_data , batch_size=6 , max_length=GPT_CONFIG_360M_Memory['context_length']  ,tokenizer  = teacher_tokenizer, shuffle=False , drop_last=False )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T05:04:13.702921Z","iopub.execute_input":"2025-06-09T05:04:13.703239Z","iopub.status.idle":"2025-06-09T05:04:15.407508Z","shell.execute_reply.started":"2025-06-09T05:04:13.703211Z","shell.execute_reply":"2025-06-09T05:04:15.406721Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"teacher_model.config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:29:02.432867Z","iopub.execute_input":"2025-06-12T16:29:02.433311Z","iopub.status.idle":"2025-06-12T16:29:02.441781Z","shell.execute_reply.started":"2025-06-12T16:29:02.433276Z","shell.execute_reply":"2025-06-12T16:29:02.440488Z"}},"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"LlamaConfig {\n  \"_attn_implementation_autoset\": true,\n  \"_name_or_path\": \"deepseek-ai/deepseek-llm-7b-base\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 4096,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 30,\n  \"num_key_value_heads\": 32,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 102400\n}"},"metadata":{}}],"execution_count":114},{"cell_type":"code","source":" teacher_model,\n        student_model,\n        train_dataloader,\n        val_dataloader,\n        optimizer,\n        annealing_scheduler,\n        hidden_mapping_dict,\n        attn_mapping_dict,\n        student_hidden_dim,\n        teacher_hidden_dim,\n        student_num_layers,\n        teacher_num_layers,\n        device,\n        start_context:str ,\n        eval_freq = 5,\n        total_epochs=10,\n        grad_accum_steps=1,\n        ema_decay=0.999,\n        warmup_ratio=0.05","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"annealing_scherduler = AnnealingScheduler(tau_max=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:24:13.603837Z","iopub.execute_input":"2025-06-12T16:24:13.604094Z","iopub.status.idle":"2025-06-12T16:24:13.618102Z","shell.execute_reply.started":"2025-06-12T16:24:13.604074Z","shell.execute_reply":"2025-06-12T16:24:13.617254Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"student_layer_count  = 12 \nteacher_layer_count =  30\nattn_mapping =  {i:get_teacher_layer(i , student_layer_count , teacher_layer_count , 0.9) for i in range(student_layer_count)}\nhidden_mapping =  {i:get_teacher_layer(i , student_layer_count , teacher_layer_count , 0.7) for i in range(student_layer_count)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:25:20.358895Z","iopub.execute_input":"2025-06-12T16:25:20.359260Z","iopub.status.idle":"2025-06-12T16:25:20.363503Z","shell.execute_reply.started":"2025-06-12T16:25:20.359230Z","shell.execute_reply":"2025-06-12T16:25:20.362621Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T05:01:32.849652Z","iopub.execute_input":"2025-06-09T05:01:32.849942Z","iopub.status.idle":"2025-06-09T05:01:32.854943Z","shell.execute_reply.started":"2025-06-09T05:01:32.849920Z","shell.execute_reply":"2025-06-09T05:01:32.854207Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"150"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"next(iter(train_dataloader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T05:02:16.730583Z","iopub.execute_input":"2025-06-09T05:02:16.730971Z","iopub.status.idle":"2025-06-09T05:02:16.907558Z","shell.execute_reply.started":"2025-06-09T05:02:16.730933Z","shell.execute_reply":"2025-06-09T05:02:16.905970Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-e913730b33c1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-57-31856a3d8c5e>\", line 96, in collate_fn\n    inputs, targets = zip(*batch)\nValueError: too many values to unpack (expected 2)\n"],"ename":"ValueError","evalue":"Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-57-31856a3d8c5e>\", line 96, in collate_fn\n    inputs, targets = zip(*batch)\nValueError: too many values to unpack (expected 2)\n","output_type":"error"}],"execution_count":68},{"cell_type":"code","source":"teacher_tokenizer.decode(next(iter(train_dataloader))[0][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T05:01:34.543840Z","iopub.execute_input":"2025-06-09T05:01:34.544137Z","iopub.status.idle":"2025-06-09T05:01:34.730410Z","shell.execute_reply.started":"2025-06-09T05:01:34.544114Z","shell.execute_reply":"2025-06-09T05:01:34.728857Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-7768e3824e18>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mteacher_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-57-31856a3d8c5e>\", line 96, in collate_fn\n    inputs, targets = zip(*batch)\nValueError: too many values to unpack (expected 2)\n"],"ename":"ValueError","evalue":"Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-57-31856a3d8c5e>\", line 96, in collate_fn\n    inputs, targets = zip(*batch)\nValueError: too many values to unpack (expected 2)\n","output_type":"error"}],"execution_count":63},{"cell_type":"code","source":"text_data[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T04:49:29.526807Z","iopub.execute_input":"2025-06-09T04:49:29.527113Z","iopub.status.idle":"2025-06-09T04:49:29.532911Z","shell.execute_reply.started":"2025-06-09T04:49:29.527086Z","shell.execute_reply":"2025-06-09T04:49:29.532028Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'What are the three primary colors?',\n 'input': '',\n 'output': 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).'}"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:19:52.601993Z","iopub.execute_input":"2025-06-07T16:19:52.602264Z","iopub.status.idle":"2025-06-07T16:19:52.607485Z","shell.execute_reply.started":"2025-06-07T16:19:52.602243Z","shell.execute_reply":"2025-06-07T16:19:52.606803Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"215"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"train_encoder(model , train_dataloader )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:28:30.341972Z","iopub.execute_input":"2025-06-07T16:28:30.342330Z","iopub.status.idle":"2025-06-07T16:28:48.340037Z","shell.execute_reply.started":"2025-06-07T16:28:30.342267Z","shell.execute_reply":"2025-06-07T16:28:48.338997Z"}},"outputs":[{"name":"stdout","text":"tatal step  750\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80aa69917f994c699229a4b47189564c"}},"metadata":{}},{"name":"stdout","text":"saved memory_encoder.pth\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import torch._dynamo\nimport logging \ndyno_logger =  logging.getLogger('torch._dynamo')\ndyno_logger.setLevel(logging.ERROR)\nfunction_logger =  logging.getLogger('torch._functorch')\nfunction_logger.setLevel(logging.ERROR)\ntorch._dynamo.config.suppress_errors = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:31:56.017023Z","iopub.execute_input":"2025-06-12T16:31:56.017430Z","iopub.status.idle":"2025-06-12T16:31:56.023721Z","shell.execute_reply.started":"2025-06-12T16:31:56.017389Z","shell.execute_reply":"2025-06-12T16:31:56.022762Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"compiled_model = torch.compile(model, mode=\"max-autotune\", backend=\"aot_eager\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:31:56.938795Z","iopub.execute_input":"2025-06-12T16:31:56.939179Z","iopub.status.idle":"2025-06-12T16:31:57.036503Z","shell.execute_reply.started":"2025-06-12T16:31:56.939150Z","shell.execute_reply":"2025-06-12T16:31:57.035813Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"return_dict   = train_KD_advanced(teacher_model = teacher_model , student_model =model , \n                                  train_dataloader =  train_loader ,\n                                  val_dataloader =  val_loader ,\n                                  optimizer = optimizer ,\n                                  annealing_scheduler = annealing_scherduler ,\n                                  hidden_mapping_dict = hidden_mapping , \n                                  attn_mapping_dict = attn_mapping , \n                                  student_hidden_dim = 512  , \n                                  teacher_hidden_dim =teacher_model.config.hidden_size   , \n                                  student_num_layers = student_layer_count , \n                                  teacher_num_layers = teacher_layer_count , \n                                  device = device , \n                                  total_epochs = 5,\n                                  start_context = 'so much for the prerogative position of sensations in regard to our belief. But among the sensations themselves all are not deemed equally real.'\n                                   \n                                 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:39:15.294122Z","iopub.execute_input":"2025-06-12T16:39:15.294357Z","iopub.status.idle":"2025-06-12T16:39:15.300425Z","shell.execute_reply.started":"2025-06-12T16:39:15.294335Z","shell.execute_reply":"2025-06-12T16:39:15.299187Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f51db9b5ed49>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    total_epochs = 5\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"],"ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (<ipython-input-1-f51db9b5ed49>, line 13)","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"\n\nfrom tqdm.notebook import tqdm\n\nprint('start training')\ntrain_losses , val_losses , token_seen =  train_model(\n    teacher_model =teacher_model,\n    \n    model=compiled_model.to(device) , \n    train_dataloader=train_dataloader, \n    device=device, \n    eval_freq=5 , \n    eval_dataloader=val_dataloader , \n    optimizer=optimizer, \n    eval_iter=3,  \n    num_epochs= 10, \n    # start_context='### Instruction: What are the three primary colors? .n### Response:'\n    start_context = \"User: What are the three primary colors? Assistant: \"\n\n)\nprint(model.memory.get_memory_metrics())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:29:00.236772Z","iopub.execute_input":"2025-06-07T16:29:00.237080Z","iopub.status.idle":"2025-06-07T16:29:36.385620Z","shell.execute_reply.started":"2025-06-07T16:29:00.237056Z","shell.execute_reply":"2025-06-07T16:29:36.384200Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"start training\n🚀 Total training steps: 1500\nEncoder weights are loaded \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88db8bb8c3b34d73adf1eb6a30f38dca"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-ab1e9fbe3c37>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_losses , val_losses , token_seen =  train_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mteacher_model\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-3ba8b720b35a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(teacher_model, model, train_dataloader, device, eval_dataloader, optimizer, eval_freq, eval_iter, start_context, num_epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# 1) Compute loss under autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 loss , ce_loss , distillation_loss , memory_loss  = cal_loss_batch(\n\u001b[0m\u001b[1;32m     73\u001b[0m                     \u001b[0minput_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mtarget_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-2f7b60f422d8>\u001b[0m in \u001b[0;36mcal_loss_batch\u001b[0;34m(input_batch, target_batch, teacher_model, student_model, device, mem_cof, distil_temp, alpha)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 2. Student output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# student_logits, x_emb, mem_output = student_model(input_batch)  # [B, T, V]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mstudent_output\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mstudent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mstudent_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_output\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mstudent_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstudent_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_emb'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstudent_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-093920dc5a40>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tokens, mask, output_hidden_state, output_attentions)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmemory_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmem_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# memory_out, _ ,_ =  self.memory(x.las_hidden_state.mean(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmem_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_expander\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-093920dc5a40>\u001b[0m in \u001b[0;36mtorch_dynamo_resume_in_forward_at_35\u001b[0;34m(___stack0, self, x, x_emb, hidden_states, all_attentions)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmemory_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmem_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# memory_out, _ ,_ =  self.memory(x.las_hidden_state.mean(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmem_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_expander\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;31m# Just for convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mruntime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             ), torch.enable_grad():\n\u001b[0;32m--> 308\u001b[0;31m                 all_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m    309\u001b[0m                     \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteal_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_boxed_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boxed_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m                 \u001b[0;31m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m                 \u001b[0;31m#   in the fw output order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m                 fw_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m   1526\u001b[0m                     \u001b[0mCompiledFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_fw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 )\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36minner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mold_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Inductor cache DummyModule can return None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/debugging.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mboxed_nop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxed_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boxed_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mboxed_run\u001b[0;34m(self, args_list)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0margs_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_traceback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# Main Node running APIs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# Execute the function and return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T07:07:10.035257Z","iopub.execute_input":"2025-06-05T07:07:10.035764Z","iopub.status.idle":"2025-06-05T07:09:55.181667Z","shell.execute_reply.started":"2025-06-05T07:07:10.035719Z","shell.execute_reply":"2025-06-05T07:09:55.180564Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torch\n  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.3.1 (from torch)\n  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch) (75.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nDownloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cufile-cu12, nvidia-cudnn-cu12, nvidia-cuda-nvrtc-cu12, torch\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"model.memory.model_save(path =  'Memory_saved.pth')\nmodel.memory.model_save(path = 'memory_saved.pt')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# Assume `memory` is your EfiBioSemanticMemory_V2 instance, already initialized\n# and `W_cell` (compression) is frozen or pretrained so that random_input→embedding.\nmemory = model.memory\nmemory.to(device)\nmemory.eval()\nmemory.hit_count  = torch.tensor(0).to(device) # if you have a method; otherwise manually zero out hit_count, write_count, query_count\nmemory.write_count  = torch.tensor(0).to(device) \nmemory.query_count = torch.tensor(0).to(device) \n# Helper to run a single step and report\ndef step_test(x):\n    x= x.unsqueeze(0).unsqueeze(0)\n    print(x.shape)\n    out, retrieved, topk_idx, attn_weights = memory(x , training =  False ) # x shape [D]; expand to [1,1,D]\n    max_sim = float(F.cosine_similarity(memory.key_memory[topk_idx.squeeze()], memory.key_memory[topk_idx.squeeze()], dim=-1).mean())\n    return max_sim\n\n# 1) Same vector twice\nvec1 = torch.randn(memory.input_dim, device=memory.key_memory.device)\nsim1 = step_test(vec1)\nsim2 = step_test(vec1)\n\n# 2) Two distinct vectors\nvec2 = torch.randn(memory.input_dim, device=memory.key_memory.device)\nsim3 = step_test(vec2)\nsim4 = step_test(vec2)\n\nprint(\" sims: first_pass(vec1) =\", f\"{sim1:.4f}\",\n      \"| second_pass(vec1) =\", f\"{sim2:.4f}\")\nprint(\" sims: first_pass(vec2) =\", f\"{sim3:.4f}\",\n      \"| second_pass(vec2) =\", f\"{sim4:.4f}\")\n\nprint(\" final metrics:\",\n      f\"hit_count={memory.hit_count.item()}\",\n      f\"write_count={memory.write_count.item()}\",\n      f\"query_count={memory.query_count.item()}\")\n\n# Expected outcome:\n# - sim1 < hit_threshold (no slot existed) → write_count=1\n# - sim2  ≃ 0.9–1.0 (slot now exists) → hit_count=1\n# - sim3 < hit_threshold (new) → write_count=2\n# - sim4  ≃ 0.9–1.0 → hit_count=2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T15:12:39.331726Z","iopub.execute_input":"2025-06-02T15:12:39.332044Z","iopub.status.idle":"2025-06-02T15:12:39.531780Z","shell.execute_reply.started":"2025-06-02T15:12:39.332016Z","shell.execute_reply":"2025-06-02T15:12:39.531021Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"torch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.0303, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(0, device='cuda:0')\nWriting Happen\nWriting Happen\ntorch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.9465, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(1, device='cuda:0')\nUpdate Happen\n average update_gate: 0.6290084719657898  std: 0.001736109028570354\ntorch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.3093, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(1, device='cuda:0')\nWriting Happen\nWriting Happen\ntorch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.9538, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(2, device='cuda:0')\nUpdate Happen\n average update_gate: 0.8370599150657654  std: 0.010752998292446136\n sims: first_pass(vec1) = nan | second_pass(vec1) = 1.0000\n sims: first_pass(vec2) = nan | second_pass(vec2) = 1.0000\n final metrics: hit_count=2 write_count=2 query_count=4\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"model.memory.write_count ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.416079Z","iopub.status.idle":"2025-05-21T17:14:19.416328Z","shell.execute_reply":"2025-05-21T17:14:19.416229Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# If you loaded from a JSON file\n# with open(\"loss_history.json\", \"r\") as f:\n#     data = json.load(f)\n#     train_losses = data[\"train_loss\"]\n#     val_losses = data[\"val_loss\"]\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\nplt.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\nplt.xlabel(\"Evaluation Step\")\npri\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")  # Save the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.425933Z","iopub.status.idle":"2025-05-21T17:14:19.426202Z","shell.execute_reply":"2025-05-21T17:14:19.426098Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\n\n# Load model\nmodel = GPTMQModel2(GPT_CONFIG_124M)\n# model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_7.pt\"))\ncheckpoint = torch.load(\"checkpoint_epoch_7.pt\")\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\nmodel.eval().to(device)\n\n# Tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Utility functions\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n# Sampling-based generate function (uses your logic)\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\nf\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n# High-level text generation function\ndef generate_response(prompt, model, tokenizer, max_new_tokens=100, context_size=128, temperature=1.0, top_k=50):\n    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n    generated_ids = generate(\n        model=model,\n        idx=input_ids,\n        max_new_tokens=max_new_tokens,\n        context_size=context_size,\n        temperature=temperature,\n        top_k=top_k\n    )\n    return token_ids_to_text(generated_ids, tokenizer)\n\n# Try it out\n# prompt = \"### Instruction:\\nExplain what is deep learning.\\n\\n### Response:\\n <bot>\"\nprompt = \"\"\"\n\n'### Instruction :Give three tips for staying healthy ### Response:'\n\"\"\".strip()\n\n\noutput = generate_response(prompt, model, tokenizer)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.427072Z","iopub.status.idle":"2025-05-21T17:14:19.427368Z","shell.execute_reply":"2025-05-21T17:14:19.427264Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is generated\n        if idx_next.item() == end_token_id:\n            break\n\n    return idx\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\nraw_output = generate_response(prompt, model, tokenizer)\ncleaned_output = truncate_after_n_bullets(raw_output)\nprint(cleaned_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.428166Z","iopub.status.idle":"2025-05-21T17:14:19.428492Z","shell.execute_reply":"2025-05-21T17:14:19.428345Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_response(\n    prompt, model, tokenizer,\n    temperature=0.8,  # better balance\n    top_k=40,         # a bit narrower selection\n    max_new_tokens=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.429419Z","iopub.status.idle":"2025-05-21T17:14:19.429775Z","shell.execute_reply":"2025-05-21T17:14:19.429611Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device).unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is in the generated output\n        if end_token_id in idx_next:\n            break\n\n    return idx\n\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\n\n# 🔁 Input prompt\nprompt = \"### Instruction: What are the three primary colors? \\n### Response:\"\n\n# 🔁 Tokenize input\ninput_ids = text_to_token_ids(prompt, tokenizer).to(device)\n\n# 🔁 Generate output tokens\noutput_ids = generate(\n    model=model,\n    idx=input_ids,\n    max_new_tokens=100,\n    context_size=128,\n    temperature=0.7,\n    top_k=40\n)\n\n# 🔁 Decode and postprocess\noutput_text = tokenizer.decode(output_ids[0].tolist())\n\n# ✂️ Truncate after 3 bullets (optional)\nfinal_output = truncate_after_n_bullets(output_text)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.421013Z","iopub.status.idle":"2025-04-24T09:34:44.421333Z","shell.execute_reply":"2025-04-24T09:34:44.421177Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}