{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339249,"sourceType":"datasetVersion","datasetId":7093845},{"sourceId":11378548,"sourceType":"datasetVersion","datasetId":7124129},{"sourceId":11378997,"sourceType":"datasetVersion","datasetId":7124489},{"sourceId":12037038,"sourceType":"datasetVersion","datasetId":7574150},{"sourceId":12046667,"sourceType":"datasetVersion","datasetId":7581007},{"sourceId":12071125,"sourceType":"datasetVersion","datasetId":7598368},{"sourceId":12076822,"sourceType":"datasetVersion","datasetId":7602140}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math \nimport torch \nimport torch.nn as nn \nimport transformers \nfrom tqdm.notebook import tqdm\n# Memory Network\nimport torch.nn.functional as F \nimport random\nfrom typing import Tuple , Optional\nimport torch.bin \nfrom transformers import AutoModelForCausalLM , AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:02.712161Z","iopub.execute_input":"2025-06-06T06:07:02.712466Z","iopub.status.idle":"2025-06-06T06:07:09.227184Z","shell.execute_reply.started":"2025-06-06T06:07:02.712443Z","shell.execute_reply":"2025-06-06T06:07:09.226258Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\ntorch.backends.cudnn.benchmark = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.228279Z","iopub.execute_input":"2025-06-06T06:07:09.228721Z","iopub.status.idle":"2025-06-06T06:07:09.232455Z","shell.execute_reply.started":"2025-06-06T06:07:09.228698Z","shell.execute_reply":"2025-06-06T06:07:09.231730Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# RMS NORM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim)) \n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        norm_x = self._norm(x.float()).type_as(x) \n        return norm_x * self.scale \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.233638Z","iopub.execute_input":"2025-06-06T06:07:09.233863Z","iopub.status.idle":"2025-06-06T06:07:09.253236Z","shell.execute_reply.started":"2025-06-06T06:07:09.233843Z","shell.execute_reply":"2025-06-06T06:07:09.252340Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Embedding Layer","metadata":{}},{"cell_type":"code","source":"import torch \n\nclass EmbeddingLayer(torch.nn.Module):\n    def __init__(self , vocab_size , embedding_dim):\n        super().__init__()\n\n        self.embedding_layer= torch.nn.Embedding(vocab_size , embedding_dim)\n\n    def forward(self , input_tokens):\n        return self.embedding_layer(input_tokens)\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.254611Z","iopub.execute_input":"2025-06-06T06:07:09.254932Z","iopub.status.idle":"2025-06-06T06:07:09.269911Z","shell.execute_reply.started":"2025-06-06T06:07:09.254911Z","shell.execute_reply":"2025-06-06T06:07:09.269151Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# FeedForward Layer ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n      \n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n            GELU(),\n            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n        )\n    def forward(self, x ):\n        return self.layers(x)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.270691Z","iopub.execute_input":"2025-06-06T06:07:09.270973Z","iopub.status.idle":"2025-06-06T06:07:09.286663Z","shell.execute_reply.started":"2025-06-06T06:07:09.270946Z","shell.execute_reply":"2025-06-06T06:07:09.285918Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Normalization Layer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch \n\nclass LayerNorm(nn.Module):\n    def __init__(self , emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale  = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self , x):\n        mean = x.mean(dim= -1, keepdim = True)\n        var = x.var(dim =-1, keepdim = True)\n        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n        return self.scale * norm_x + self.shift \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.287326Z","iopub.execute_input":"2025-06-06T06:07:09.287562Z","iopub.status.idle":"2025-06-06T06:07:09.308451Z","shell.execute_reply.started":"2025-06-06T06:07:09.287540Z","shell.execute_reply":"2025-06-06T06:07:09.307604Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# RoPE Embdding ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport torch \nfrom dataclasses import dataclass\n\n\nclass NRopE: # RopE in Numpy \n    def rotate_2d(self,vec , theta_p):\n        cos_theta  , sin_theta  = np.cos(theta_p) , np.sin(theta_p)\n        rotat_vec = np.array([[cos_theta , -sin_theta],\n                    [sin_theta ,cos_theta]])\n        \n        return rotat_vec @ vec\n\n\n    def RoPe(self,x , p , theta = 10000):\n        d = len(x)\n        x_rotate =  np.zeros_like(x)\n        for i in range(0 , d , 2):\n            if i +1< d:\n                theta_p = (theta **(-2*(i//2)))**p \n                roted_pair = self.rotate_2d(x[i:i+1] , theta_p)    \n                x_rotate[i:i+1] = roted_pair\n\n        return x_rotate\n\n\n\n@dataclass\nclass TRopE(torch.nn.Module): # RopE in torch \n    def __init__(self, dim:int ,theta:float = 10000):\n        self.dim = dim \n        self.theta = theta \n        self.freq =  torch.pow(self.theta ,-torch.arange(0 ,dim  , 2)/dim )\n        torch.nn.Parameter('freq' , self.freq)\n\n    def forward(self, x:torch.Tensor , pos:torch.Tensor):\n        batch_size , seq_len, dim = x.shape\n        assert dim ==self.dim ,\"Error dim must be same\"\n        theta_p = torch.einsum(\"n,d->nd\" , pos, self.freq.to(x.device))\n        cos_theta  , sin_theta = torch.cos(theta_p) , torch.sin(theta_p)\n        x_even , x_odd =  x[... , ::2] , x[... , 1::2]\n        x_rotated =  torch.empty_like(x)\n        x_rotated[...,::2] =  x_even * cos_theta - x_odd * sin_theta\n        x_rotated[...,1::2] =  x_even * sin_theta + x_odd * cos_theta\n\n        return x_rotated\n\n\n\n\n\n\n\ndef precompute_freq_cis(  dim:int , end:int , theta:float = 10000.0):\n        \"\"\"dim : dimentions \n        end: end index   \n        \"\"\"\n        freqs =  1/(theta **(torch.arange(0 , dim , 2)[:dim//2].float() / dim))\n        t =  torch.arange(end, device=freqs.device)\n        freqs = torch.outer(t , freqs).float()\n        freqs_cis =  torch.polar(torch.ones_like(freqs), freqs)\n        return freqs_cis \n\n\ndef reshape_for_broadcast(freq_cis  , x):\n        \"\"\" reshape the freqcies to match x dimentions \"\"\"\n        ndim=  x.ndim\n        assert 0<=1<ndim \n        assert freq_cis.shape == (x.shape[1], x.shape[-1]), f\"Expected {(x.shape[1], x.shape[-1])}, got {freq_cis.shape}\" \n        shape = [d if i == 1 or i ==  ndim -1 else 1 for i , d in enumerate(x.shape)]\n        return freq_cis.view(*shape)\n\n\ndef apply_rotary_embedding( xq:torch.Tensor ,xk:torch.Tensor ,  freq_cis:torch.Tensor):\n\n            xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n            xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n\n\n            freq_cies =  reshape_for_broadcast(freq_cis , xq_)\n    \n\n            xq_out = torch.view_as_real(xq_* freq_cies).flatten(3)\n            \n            xk_out = torch.view_as_real(xk_*freq_cies).flatten(3)\n\n\n            return  xq_out.type_as(xq)   ,  xk_out.type_as(xq) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.309230Z","iopub.execute_input":"2025-06-06T06:07:09.309465Z","iopub.status.idle":"2025-06-06T06:07:09.325045Z","shell.execute_reply.started":"2025-06-06T06:07:09.309445Z","shell.execute_reply":"2025-06-06T06:07:09.324192Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# MultiHead & MultiQuery Attention Layer ","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention_V2(nn.Module):\n    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n        super().__init__()\n        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n        self.d_in =d_in\n        self.d_out = d_out\n        self.dropout = nn.Dropout(dropout)\n        self.num_heads  = num_heads\n        self.head_dim = d_out // num_heads\n        self.out_proj  = nn.Linear(d_out , d_out)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens , d_in = x.shape\n        keys = self.w_key(x)\n        queries  = self.w_query(x)\n        values = self.w_value(x)\n        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(2, 3)\n        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n        attn_scores.masked_fill(mask_bool , -torch.inf)\n        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n        attn_weights = self.dropout(attn_weights)\n        context_vector = (attn_weights  @ values).transpose(1, 2)\n        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n        context_vector = self.out_proj(context_vector)\n        return context_vector\n\n\n\n\ndef apply_rotary_embedding(xq:torch.Tensor , xk:torch.Tensor , freq_cies:torch.Tensor):\n\n    assert xq.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n    assert xk.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1] , -1, 2))\n\n    freq_cies = reshape_for_broadcast(freq_cies , xq_)\n\n    xq_out = torch.view_as_real(xq_ * freq_cies ).flatten(3)\n\n    xk_out = torch.view_as_real(xk_ * freq_cies).flatten(3)\n\n    return xq_out.type_as(xq) ,  xk_out.type_as(xk)\n\n\n\n\nclass MultiQueryAttentionBlock(nn.Module):\n    def __init__(self, d_model:int , h:int , dropout:float , seq_len:int , qkv_bias =  False ):\n        super().__init__()\n        self.d_model  = d_model \n\n        self.seq_len=  seq_len\n\n        assert d_model % h == 0, \"d_model is must be divided by th head\"\n        self.dropout = nn.Dropout(dropout)\n\n        self.h = h  \n\n        self.d_k = d_model // h\n\n        self.w_qkv =  nn.Linear(d_model , d_model +2 * self.d_k )\n\n        self.w_o = nn.Linear(d_model  , d_model)\n\n        freq_cies = precompute_freq_cis(dim=self.d_k , end=self.seq_len * 2 )\n\n        self.register_buffer('freq_cies' , freq_cies , persistent= False )\n\n    def generate_causal_mask(self, seq_len, device):\n        # shape: (1, 1, seq_len, seq_len)\n        return torch.tril(torch.ones((1, 1, seq_len, seq_len), device=device)).bool()\n\n    @staticmethod\n    def attention(q, k  , v,mask  , dropout):\n        d_k = q.shape[-1]\n\n        attention_score =  (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n\n        if mask is not None :\n            if mask.dim() == 2:\n                      mask = mask.unsqueeze(1).unsqueeze(2)\n            elif mask.dim() == 3:\n                     mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n        \n        attention_score = attention_score.softmax(dim=-1)\n\n        if dropout is not None :\n            attention_score = dropout(attention_score)\n\n        context_vector =  attention_score @ v\n\n        return context_vector  , attention_score\n    \n\n\n    def forward(self, q, mask= None):\n        if mask is None:\n            mask = self.generate_causal_mask(self.seq_len , device = q.device)\n        qkv =  self.w_qkv(q)\n\n        query , key, value =  torch.split(qkv , [self.d_model  , self.d_k , self.d_k], dim=-1)\n\n        query = query.view(query.shape[0] , -1 , self.h , self.d_k).transpose(1, 2)\n\n        key =  key.unsqueeze(1)\n\n        value =  value.unsqueeze(1)\n\n        seq_len =  q.size(1)\n\n        freq_cies = self.freq_cies[:query.shape[1]].to(q.device)\n\n        # freq_cies =  self.freq_cies[:seq_len].to(q.device)\n\n        query , key = apply_rotary_embedding(query , key , freq_cies)\n\n        x , self.attention_score = MultiQueryAttentionBlock.attention(q = query,k =  key,v= value ,mask=mask , dropout= self.dropout)\n\n        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h* self.d_k)\n\n        x = self.w_o(x)\n\n        return x \n    \n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.327489Z","iopub.execute_input":"2025-06-06T06:07:09.327757Z","iopub.status.idle":"2025-06-06T06:07:09.344411Z","shell.execute_reply.started":"2025-06-06T06:07:09.327737Z","shell.execute_reply":"2025-06-06T06:07:09.343499Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Memory Network ","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn \nimport torch.nn.functional as F \nimport random\n\n\nclass EfiBioSemanticMemory_V2(nn.Module):\n    def __init__(self, input_dim:int ,semantic_memory_dim , max_slots:int = 1000 , compress_dim:int =  128 , top_k:int = 5 , num_heads:int =  4 ):\n        super().__init__()\n\n        self.input_dim = input_dim \n        self.max_slots =  max_slots \n        self.compress_dim =  compress_dim \n        self.top_k =  top_k \n        self.num_heads =  num_heads \n        self.semantic_memory_dim = semantic_memory_dim\n        # self.memory_size =  semantic_memory_dim \n\n\n\n        self.key_memory =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.value_memory = nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.cell_state =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.register_buffer('active_mask' , torch.zeros(max_slots , dtype= torch.bool))\n        self.active_mask[:semantic_memory_dim] = True  \n\n        \n        # Meta data parameter \n        self.register_buffer('age', torch.zeros(max_slots))\n        self.register_buffer('usage', torch.zeros(max_slots))\n        self.register_buffer('concept_energy', torch.ones(max_slots))\n        self.register_buffer('memory_age', torch.zeros(max_slots))\n        self.register_buffer('access_count', torch.zeros(max_slots))\n        self.register_buffer(\"_memory_version\", torch.tensor(0))\n        self.concept_energy[:semantic_memory_dim] =  0.2\n        # self.new_slot_mask  = torch.zeros(self.memory_size).bool()\n\n        #stats params\n        self.register_buffer(\"step_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer('replay_count', torch.zeros(1 , dtype= torch.long))\n        self.register_buffer(\"query_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"novel_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"write_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"hit_count\", torch.zeros(1, dtype=torch.long)) \n        self.register_buffer('merge_count' , torch.zeros(1  , dtype= torch.long))\n        self.register_buffer('neuroslot_count' , torch.zeros(1, dtype= torch.long))\n        self.register_buffer('prune_count' , torch.zeros(1, dtype=torch.long))\n        self.register_buffer('consalidate_count', torch.zeros(1,dtype=torch.long))\n        self.register_buffer('update_count', torch.zeros(1,dtype=torch.long))\n\n\n        self.initial_write_step  = 300\n        \n        # Threshold Parameter \n        self.consolidation_threshold = nn.Parameter(torch.tensor(100.0))\n        self.energy_threshold = nn.Parameter(torch.tensor(0.2))\n        self.decay_rate = nn.Parameter(torch.tensor(0.99))\n        # self.novelty_threshold = nn.Parameter(torch.tensor(0.2))\n        # self.novelty_threshold = 0.2 * (1 - (self.memory_size / self.max_slots))\n        self.novelty_threshold = 0.1\n\n        self.register_buffer(\"prune_age_threshold\", torch.tensor(100))\n        self.register_buffer(\"neurogenesis_threshold\", torch.tensor(0.5))\n        self.register_buffer(\"new_slot_maturation_steps\", torch.tensor(20)) \n        self.synaptic_scale = nn.Parameter(torch.tensor(0.5))\n        self.sparsity = nn.Parameter(torch.tensor(0.5))\n        self.sim_thershold =  nn.Parameter(torch.tensor(0.5))\n        self.confidence_threshold_att = 0.15 \n\n        # Concept queue Params\n        self.register_buffer('queue_max_size' , torch.tensor(1000))\n        self.register_buffer('concept_queue' ,  torch.zeros(self.queue_max_size , self.compress_dim))\n        self.queue_ptr = 0\n        self.queue_count = 0 \n\n        # Networks \n        self.important_net = nn.Sequential(\n            nn.Linear(compress_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        \n\n        self.update_gate = nn.Sequential(\n            nn.Linear(3 * compress_dim, 1),\n            nn.Hardsigmoid()\n        )\n\n        for layer in self.update_gate:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n                nn.init.constant_(layer.bias, 0.1) \n        self.forgot_gate = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 3),\n            nn.Sigmoid()\n        )\n        self.forget_gate_net = nn.Linear(compress_dim * 2, compress_dim)\n\n        self.compression = nn.Sequential(\n            nn.Linear(input_dim, semantic_memory_dim),\n            nn.RMSNorm(semantic_memory_dim),\n            nn.GELU(),\n            nn.Linear(semantic_memory_dim, self.compress_dim)\n        )\n\n        self.decompression = nn.Sequential(\n            nn.Linear(self.compress_dim, input_dim),\n        )\n\n        self.W_cell = nn.Linear(self.compress_dim, compress_dim, bias=False)\n        self.memory_projection = nn.Linear(self.compress_dim, self.input_dim)\n        # self.query_proj =  nn.Linear(self.semantic_memory_dim  , self.compress_dim)\n        self.attn = nn.MultiheadAttention(\n            embed_dim=compress_dim,\n            num_heads=num_heads,\n            batch_first=False\n        )\n        self.mem_key_proj   = nn.Linear(compress_dim, semantic_memory_dim, bias=False)\n        self.mem_value_proj = nn.Linear(compress_dim, semantic_memory_dim, bias=False)\n        self.no_memory_embedding = nn.Parameter(torch.randn(1, 1, self.input_dim))\n\n        nn.init.kaiming_uniform_(self.key_memory, mode='fan_out')\n        nn.init.xavier_normal_(self.value_memory)\n        nn.init.xavier_normal_(self.cell_state)\n        # centroids =  centroids.to(self.key_memory.device)\n        # self.key_memory.data = F.normalize(centroids.clone(), dim=-1)\n        # self.value_memory.data = F.normalize(centroids.clone(), dim=-1)\n        # self.cell_state.data = F.normalize(centroids.clone(), dim=-1)\n\n    def _get_active_memory(self):\n\n        \"\"\"\n            Get the active memries slot \n        \"\"\"\n        idx = torch.nonzero(self.active_mask, as_tuple=False).squeeze(1)\n        assert idx.numel() > 0, \"No active memory slots\"\n       \n        return  (\n            self.key_memory[idx] , \n            self.value_memory[idx], \n            self.cell_state[idx]\n\n        )\n        \n    @property \n    def active_capacity(self):\n        return torch.sum(self.active_mask).item()/ self.max_slots\n\n\n    def _adaptive_decay(self, memory_idx):\n        energy = self.concept_energy[memory_idx]\n        access = self.usage[memory_idx]\n\n        decay = torch.exp((1-self.decay_rate) * (1-energy) * (1-access))\n        return decay  \n    \n    def _boost_energy_on_access(self,indices , sims):\n        weights =  F.softmax(sims , dim=-1)\n        boost =  weights * 0.1 \n        self.concept_energy[indices] = torch.clamp(\n            self.concept_energy[indices] * 0.95 + boost, 0, 1)\n        \n        \n    def _update_memory(self, topk_idx: torch.LongTensor, cells: torch.Tensor,\n                   projected: torch.Tensor, update_gates: torch.Tensor):\n        projected_exp = projected.unsqueeze(1).expand(-1, cells.size(1), -1)\n    \n        cell_input = torch.sigmoid(cells + projected_exp)  \n    \n        decay_factor = self._adaptive_decay(topk_idx)     \n        decay_factor = decay_factor.unsqueeze(-1)         \n        cell_updates = decay_factor * cell_input\n    \n        # apply update_gates: [B, top_k, D]\n        delta = update_gates.unsqueeze(-1) * cell_updates\n    \n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0]\n        new_slot_mask = torch.isin(topk_idx, active_indices[-10:])\n        if new_slot_mask.any():\n            cell_updates = cell_updates.clone()  \n            cell_updates[new_slot_mask] *= 0.1\n            delta = delta * torch.where(new_slot_mask.unsqueeze(-1), 0.5, 1.0)\n    \n        delta = delta * self.synaptic_scale   \n    \n        batch_size, top_k, dim = delta.shape\n        flat_topk_idx = topk_idx.view(-1)             \n        flat_delta    = delta.view(-1, dim)         \n    \n        flat_cells = F.normalize(cells, dim=-1).view(-1, dim)  \n    \n        self.age.data += 1\n        self.update_count += 1\n        assert flat_topk_idx.max() < self.key_memory.size(0), (\n            f\"Index {flat_topk_idx.max()} >= {self.key_memory.size(0)}\"\n        )\n        max_slots, _ = self.cell_state.shape \n        device = self.cell_state.device\n        dtype = self.cell_state.dtype\n    \n        delta_buffer = torch.zeros((max_slots, dim), device=device, dtype=dtype)\n        key_buffer   = torch.zeros_like(delta_buffer)\n        val_buffer   = torch.zeros_like(delta_buffer)\n    \n        \n        idx_expanded = flat_topk_idx.unsqueeze(-1).expand(-1, dim) \n        delta_buffer.scatter_add_(0, idx_expanded, flat_delta)\n    \n        key_buffer.scatter_add_(0, idx_expanded, flat_cells)\n        val_buffer.scatter_add_(0, idx_expanded, flat_cells)\n    \n        with torch.no_grad():\n            self.cell_state.data.add_(delta_buffer)\n            self.key_memory.data.add_(key_buffer)\n            self.value_memory.data.add_(val_buffer)\n    \n            updated_idx = flat_topk_idx.unique()\n            self.age.data[updated_idx] = 0\n    \n            decay = self.decay_rate ** self.age.unsqueeze(-1)\n            self.cell_state.data.mul_(decay)\n            self.key_memory.data.mul_(decay)\n            self.value_memory.data.mul_(decay)\n    \n            k_norm = F.normalize(self.key_memory.data[updated_idx], dim=-1)\n            v_norm = F.normalize(self.value_memory.data[updated_idx], dim=-1)\n            c_norm = F.normalize(self.cell_state.data[updated_idx], dim=-1)\n    \n            self.key_memory.data[updated_idx]   = torch.tanh(k_norm)\n            self.value_memory.data[updated_idx] = torch.tanh(v_norm)\n            self.cell_state.data[updated_idx]   = torch.tanh(c_norm)\n\n\n\n    def _get_low_energy_slots(self, candidate_indices):\n        if len(candidate_indices) == 0:\n            return candidate_indices \n        candidate_energy =  self.concept_energy[candidate_indices]\n\n        sorted_indices   =  torch.argsort(candidate_energy)\n        return candidate_indices[sorted_indices]\n       \n\n    @property\n    def memory_size(self):\n        return self.active_mask.sum().item()\n    \n    @property\n    def utilization(self):\n        return    self.active_mask.sum().item() / self.memory_size\n\n        \n    # @torch.no_grad()\n    def _batch_update_with_old(self, indices: torch.LongTensor, new_data: torch.Tensor):\n     \n        old_keys = self.key_memory[indices]      \n        old_vals = self.value_memory[indices]     \n        old_cells = self.cell_state[indices]     \n    \n        gate_input = torch.cat([old_cells, old_keys, new_data], dim=-1)\n    \n        pre = self.update_gate(gate_input)            \n        gate = self.hard_sigmoid(pre)                \n    \n\n    \n        with torch.no_grad():\n            blended_key = gate * new_data + (1.0 - gate) * old_keys   # [N, D]\n            blended_val = (0.35 * old_vals) + (0.65 * new_data * (1.0 - gate))  # [N, D]\n            blended_cell = gate * new_data + (1.0 - gate) * old_cells  # [N, D]\n    \n            new_key_norm = F.normalize(blended_key, dim=-1)\n            new_val_norm = F.normalize(blended_val, dim=-1)\n            new_cell_norm = F.normalize(blended_cell, dim=-1)\n    \n            self.key_memory.data[indices]   = new_key_norm\n            self.value_memory.data[indices] = new_val_norm\n            self.cell_state.data[indices]   = new_cell_norm\n    \n            self.usage[indices] = 0.0\n            self.age[indices] *= 0.25\n            self.memory_age[indices] = 0\n            self.concept_energy[indices] = 0.5\n            self.access_count[indices] = 0\n    \n\n\n    def _batch_update_with_old(self , indices , new_data, importance_score = None):\n        if importance_score is None:\n            importance_score =  torch.ones_like(indices, dtype = torch.float32) * 0.5\n\n        old_keys =  self.key_memory[indices]\n        old_vals =  self.value_memory[indices]\n        old_cell =  self.cell_state[indices]\n        concept_energy =  self.concept_energy[indices]\n\n        gate_input =  torch.cat([old_keys, new_data]  , dim=-1)\n        forget_gate =  torch.sigmoid(self.forget_gate_net(gate_input))\n        inactive_mask = (concept_energy < 0.15).float().unsqueeze(-1)\n        write_gate = torch.clamp(importance_score.unsqueeze(-1) + inactive_mask, 0.0, 1.0)\n    \n        # Update memory with gated blend\n        updated_keys = F.normalize((1 - forget_gate) * old_keys + forget_gate * new_data, dim=-1)\n        updated_vals = F.normalize(0.35 * old_vals + 0.65 * new_data, dim=-1)\n    \n        updated_cell = F.normalize(\n            self.update_gate(torch.cat([old_cell, old_keys, new_data], dim=-1)) * new_data\n            + (1 - self.update_gate(torch.cat([old_cell, old_keys, new_data], dim=-1))) * old_cell,\n            dim=-1\n        )\n    \n        # Final gated write\n        with torch.no_grad():\n            self.key_memory[indices] = (1 - write_gate) * old_keys + write_gate * updated_keys\n            self.value_memory[indices] = (1 - write_gate) * old_vals + write_gate * updated_vals\n            self.cell_state[indices] = (1 - write_gate) * old_cell + write_gate * updated_cell\n    \n            # Reset metadata\n            self.usage[indices] = 0.0\n            self.age[indices] *= 0.25\n            self.memory_age[indices] = 0\n            self.concept_energy[indices] = 0.5\n            self.access_count[indices] = 0\n            \n\n    @torch.no_grad()\n    def _batch_update_with_new(self, new_idx, new_data):\n        self.key_memory[new_idx] = new_data\n        self.value_memory[new_idx] = new_data\n        self.cell_state[new_idx] = new_data\n        self.usage[new_idx] = 0.0\n        self.age[new_idx] = 0\n        self.access_count[new_idx] = 0\n        self.memory_age[new_idx] = 0\n        self.concept_energy[new_idx] = 0.5\n        self.active_mask[new_idx] = True\n\n    \n    def _write_memory_update(self, new_concepts: torch.Tensor, retry_count=0):\n        # if retry_count ==  0 and self.query_count > 0:\n        #     self.flush_concept_queue()\n        # print('Writing Happen')\n        if retry_count > 2 or new_concepts.size(0) == 0:\n            return\n        importance = self.important_net(new_concepts).squeeze(-1)\n        keep_mask = importance > 0.10\n        if not keep_mask.any():\n            return\n\n        new_concepts = new_concepts[keep_mask]\n        remaining = new_concepts.size(0)\n\n        # 1. Update low-energy active slots\n        low_energy_candidate = torch.where(self.active_mask & (self.energy_threshold > self.concept_energy))[0]\n        if low_energy_candidate.numel() > 0:\n            candidate = self._get_low_energy_slots(low_energy_candidate)\n            num_reuse = min(len(candidate), remaining)\n            if num_reuse > 0:\n                self._batch_update_with_old(indices=candidate[:num_reuse], new_data=new_concepts[:num_reuse])\n                new_concepts = new_concepts[num_reuse:]\n                remaining = new_concepts.size(0)\n\n        # 2. Write to new slots\n        if remaining > 0 and self.memory_size < self.max_slots:\n            add = min(remaining, self.max_slots - self.memory_size)\n            new_idx = self.neurogenesis(return_index=True, required_slots=add)\n            if new_idx is not None:\n                self._batch_update_with_new(new_idx, new_concepts[:add])\n            # self.memory_size += add\n                new_concepts = new_concepts[add:]\n                remaining = new_concepts.size(0)\n\n        if remaining == 0:\n            return\n        prev_active = self.active_mask.sum()\n        self._optimize_memory()\n        if self.active_mask.sum() > prev_active and retry_count < 2:\n            \n            self._write_memory_update(new_concepts, retry_count + 1)\n        else:\n            self._enqueue_to_queue_buffer(new_concepts)\n\n        assert self.memory_size <= self.max_slots\n        assert torch.all(self.active_mask[:self.memory_size])\n\n    @torch.no_grad()\n    def flush_concept_queue(self):\n        if self.query_count > 0:\n            concepts = self.concept_queue[:self.queue_count]\n            self._write_memory_update(concepts)\n            self.queue_count = 0\n            self.queue_ptr = 0\n           \n\n    @torch.no_grad()\n    def _enqueue(self, data):\n        if data.size(0) == 0:\n            return\n\n        capacity = self.concept_queue.size(0)\n        avail = capacity - self.queue_count\n        to_add = data[:avail]\n        if to_add.size(0) == 0:\n            return\n\n        start = self.queue_ptr\n        end = (start + to_add.size(0)) % self.queue_max_size  \n        if end <= capacity:\n\n            self.concept_queue[start:end] = to_add\n        else:\n            split = capacity - start\n            self.concept_queue[start:] = to_add[:split]\n            self.concept_queue[:end % capacity] = to_add[split:]\n\n        self.queue_ptr = end % capacity\n        self.queue_count = min(self.queue_count + to_add.size(0), capacity)\n\n    @torch.no_grad()\n    def _enqueue_to_queue_buffer(self, new_concepts):\n        to_enqueue = new_concepts.size(0)\n        if to_enqueue == 0:\n            return\n\n        capacity = self.queue_max_size.item()\n        current = self.queue_count\n        overflow = max(0, current + to_enqueue - capacity)\n        important =  self.important_net(new_concepts).squeeze(-1)\n        keep_mask = important > 0.65\n        new_concepts = new_concepts[keep_mask]\n        if new_concepts.size(0) == 0:\n            return \n        if overflow >= new_concepts.size(0):\n            self._enqueue(new_concepts[-capacity:])\n            return  \n\n        kept_new = new_concepts[-(to_enqueue - overflow):]\n        self._enqueue(kept_new)\n\n    @torch.no_grad()\n    def _optimize_memory(self , aggressive = False):\n        if aggressive:\n            self._consolidate_important_memories()\n            self._merge_similar_slots()\n            self._prune_memories()\n            self._prune_slots()\n            self.neurogenesis()\n        else:\n            self._consolidate_important_memories()\n            self._prune_memories()\n            self.neurogenesis()\n\n\n        \n\n    def _update_energy_level(self):\n        # key_memory = self.mem_key_proj(self.key_memory)\n        importance = self.important_net(self.key_memory).squeeze()\n        assert torch.all(self.concept_energy >= 0)\n        assert torch.all(self.concept_energy <= 1.01)  \n        new_energy =  (\n            # self.decay_rate * \n            0.3 * self.concept_energy + 0.1 * self.usage + 0.6 * importance *(1-self.concept_energy)\n                    )\n        \n        deactivated  = ~self.active_mask \n\n        valid_deactivate =  deactivated.nonzero().squeeze()\n        valid_deactivate = valid_deactivate[(valid_deactivate>=0)&(valid_deactivate < self.memory_size)]\n        with torch.no_grad():\n            self.concept_energy.data =  torch.clamp(new_energy  , 0, 1)\n\n            if valid_deactivate.numel() >0:\n                self.key_memory.data[valid_deactivate] *=0.01\n                self.value_memory.data[valid_deactivate]*=0.01\n                self.cell_state.data[valid_deactivate]*=0.1\n\n        self.active_concepts =  torch.sum(self.active_mask).clamp(min=0 , max=self.memory_size)\n    \n    def forward(self ,x:torch.Tensor , training:bool = True):\n        if training:\n           x=  self.replay_consolidation(x=x)\n    \n        batch_size  , seq_len , _  = x.shape \n        self.step_count += 1\n        self.query_count += batch_size \n        compressed = self.compression(x.mean(dim=1))\n        query = self.W_cell(compressed)\n        k_active , v_active , c_active =  self._get_active_memory()\n        k  = k_active.unsqueeze(1).expand(-1 , batch_size ,-1)\n        v = v_active.unsqueeze(1).expand(-1, batch_size , -1)\n        assert k.size(1) == batch_size\n        # attn_output , attn_weights = self.attn(\n        #         query.unsqueeze(0), k ,v , need_weights =  True \n        #     )\n\n        # Cosine similarity instead of MHA\n        sims =  F.cosine_similarity(query.unsqueeze(1),\n                                     k_active.unsqueeze(0) , dim=-1)\n        # print(\"   pre‑write mean/sd:\", sims.mean().item(), sims.std().item())\n        top_vals , local_topk = sims.topk(self.top_k , dim=-1)\n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0] \n        topk_idx = active_indices[local_topk]\n        attn_weights = torch.zeros(1, batch_size, k_active.size(0), device=query.device)\n        attn_weights[0].scatter_(1, local_topk, 1.0)\n        # indices = torch.nonzero(self.active_mask, as_tuple=True)[0][topk_idx]\n\n        # self._boost_energy_on_access(indices.view(-1), sims.view(-1, k.size(0)))\n\n        v_exp    = v_active.unsqueeze(0).expand(batch_size, -1, -1)  \n        gathered = torch.gather(\n            v_exp,\n            1,\n            local_topk.unsqueeze(-1).expand(-1, -1, self.compress_dim)\n        )                                                             \n        retrieved = gathered.mean(dim=1)                            \n\n        attn_output = retrieved.unsqueeze(0)                        \n\n         \n\n        max_scores , _  = sims.max(dim = -1)\n        # if self.query_count < self.initial_write_step:\n        #     hit_thershold = 0.79\n        # else:\n        #     hit_thershold=  0.50\n        # hit_thershold = 0.5 + 0.2 * (self.memory_size / self.max_slots)\n        hit_thershold =  0.61\n        # print('hit_threshold', hit_thershold)\n        # query_proj = self.query_proj(query)    \n        # hit_threshold = 0.3 + 0.2 * (self.memory_size / self.max_slots)\n        max_scores = max_scores.squeeze(-1)\n        # print('max scores ',max_scores)\n        hits = (max_scores>hit_thershold).sum()\n        self.hit_count +=  hits\n        # print('hit count', self.hit_count)\n        # # self.novelty_threshold = torch.clamp(\n        #     torch.tensor(0.4 - 0.3 * (self.memory_size / self.max_slots)), \n        #     min=0.1, \n        #     max=0.5\n        # )     \n        #   \n        novel_mask =  max_scores <  hit_thershold\n        \n        \n        self.novel_count+= novel_mask.sum()\n        if novel_mask.any():\n                novel_projection =  query[novel_mask]\n            # sim_scores =  F.cosine_similarity(novel_projection.unsqueeze(1) , k_active.unsqueeze(0) , dim=-1)\n            # similarity_threshold = self.sim_thershold - 0.2 * (self.memory_size / self.max_slots)\n            # is_novel =  sim_scores.max(dim=-1).values< (self.sim_thershold - 0.2 * (self.memory_size/self.max_slots))\n            # write_mask = is_novel\n            # write_mask = sim_scores.max(dim=-1).values < similarity_threshold\n            # if write_mask.any():\n                if self.query_count >  0:\n                    self.flush_concept_queue()\n                # new_concepts =  novel_projection[write_mask]\n                new_concepts = novel_projection\n                self.write_count += new_concepts.size(0)\n                assert new_concepts.size(0) <= self.max_slots - self.memory_size \n                \"Exceeding maximum memory capacity\"\n                self._write_memory_update(new_concepts=new_concepts)\n                no_memory_out =  self.no_memory_embedding.repeat(batch_size , seq_len, 1)\n                return no_memory_out  , self.no_memory_embedding.squeeze(0), torch.tensor([], dtype= torch.long) , None \n        # attn_output = attn_output + torch.randn_like(attn_output) * 0.1\n        \n\n        with torch.no_grad():\n            self.usage *= 0.95\n            self.usage[topk_idx] +=0.1\n            self.usage.clamp(0,1)\n            self.usage.mul_(0.9)\n            self.usage.scatter_add_(0, topk_idx.flatten(), torch.ones_like(topk_idx, dtype=torch.float).flatten())\n            self.usage.clamp_(max=1.0)\n            # self.concept_energy[topk_idx] += 0.15 * max_scores.squeeze()\n            # self.concept_energy.clamp_(max=1.0)\n       \n        keys= self.key_memory[topk_idx]\n        value = self.value_memory[topk_idx]\n        cells = self.cell_state[topk_idx]\n        if training and self.query_count % 31 == 0:\n            self._update_energy_level()\n            self._update_thersholds()\n             \n        gate_input = torch.cat([\n            keys, cells, query.unsqueeze(1).expand(-1, self.top_k , -1)\n        ], dim= -1) \n        update_gates =  self.update_gate(gate_input.view(-1, 3 *self.compress_dim))\n        update_gates = update_gates.view(batch_size, self.top_k)\n        self._update_memory(topk_idx=topk_idx, cells=cells ,projected=query, update_gates=update_gates)\n        \n        # Project the output to the out \n        out =  self.memory_projection(retrieved)\n        out = out.unsqueeze(1).repeat(1, seq_len, 1)\n        self._memory_version +=1 \n        self._update_memory_metadata(topk_idx)\n        return  out , retrieved , topk_idx , attn_weights\n\n\n    @torch.no_grad()\n    def _gradual_influence_increase(self):\n        \"\"\"\n        Gradually increase the influence of newly added memory slots based on their age and access.\n        \"\"\"\n        new_slots_mask  = (self.age <= self.new_slot_maturation_steps) & self.active_mask \n        if not torch.any(new_slots_mask):\n            return  \n        \n        age_normalized = self.age[new_slots_mask] / self.new_slot_maturation_steps\n        usage_normalized =  self.usage[new_slots_mask]\n\n        growth_factor =torch.sigmoid((age_normalized + usage_normalized) * 3 ).unsqueeze(-1)\n\n        self.key_memory[new_slots_mask] = F.normalize(self.key_memory[new_slots_mask] * (1 + growth_factor * 0.5),\n        dim=-1)\n        self.value_memory[new_slots_mask] =  F.normalize(self.value_memory[new_slots_mask]* (1+growth_factor * 0.3) ,dim=-1 )\n\n        energy_boost = torch.clamp(0.1 * growth_factor.squeeze(), max=0.15)\n        self.concept_energy.data[new_slots_mask] = torch.clamp(\n        self.concept_energy[new_slots_mask] + energy_boost,\n        min=0.3,\n        max=0.7\n    )\n\n        self.age.data[new_slots_mask] += 1 \n\n        \n    def _consolidate_important_memories(self):\n        key_memory =  self.mem_key_proj(self.key_memory)\n        importance =  self.important_net(self.key_memory).squeeze()\n        consolidate_mask  = importance > 0.1\n        if consolidate_mask.any():\n          with torch.no_grad():\n            self.key_memory[consolidate_mask] = F.normalize(\n                self.key_memory[consolidate_mask] , dim=-1\n            )\n            mean_value = self.value_memory[consolidate_mask].mean(dim=0)\n           \n            self.value_memory[consolidate_mask] = (\n                    0.9 * self.value_memory[consolidate_mask] +\n                    0.1 * mean_value\n                )\n            self.concept_energy[consolidate_mask] = torch.clamp(self.concept_energy[consolidate_mask] + 0.05, 0, 1)\n            self.concept_energy[~consolidate_mask] *= 0.85\n            self.consalidate_count +=1 \n\n    # @torch.no_grad()\n    # def _prune_memories(self):\n    #     prune_condidate =  ((self.age > self.prune_age_threshold * 0.5 ) & (self.usage < 0.05) & (self.concept_energy < self.energy_threshold))\n    #     if prune_condidate.any():\n    #         self.key_memory.data[prune_condidate] *=  0.1\n    #         self.value_memory.data[prune_condidate]*=0.01\n    #         self.cell_state.data[prune_condidate] *= 0.01\n    #         self.age.data[prune_condidate] = 0 \n    #         self.usage[prune_condidate] = 0 \n    #         self.concept_energy.data[prune_condidate] = 0.1\n    #         self.prune_count +=1 \n    #         self._memory_version += prune_condidate.sum().item()\n\n    def _prune_memories(self):\n        # prune_mask: [max_slots]\n        prune_mask = torch.sigmoid((self.age - 100) / 20) * (1 - self.usage)\n        prune_mask *= torch.sigmoid(-self.concept_energy * 5)\n\n        # no need to unsqueeze for concept_energy\n        with torch.no_grad():\n            self.key_memory.data *= (1 - prune_mask.unsqueeze(-1) * 0.5)   # keeps shape [max_slots, dim]\n            self.concept_energy.data *= (1 - prune_mask * 0.3)             # shape [max_slots]\n\n\n    @torch.no_grad()\n    def _prune_slots(self):\n            mask = self.age > self.prune_age_threshold\n            if mask.any():\n                self.key_memory.data[mask] *= 0.01\n                self.value_memory.data[mask] *= 0.01\n                self.cell_state.data[mask] *= 0.01\n                self.concept_energy.data[mask] = 0\n                self.usage[mask]= 0 \n                self.age.data[mask] = 0 \n                self.prune_count +=1 \n                self.active_slots[mask] = False \n    \n\n\n    def replay_consolidation(self, x: torch.Tensor):\n\n        active_indices = torch.nonzero(self.active_mask, as_tuple=True)[0] \n        active_key , active_value, _ = self._get_active_memory()\n        if  self.training and random.random() < 0.2: \n            high_energy_mask = self.concept_energy[active_indices] > 0.8\n            if high_energy_mask.sum() == 0:\n                return x \n            if high_energy_mask.sum() > 0:\n                replay_keys = active_key[high_energy_mask]\n                replay_values = active_value[high_energy_mask]\n                \n                replay_input = self.decompression(replay_values.mean(dim=0, keepdim=True))\n                B, T , D =  x.shape\n                self.replay_count+= 1\n                return replay_input.unsqueeze(1).expand(B,T,D)\n        return x\n\n    def get_reusable_slots(self ,num_needed:int):\n       \n\n        age_score =  1-torch.sigmoid(self.age / 100) # old age \n        energy_score = (1-  self.concept_energy ) *2 \n        usage_score = 1 - self.usage \n        reuse_scores = (0.4 * energy_score  + 0.3 * age_score + 0.3 * usage_score \n                       )\n\n        mask =(self.concept_energy  < self.energy_threshold) & (self.age< 100)\n        reuse_scores[~mask]= -float('inf')\n\n        topk_scores  , candidates = torch.topk(reuse_scores, min(num_needed, self.memory_size))\n        return candidates \n\n            \n    def _reinitialize_slot(self, idx):\n        \"\"\"Reset a slot to initial state\"\"\"\n        with torch.no_grad():\n            scale = 0.1 + 0.05 * torch.rand(1, device=idx.device)\n            self.key_memory[idx] = torch.randn_like(self.key_memory[idx]) * scale\n            self.value_memory[idx] = torch.randn_like(self.value_memory[idx]) * scale\n            self.cell_state[idx] =  0.2 * self.cell_state.data[idx].mean(dim=0)\n            \n            # Reset metadata\n            # neighbor_energy = self.concept_energy[idx±5].mean()   \n            self.concept_energy[idx] = 0.3 + 0.2 * torch.rand_like(self.concept_energy[idx])\n            # self.usage[idx] =  0.1 * torch.rand_like(self.usage[idx])\n            self.usage[idx] = 0.05\n            self.age[idx] = 0\n            self.memory_age[idx] = 0\n            self.access_count[idx] = 0\n            self.active_mask[idx] = True \n\n\n\n\n\n    def _consalidate_new_slots(self):\n        new_slot_indices =  torch.arange(self.memory_size - 10 , self.memory_size )\n        new_slot_energy = self.concept_energy[new_slot_indices]\n        with torch.no_grad():\n            self.concept_energy[new_slot_indices] = torch.clamp(\n                new_slot_energy + 0.1 * self.age[new_slot_indices] , 0 ,1\n            )\n            self.key_memory[new_slot_indices] *=  0.1\n            self.value_memory[new_slot_indices] *= 0.1\n            self.age[new_slot_indices] +=1\n            self.access_count[new_slot_indices] +=1 \n\n    def _update_memory_metadata(self, used_indices):\n       \n        self.access_count[used_indices] += 1\n        \n        self.memory_age += 1\n        self.memory_age[used_indices] = 0\n        with torch.no_grad():\n            self.concept_energy[used_indices] += 0.1\n            # self.concept_energy= torch.clamp(self.concept_energy * 0.95, 0, 1)\n            self.concept_energy.mul_(0.95).clamp_(0,1)\n\n            # self.age[used_indices] -= 5 \n    @torch.no_grad()\n    def neurogenesis(self, required_slots:int= 10 , return_index = False):\n        device = self.key_memory.device\n        if self.max_slots > self.memory_size:\n\n            usage_rate =  (self.usage > 0.1).float().mean()\n            if usage_rate > self.neurogenesis_threshold:\n                reusable =  self.get_reusable_slots()\n                num_reuse =  min(reusable.numel() , required_slots)\n                reused_indices = reusable[:num_reuse]\n                if num_reuse > 0:\n                    with torch.no_grad():\n                        device =  self.key_memory.device \n                        self._reinitialize_slot(idx=reused_indices)\n                        \n                new_slots =  min(max(0 ,    required_slots -  num_reuse ), self.max_slots- self.memory_size)\n                if new_slots > 0:\n                    start_idx =self.memory_size\n                    end_idx =  start_idx  + new_slots \n                    new_indices  = torch.arange(start_idx , end_idx, device = device )\n                  \n                   \n                    self.key_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.value_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.cell_state.data[new_indices] = 0\n                    self.concept_energy.data[new_indices] = 0.5\n                    self.usage.data[new_indices] = 0.0\n                    self.age.data[new_indices] = 0.0\n                    self.access_count.data[new_indices] = 0.0\n                    self.active_mask.data[new_indices] = True\n                    self.neuroslot_count +=1 \n                    self._gradual_influence_increase()\n                    self._memory_version += new_slots\n                    # self.new_slot_mask[new_indices] =  True \n\n\n                \n                    \n    \n       \n                if return_index:\n                    return torch.cat([reused_indices, new_indices]) if new_indices.numel() > 0 else reused_indices\n            \n        elif return_index:\n            return  torch.empty(0, dtype=torch.long, device=self.key_memory.device) \n\n    def emergency_recovery(self):\n        # Reset unstable memories\n        unstable = self.concept_energy < 0.2\n        self._reinitialize_slot(unstable)\n        \n        self._optimize_memory(aggressive=True)\n    def _protect_critical_memories(self):\n            # Protect top 10% of important memories\n            importance = self.important_net(self.key_memory).squeeze()\n            topk = importance.topk(int(self.max_slots * 0.1)).indices\n            self.concept_energy[topk] = 1.0\n            self.age[topk] -= 10\n    @torch.no_grad()\n    def _merge_similar_slots(self, top_k: int = 8):\n        device = self.key_memory.device\n        active_idx = torch.nonzero(self.active_mask, as_tuple=True)[0]\n        N = active_idx.size(0)\n        if N < 2:\n            return\n\n        # 1. Normalized vectors\n        keys = F.normalize(self.key_memory[active_idx], dim=-1)\n        values = F.normalize(self.value_memory[active_idx], dim=-1)\n        D = keys.size(-1)\n\n        # 2. Similarity search\n        K = min(top_k, N-1)\n        sims, nbrs = torch.topk(keys @ keys.T, k=K+1, dim=-1)\n        sims, nbrs = sims[:, 1:], nbrs[:, 1:]  # Remove self\n\n        # 3. Dynamic threshold\n        # pressure = torch.tensor(N / self.max_slots, device=device)\n        # threshold = (0.9 - 0.4 * pressure).clamp(0.65, 0.9)\n        # energy_factor = torch.sigmoid((self.concept_energy.mean() - 0.5) * 5)\n        confidence_factor = sims.mean()\n        utilization_factor = (self.concept_energy < 0.9).float().mean()\n        \n        adaptive_threshold = 0.4 + 0.2 * self.concept_energy.mean() + 0.2 * confidence_factor + 0.2 * utilization_factor\n\n        # mask = sims > threshold\n        mask = sims >  adaptive_threshold \n\n        # 4. Graph construction\n        row = torch.arange(N, device=device).unsqueeze(1).expand(-1, K)[mask]\n        col = nbrs[mask]\n        edges = torch.stack([\n            torch.cat([row, col]),\n            torch.cat([col, row])\n        ])\n\n        # 5. Label propagation\n        labels = torch.arange(N, device=device)\n        for _ in range(3):\n            neighbor_labels = labels[edges[1]]\n            updates = torch.minimum(labels[edges[0]], neighbor_labels)\n            labels.scatter_reduce_(0, edges[0], updates, reduce='amin')  # Fixed reduction\n\n        # 6. Cluster analysis\n        uniq, inv, counts = torch.unique(labels, return_inverse=True, return_counts=True)\n        cluster_mask = counts >= 2\n        big_clusters = uniq[cluster_mask]\n        big_counts = counts[cluster_mask]\n        num_clust = big_clusters.size(0)\n        if num_clust == 0:\n            return\n\n        # 7. Cluster mapping\n        cluster_id_map = torch.zeros(uniq.max()+1, dtype=torch.long, device=device)\n        cluster_id_map[big_clusters] = torch.arange(num_clust, device=device)\n        member_mask = torch.isin(inv, big_clusters)\n        global_idx = active_idx[member_mask]\n        cluster_ids = cluster_id_map[inv[member_mask]]  # Proper mapping\n        # 8. Energy aggregation\n        energy = self.concept_energy[global_idx]\n        weights = (energy / big_counts[cluster_ids].float()).unsqueeze(-1)\n        expanded_ids = cluster_ids.unsqueeze(-1).expand(-1, D)\n        # 8.1 Weighted sum of the seleceted slots datat \n        new_keys = torch.zeros((num_clust, D), device=device)\n        new_vals = torch.zeros_like(new_keys)\n        new_keys.scatter_add_(0, expanded_ids, keys[member_mask] * weights)\n        new_vals.scatter_add_(0, expanded_ids, values[member_mask] * weights)\n\n        # 9. Representative selection\n        cluster_ages = self.age[global_idx]\n        min_ages = torch.zeros(num_clust, device=device)\n        min_ages.scatter_reduce_(0, cluster_ids, cluster_ages, reduce='amin', include_self=False)\n        \n        # Find first occurrence of min age\n        _, sorted_idx = torch.sort(cluster_ids)\n        cluster_ids_sorted = cluster_ids[sorted_idx]\n        age_mask = (cluster_ages[sorted_idx] == min_ages[cluster_ids_sorted])\n        _, first_occurrence = torch.unique_consecutive(cluster_ids_sorted, return_inverse=True)\n        rep_mask = age_mask & (first_occurrence == 0)\n        rep_cluster_ids = cluster_ids_sorted[rep_mask]\n        representatives = global_idx[sorted_idx][rep_mask]\n    \n        # 10. Memory updates\n        self.key_memory[representatives] = F.normalize(new_keys[cluster_ids_sorted[rep_mask]], dim=-1)\n        self.value_memory[representatives] = F.normalize(new_vals[cluster_ids_sorted[rep_mask]], dim=-1)\n        \n        clust_energy = torch.bincount(cluster_ids, weights=energy, minlength=num_clust)\n        # self.concept_energy[representatives] = clust_energy[rep_cluster_ids].clamp(min=1e-5, max=1.0)\n        self.concept_energy[representatives] = torch.clamp(\n            clust_energy[rep_cluster_ids] * 1.2,  \n            min=0.7, \n            max=1.0\n        )\n     \n        self.merge_count+= 1 \n        self._memory_version += num_clust\n        # 11. Usage update\n        per_cluster_usage = torch.bincount(\n            cluster_ids,\n            weights=self.usage[global_idx],\n            minlength=num_clust\n        ).float() / big_counts.float() \n        # self.usage[representatives] = per_cluster_usage[rep_cluster_ids]\n        self.usage[representatives] = torch.clamp(\n            per_cluster_usage[rep_cluster_ids] * 1.5,\n            min=0.3,\n            max=1.0\n        )\n\n        # 12. Deactivation\n        active_mask_modified = torch.zeros_like(self.active_mask)\n        active_mask_modified[representatives] = True\n        deactivate_idx = member_mask & ~active_mask_modified[active_idx]\n        \n        if deactivate_idx.any():\n            to_deactivate = active_idx[deactivate_idx]\n            self.concept_energy[to_deactivate] *= 0.1\n            self.key_memory[to_deactivate] *= 0.1\n            self.value_memory[to_deactivate] *= 0.1\n            self.usage[to_deactivate] *= 0.25\n\n        self._consolidate_important_memories()\n        self._update_memory_metadata(representatives)\n            \n    @torch.no_grad()\n    def _update_thersholds(self , momentum:float=0.9 ):\n        hit_rate =  float(self.hit_count / max(self.query_count, 1))\n        write_rate =  float(self.write_count  / max(self.query_count , 1))\n        novely_rate =  float(self.novel_count / max(self.query_count  , 1))\n\n        util =  float(self.active_capacity)\n\n        new_nov =   (0.2 * (1-util) + 0.1 * novely_rate)\n        self.novelty_threshold =    momentum * self.novelty_threshold + (1-momentum) * new_nov\n\n        new_enger_thr = 0.3 + 0.3 *(1-hit_rate)\n        self.energy_threshold.data  = momentum * self.energy_threshold + (1-momentum) * new_enger_thr \n        \n        new_consal = 50.0 +50.0 * write_rate \n        self.consolidation_threshold.data.mul_(momentum).add_(new_consal * (1-momentum))\n\n        new_decay = 0.995 + 0.003 * (1-util)\n        self.decay_rate.data.mul_(momentum).add_(new_decay *(1-momentum))\n\n\n        new_prune_age = 100 * (1- util ) + 20 * util \n        self.prune_age_threshold.fill_(momentum * self.prune_age_threshold+(1-momentum) * new_prune_age)\n\n        new_neuro = 0.8 + 0.1 * write_rate - 0.1 * util\n        self.neurogenesis_threshold.fill_(momentum * self.neurogenesis_threshold + (1-momentum) * new_neuro)\n\n        new_mat = 50 + 50 * write_rate\n        self.new_slot_maturation_steps.fill_(momentum * self.new_slot_maturation_steps + (1-momentum) * new_mat)\n\n        new_scale = 0.05 + 0.2 * (1 - hit_rate)\n        self.synaptic_scale.data.mul_(momentum).add_(new_scale * (1-momentum))\n\n        new_sp = 0.5 + 0.3 * util\n        self.sparsity.data.mul_(momentum).add_(new_sp * (1-momentum))\n\n        new_sim = 0.5 - 0.2 * novely_rate\n        self.sim_thershold.data.mul_(momentum).add_(new_sim * (1-momentum))\n\n\n    \n\n        \n    def get_memory_metrics(self):\n\n        \"Return memory health and retivel param details\"\n        active_mask  =  self.concept_energy > self.energy_threshold\n        energy  =  self.concept_energy \n        usage =  self.usage \n        access =  self.access_count\n\n        age_hist   = torch.histc(self.memory_age.float(), bins=10, min=0, max=float(self.memory_age.max()))\n        usage_hist = torch.histc(usage, bins=10, min=0, max=1.0)\n        access_hist= torch.histc(access.float(), bins=10, min=0, max=float(access.max()))\n        active_concepts = self.active_mask &(self.concept_energy > 0.70)\n\n        return  {\n\n            #________Memory Health _______________________________\n            'memory_size': self.memory_size  , \n            'active_concepts':self.active_mask.sum().item(),\n            'active_concepts_with_high_energy':active_concepts.sum().item(),\n            'utilization':self.utilization , \n            'energy_mean':energy.mean().item(), \n            'energy_std':energy.std().item(), \n            'age_histogram': age_hist, \n            'usage_histogram':usage_hist, \n            'access_histogram':access_hist, \n            # 'merge_rate':(energy < 0.3).sum().item() / self.memory_size ,\n            'merge_rate':self.merge_count.item() / max(1 , self.step_count.item()),\n            # \"prune_rate\":((self.age > self.prune_age_threshold) & (usage < 0.01)).float().mean().item(),            \n            'prune_rate':self.prune_count.item() / max(1 , self.step_count.item()),\n            'neuro_rate':(self.memory_age < 10).float().mean().item(), \n            \"reuse_efficiency\":      access[energy > 0.5].float().mean().item(),\n\n             # —— retrieval/write stats ———————————————————————\n            \"steps\":                 self.step_count.item(),\n            \"queries\":               self.query_count.item(),\n            \"novelty_rate\":          self.novel_count.item() / max(1, self.query_count.item()),\n            \"write_rate\":            self.write_count.item() / max(1, self.query_count.item()),\n            \"hit_rate\":              self.hit_count.item() / max(1, self.query_count.item()),\n            'merge_count':self.merge_count.item() , \n            'neuroslot_count':self.neuroslot_count.item(), \n            'prune_count':self.prune_count.item(), \n            'consalidate_count':self.consalidate_count.item(),\n            'hit_count':self.hit_count.item() , \n            'write_count':self.write_count.item(),\n            'memory_version':self._memory_version.item(),\n            'access_count':self.access_count[self.active_mask].tolist(), \n            'replay_count':self.replay_count.item(),\n            'access_count_sum': self.access_count[self.active_mask].sum().item(),\n\n                'access_count_mean': self.access_count[self.active_mask].mean().item(),\n            'update_count':self.update_count.item()\n            \n        }\n\n\n        \n    def model_save(self , path):\n            torch.save({\n\n                'key_memory':self.key_memory.data.cpu(), \n                'value_memory':self.value_memory.data.cpu() , \n                'cell_state':self.cell_state.data.cpu(), \n                'active_mask':self.active_mask.cpu(),\n                'age':self.age.data.cpu() , \n                'usage':self.usage.data.cpu(), \n                'access_count':self.access_count.data.cpu(), \n                'memory_version':self._memory_version.data.cpu(), \n                'memory_age':self.memory_age.data.cpu(), \n                'concept_queue':self.concept_queue.data.cpu(), \n                'queue_ptr':self.queue_ptr , \n                'queue_count':self.queue_count , \n                'query_count':self.query_count , \n                'step_count':self.step_count , \n                'novel_count':self.novel_count , \n                'write_count':self.write_count , \n                'hit_count':self.hit_count , \n                'merge_count':self.merge_count , \n                'neuroslot_count':self.neuroslot_count , \n                'prune_count':self.prune_count , \n                'consalidate_count':self.consalidate_count ,\n                'memory_size':self.memory_size,\n                'replay_count':self.replay_count\n\n             } , path)\n            \n    def model_load(self, path, map_location=None):\n        state = torch.load(path, map_location=map_location)\n        self.key_memory.data.copy_(state['key_memory'])\n        self.value_memory.data.copy_(state['value_memory'])\n        self.cell_state.data.copy_(state['cell_state'])\n        self.active_mask.data.copy_(state['active_mask'])\n        self.age.data.copy_(state['age'])\n        self.usage.data.copy_(state['usage'])\n        self.access_count.data.copy_(state['access_count'])\n        self.memory_age.data.copy_(state['memory_age'])\n        self.concept_queue.data.copy_(state['concept_queue'])\n        self.queue_ptr = state.get('queue_ptr', 0)\n        self.queue_count = state.get('queue_count', 0)\n        self._memory_version.data.copy_(\n            state.get('memory_version', torch.tensor(0))\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.345951Z","iopub.execute_input":"2025-06-06T06:07:09.346183Z","iopub.status.idle":"2025-06-06T06:07:09.443436Z","shell.execute_reply.started":"2025-06-06T06:07:09.346164Z","shell.execute_reply":"2025-06-06T06:07:09.442664Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Transformer Block","metadata":{}},{"cell_type":"code","source":"\n\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention_V2(\n        d_in=cfg[\"emb_dim\"],\n        d_out=cfg[\"emb_dim\"],\n        context_length=cfg[\"context_length\"],\n        num_heads=cfg[\"n_heads\"],\n        dropout=cfg[\"drop_rate\"],\n        qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n    #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut # Add the original input back\n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut #C\n        return x\n\n\n\nclass TransformerBlock_v2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention =  MultiQueryAttentionBlock(d_model=cfg['emb_dim'], h=cfg['n_heads'] , dropout=cfg['drop_rate'], seq_len=  cfg['context_length'] ,qkv_bias=cfg['qkv_bias'])\n\n        self.feed_forward = FeedForward(cfg)\n\n        self.layernorm1 =  LayerNorm(cfg['emb_dim'])\n    \n        self.layernorm2 =  LayerNorm(cfg['emb_dim'])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x , mask= None):\n\n        attention_output =  self.attention(self.layernorm1(x) , mask =  mask)\n\n        ff_output =  self.feed_forward(self.layernorm2(x))\n\n        return x + self.drop_out(ff_output) + self.drop_out(attention_output)\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.memory = MemorySystem(cfg=cfg)\n        self.feed_forward = FeedForward(cfg=cfg)\n        \n        self.norm1 = LayerNorm(cfg['emb_dim'])\n        self.norm2 = LayerNorm(cfg['emb_dim'])\n        self.norm3 = LayerNorm(cfg['emb_dim'])\n        \n        # Memory gate\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] * 2, cfg['emb_dim']),\n            nn.Sigmoid()\n        )\n        \n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        attn_out = self.attention(self.norm1(x), mask=mask)\n        x = x + self.dropout(attn_out)\n        \n        norm_x = self.norm2(x)\n        epic_out , semantic_out , memory_out = self.memory(norm_x)\n        \n        gate_input = torch.cat([norm_x, memory_out], dim=-1)\n        memory_gate = self.memory_gate(gate_input)\n        x = x + memory_gate * memory_out\n        \n        ff_out = self.feed_forward(self.norm3(x))\n        x = x + self.dropout(ff_out)\n        \n        return x\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg, shared_memory=None):\n        super().__init__()\n        # Core components\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.ffn = FeedForward(cfg=cfg)\n        \n        # Memory system (shared across blocks)\n        self.memory = shared_memory or MemorySystem(cfg=cfg)\n        \n        # Normalization layers\n        self.pre_ln_attn = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_mem = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_ffn = RMSNorm(cfg['emb_dim'])\n        \n        # Adaptive memory gating\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'], 1),\n            nn.Sigmoid()\n        )\n        \n        # Memory residual weights\n        self.mem_alpha = nn.Parameter(torch.tensor(0.5))\n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        # Attention phase\n        resid = x\n        x = self.pre_ln_attn(x)\n        x = resid + self.dropout(self.attention(x, mask=mask))\n        \n        # Memory phase\n        resid_mem = x\n        x_mem = self.pre_ln_mem(x)\n        # print('x shape ', x.shape)\n        _, _, memory_out = self.memory(x_mem)\n        \n        # Adaptive gating\n        gate = self.memory_gate(x_mem)\n        x = resid_mem + self.mem_alpha * gate * memory_out\n        \n        # FFN phase\n        resid_ffn = x\n        x = self.pre_ln_ffn(x)\n        x = resid_ffn + self.dropout(self.ffn(x))\n        \n        return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.444184Z","iopub.execute_input":"2025-06-06T06:07:09.444429Z","iopub.status.idle":"2025-06-06T06:07:09.460234Z","shell.execute_reply.started":"2025-06-06T06:07:09.444407Z","shell.execute_reply":"2025-06-06T06:07:09.459375Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# GPTQModel","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int, embdding_layer: nn.Embedding):\n        super().__init__()\n        self.weight = embdding_layer.weight  # share weights with input embedding\n        self.bias = nn.Parameter(torch.zeros(vocab_size))  # learnable bias\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\n\n\n\nclass GPTMQModel2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim = cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel1(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks =  nn.ModuleList([\n            TransformerBlockWithMemory(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n        \n        # Shared memory system across layers\n        self.shared_memory = MemorySystem(cfg=cfg)\n        \n        # Transformer blocks with shared memory\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlockWithMemory(\n                cfg=cfg,\n                shared_memory=self.shared_memory if cfg['share_memory'] else None\n            ) for _ in range(cfg['n_layers'])\n        ])\n        \n        # Final projections\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.projection = ProjectionLayer(\n            cfg['emb_dim'], \n            cfg['vocab_size'], \n            self.embedding.embeddings\n        )\n        self.memory_retention_alpha = nn.Parameter(torch.tensor(0.9))\n\n        # Memory loss coefficient\n        self.mem_loss_coef = cfg.get('mem_loss_coef', 0.3)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n        \n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)\n            x = self.memory_retention_alpha * x + (1 - self.memory_retention_alpha) * x.detach()\n            \n        x = self.final_norm(x)\n        logits = self.projection(x)\n        \n        return logits\n    \n    def get_memory_loss(self):\n        \"\"\"Get combined memory regularization loss\"\"\"\n        return self.mem_loss_coef * self.shared_memory.memory_loss()\n    \n    def transformer_parameters(self):\n        return [p for n, p in self.named_parameters() if 'transformer_blocks' in n and p.requires_grad]\n    \n    def memory_parameters(self):\n        return [p for n, p in self.named_parameters() if 'memory_modules' in n and p.requires_grad]\n    \n    def embedding_parameters(self):\n        return [p for n, p in self.named_parameters() if 'embedding' in n and p.requires_grad]\n    \n    def norm_parameters(self):\n        return [p for n, p in self.named_parameters() if 'normalization' in n and p.requires_grad]\n    \n    def output_parameters(self):\n        return [p for n, p in self.named_parameters() if 'output_projection' in n and p.requires_grad]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.461245Z","iopub.execute_input":"2025-06-06T06:07:09.461538Z","iopub.status.idle":"2025-06-06T06:07:09.481435Z","shell.execute_reply.started":"2025-06-06T06:07:09.461496Z","shell.execute_reply":"2025-06-06T06:07:09.480599Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\nclass GPTMemoryEnhanced(nn.Module):\n    def __init__(self,cfg ):\n        super().__init__()\n        self.embedding =  InputEmbedding(cfg['vocab_size'] , cfg['emb_dim'])\n        self.memory_proj = nn.Linear(cfg['emb_dim'], cfg['memory_dim'])\n        self.memory_expander = nn.Linear(cfg['memory_dim'], cfg['emb_dim'] )\n        self.transformer_block = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n        self.dropout =  nn.Dropout(cfg['drop_rate'])\n        self.memory =  EfiBioSemanticMemory_V2(input_dim=cfg['memory_dim'] ,semantic_memory_dim=cfg['memory_dim'],num_heads=2)\n\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.fusion_gate = nn.Sequential(\n                nn.Linear(2*cfg['emb_dim'], cfg['emb_dim']),\n                nn.Sigmoid()\n            )\n        \n\n        self.projection =  ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'] , self.embedding.embeddings)\n\n    def forward(self,input_tokens:torch.Tensor , mask =None):\n        x =  self.embedding(input_tokens)\n        x_emb = x \n        for block in self.transformer_block:\n            x  = block(x, mask = mask)\n        memory_query = self.memory_proj(x) \n        mem_out, retrieved, topk_idx, attn_w = self.memory(memory_query)\n        # memory_out, _ ,_ =  self.memory(x.las_hidden_state.mean(1))\n        mem_out = self.memory_expander(mem_out)\n        gate = self.fusion_gate(torch.cat([x, mem_out], -1))\n        fused = gate * x + (1 - gate) * mem_out\n        fused =  self.final_norm(fused)\n\n        logits =  self.projection(fused)\n        # return logits, {\n        #     \"memory_topk\": topk_idx, \n        #     \"memory_attn\": attn_w,\n        #     \"retrieved\":  retrieved\n        # }\n        return  logits ,x_emb ,  mem_out \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.482189Z","iopub.execute_input":"2025-06-06T06:07:09.482391Z","iopub.status.idle":"2025-06-06T06:07:09.499954Z","shell.execute_reply.started":"2025-06-06T06:07:09.482373Z","shell.execute_reply":"2025-06-06T06:07:09.499248Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"\n\n# Memory Reconstruction Loss\n# Ensures stored information preserves input patterns\ndef reconstruction_loss(inputs, memory_output):\n    return F.mse_loss(memory_output, inputs)\n\n\n\n# Task-Specific Loss\n# Drives memory to store task-relevant information\ndef task_loss(predictions, targets):\n    return F.cross_entropy(predictions, targets)  # For classification\n\n# Memory Sparsity Loss\n# Encourages efficient slot usage\ndef sparsity_loss(concept_energy):\n    return torch.mean(concept_energy**2)  # L2 penalty on energy levels\n\n# Memory Diversity Loss\n# Prevents slot redundancy\ndef diversity_loss(key_memory):\n    normalized_keys = F.normalize(key_memory, dim=1)\n    similarity = torch.mm(normalized_keys, normalized_keys.T)\n    return torch.mean(similarity**2) - 1/torch.numel(similarity)\n\n\n\n# Energy Maintenance Loss\n# Maintains healthy energy distribution\ndef energy_loss(concept_energy):\n    energy_mean = torch.mean(concept_energy)\n    return F.mse_loss(energy_mean, torch.tensor(0.5,device = concept_energy.device))\n\n\n\n# Pruning Incentive Loss\n# Encourages proper slot turnover\ndef pruning_loss(age, usage):\n    old_unused = (age > 100) & (usage < 0.01)\n    return torch.mean(old_unused.float())\n\n\n\n# Anti-Collapse Loss\n# Prevents memory dependency on few slots\ndef anti_collapse_loss(usage_counts):\n    return -torch.sum(usage_counts * torch.log(usage_counts + 1e-7))\n\n\n\ndef novelty_loss(new_slots, existing_memory):\n    sim = F.cosine_similarity(new_slots.unsqueeze(1), \n                            existing_memory.unsqueeze(0), dim=-1)\n    return torch.mean(sim)\n\n\n\n\ndef total_loss(inputs, outputs, targets, memory):\n    # Base losses\n    rec_loss = reconstruction_loss(inputs, outputs)\n    # t_loss = task_loss(outputs, targets)\n    \n    # Memory regularization\n    sp_loss = sparsity_loss(memory.concept_energy)\n    div_loss = diversity_loss(memory.key_memory)\n    en_loss = energy_loss(memory.concept_energy)\n    \n    # Stability terms\n    prun_loss = pruning_loss(memory.age, memory.usage)\n    anti_loss = anti_collapse_loss(F.softmax(memory.access_count, dim=0))\n    \n    # Weighted combination\n    return (1.0 * rec_loss + \n            # 0.5 * t_loss + \n            0.3 * sp_loss + \n            0.2 * div_loss +\n            0.1 * en_loss +\n            0.05 * prun_loss +\n            0.02 * anti_loss)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.500714Z","iopub.execute_input":"2025-06-06T06:07:09.500901Z","iopub.status.idle":"2025-06-06T06:07:09.514995Z","shell.execute_reply.started":"2025-06-06T06:07:09.500885Z","shell.execute_reply":"2025-06-06T06:07:09.514113Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n\n\n\ndef cal_loss_batch(input_batch , target_batch , model:torch.nn.Module , device:torch.device ):\n    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n    util_loss = -torch.log(model.memory.utilization + 1e-8)\n    \n    return loss + 0.1 * util_loss\ndef cal_loss_batch(input_batch , target_batch ,model:nn.Module, device:torch.device , mem_cof:float= 0.1):\n    input_batch  , target_batch =  input_batch.to(device) , target_batch.to(device)\n    logits ,x_emb, mem_output  =  model(input_batch)\n    B,T,V = logits.shape \n    gpt_loss = F.cross_entropy(\n            logits.view(B * T, V),\n            target_batch.view(B * T),\n            ignore_index=-100,                       # if you pad with -100\n        )   \n    # utilization_loss = -torch.log(model.memory.utilization + 1e-8)\n    memory_loss =  total_loss(inputs=x_emb , memory= model.memory , outputs=mem_output , targets= target_batch.float())\n    return gpt_loss +mem_cof * memory_loss \n\n\ndef calc_loss_loader(data_loader , model , device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:09.515731Z","iopub.execute_input":"2025-06-06T06:07:09.515928Z","iopub.status.idle":"2025-06-06T06:07:09.530038Z","shell.execute_reply.started":"2025-06-06T06:07:09.515910Z","shell.execute_reply":"2025-06-06T06:07:09.529392Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Text Generation Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\n\n\n\ndef text_to_token_ids(text,  tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n    \ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n    return encoded  # Already 2D [1, seq_len]\n\n\n\ndef token_ids_to_text(tokens , tokenizer):\n    flat  = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decoded = tokenizer.decode(flat.tolist(), skip_special_tokens=True)\n    return decoded\n\n    \ndef generate_and_sample(model  , idx , context_size ,max_new_tokens ):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits  , _ , _ = model(idx_cond)\n        logits  = logits[:, -1  , :]\n        probs  = torch.softmax(logits  , dim=-1)\n        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx \n\n#\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]  # shape: [1, current_seq_len]\n\n        # Create causal mask dynamically\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)  # [1, seq_len, seq_len]\n\n        with torch.no_grad():\n            logits , _ , _= model(idx_cond, mask=causal_mask)  # <--- pass mask here\n\n        logits = logits[:, -1, :]  # only take the last token logits\n\n        # Apply top-k sampling if needed\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        # Temperature sampling\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n        \n\n    return idx\ndef real_time_generation(model, initial_input, context_size, temperature, top_k=None, device=\"cpu\"):\n    # Tokenize the initial input and prepare the model context\n    idx = torch.tensor(initial_input).unsqueeze(0).to(device)  # Assuming initial_input is tokenized\n    \n    print(\"Starting real-time generation...\")\n    \n    # Start generating tokens in real-time\n    for new_token in generate(model, idx, max_new_tokens=50, context_size=context_size, temperature=temperature, top_k=top_k, device=device):\n        print(f\"Generated token: {new_token.item()}\")  # Or decode it back to a word\n        \n        # You can check for user input here and update idx with the new input\n        # For instance, wait for the user to input a prompt to append to the context\n        user_input = input(\"Enter new input (or press enter to continue generation): \")\n        \n        if user_input:\n            # Tokenize the new user input and append it to the context\n            user_input_tokens = torch.tensor(tokenize(user_input)).unsqueeze(0).to(device)\n            idx = torch.cat((idx, user_input_tokens), dim=1)  # Append the new tokens to the context\n        else:\n            # Continue generating if no new user input\n            continue\n\n# Function to tokenize input (adjust depending on your tokenizer)\ndef tokenize(text):\n    # Assuming you have a tokenizer function available\n    return [ord(c) for c in text]  # Dummy example: ord() converts char to token id\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:12.928395Z","iopub.execute_input":"2025-06-06T06:07:12.928704Z","iopub.status.idle":"2025-06-06T06:07:12.940441Z","shell.execute_reply.started":"2025-06-06T06:07:12.928679Z","shell.execute_reply":"2025-06-06T06:07:12.939612Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Dataset and DataLoader ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef generate_prompt(sample):\n    # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n    \n\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer =  tokenizer\n\n        all_tokens = []\n        allowed = {'<|endoftext|>'}\n        for sample in data:\n            prompt = generate_prompt(sample)\n            # tokens = tokenizer.encode(prompt , allowed_special=allowed)\n            tokens =  tokenizer.encode(prompt)\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n    return inputs, targets\n\ndef create_dataloader_v1(data,tokenizer , batch_size=4,\n    max_length=256, stride=128, shuffle=True, drop_last=True ):\n    tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n    dataset = Dataset_V1(data, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=collate_fn)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:14.731771Z","iopub.execute_input":"2025-06-06T06:07:14.732077Z","iopub.status.idle":"2025-06-06T06:07:14.773573Z","shell.execute_reply.started":"2025-06-06T06:07:14.732053Z","shell.execute_reply":"2025-06-06T06:07:14.772599Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef generate_prompt(sample):\n    return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer = tokenizer\n\n        all_tokens = []\n        for sample in data:\n            prompt = generate_prompt(sample)\n            tokens = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=teacher_tokenizer.pad_token_id)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)\n    return inputs, targets\n\ndef create_dataloader_v1(data, tokenizer, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n    dataset = Dataset_V1(data, tokenizer, max_length, stride)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n                            drop_last=drop_last, collate_fn=collate_fn)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:14.966569Z","iopub.execute_input":"2025-06-06T06:07:14.966847Z","iopub.status.idle":"2025-06-06T06:07:14.974256Z","shell.execute_reply.started":"2025-06-06T06:07:14.966824Z","shell.execute_reply":"2025-06-06T06:07:14.973494Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Dataset And DataLoader for Psycology Dataset ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport tiktoken\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass Dataset_v2(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n        self.input_ids = []\n\n        all_tokens = []\n        for sample in data:\n            tokens = tokenizer.encode(sample)  \n            all_tokens.extend(tokens)\n\n        # Split the tokens into chunks of size max_length with stride\n        for i in range(0, len(all_tokens) - self.max_length, self.stride):\n            input_chunk = all_tokens[i:i + self.max_length]\n            target_chunk = all_tokens[i + 1:i + self.max_length + 1]\n            self.input_ids.append((torch.tensor(input_chunk), torch.tensor(target_chunk)))\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, index):\n        return self.input_ids[index]\n\ndef collect_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0) \n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  \n    return inputs, targets\n\ndef create_dataloader_v2(data, batch_size=4, max_length=1024, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")  \n    dataset = Dataset_v2(data, tokenizer, max_length, stride) \n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=collect_fn)\n    return dataloader\n\ndef load_txt_file(filepath):\n    with open(filepath, 'r') as f:\n        text = f.read()\n    return text\n\ndef split_into_chunks(text, chunk_size=1024, overlap=200):\n\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        chunks.append(chunk)\n\n    return chunks\n\n\n\nfile =  '/kaggle/input/datasetcleaned/cleaned_books.txt'\nload_text =  load_txt_file(file)\nchunk = split_into_chunks(load_text)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-06T06:07:15.371562Z","iopub.execute_input":"2025-06-06T06:07:15.371839Z","iopub.status.idle":"2025-06-06T06:07:17.696953Z","shell.execute_reply.started":"2025-06-06T06:07:15.371819Z","shell.execute_reply":"2025-06-06T06:07:17.696076Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Train Script ","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"* ","metadata":{}},{"cell_type":"code","source":"\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\ndef evaluate_model(model, train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,   # Increase generation length if needed\n            context_size=126,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n\n        # Trim everything before the generation\n        generated_only = decoded_text[len(start_context):].strip()\n\n        # Stop at endoftext token if present\n        end_marker = \"<|endoftext|>\"\n        if end_marker in generated_only:\n            generated_only = generated_only.split(end_marker)[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\ndef save_model_checkpoint(model, optimizer, epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch  , \n        'memory_model_state_dict': model.memory.state_dict()   ,\n        \n    }\n    torch.save(checkpoint, path.format(epoch))\ndef after_save_load():\n    checkpoint = torch.load(\"checkpoint_epoch_7.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n\n\ndef train_model(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n    # scheduler = get_cosine_schedule_with_warmup(optimizer,\n    #                                         num_warmup_steps=500,\n    #                                         num_training_steps=total_steps)\n    for p in model.parameters():p.requires_grad= True \n    # for p in model.memory.compression.parameters():p.requires_grad =  False \n    # for p in model.memory.W_cell.parameters():p.requires_grad = False \n    ckpt = torch.load(\"memory_encoder.pth\")\n    model.memory.compression.load_state_dict(ckpt[\"compression\"])\n    model.memory.W_cell.load_state_dict(ckpt[\"W_cell\"])\n    print(\"Encoder weights are loaded \")\n    with torch.no_grad():\n        all_concepts =  []\n        \n    \n        for  input_batch , target_batch in train_dataloader:\n            input_batch = input_batch.to(device)\n            x = model.embedding(input_batch)\n            x =  model.memory.compression(x.mean(dim=1))\n            x = model.memory.W_cell(x)\n            all_concepts.append(x)\n\n    concept_pool=  torch.cat(all_concepts , dim=0)\n    k = min(model.memory.memory_size , concept_pool.shape[0])\n    kmeans = KMeans(n_clusters = k , random_state = 42)\n    kmeans.fit(concept_pool.cpu().numpy())\n    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n    print(centroids.shape , model.memory.key_memory.shape)\n    centroids = F.normalize(centroids, dim=-1)\n\n    n_centroids = centroids.size(0)  # 114\n    \n    with torch.no_grad():\n        model.memory.key_memory.data[:n_centroids] = centroids\n        model.memory.value_memory.data[:n_centroids] = centroids\n        model.memory.cell_state.data[:n_centroids] = centroids\n\n    print(centroids.shape , model.memory.key_memory.shape)\n\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n        save_model_checkpoint(model , optimizer , epoch+1)\n        \n\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:07:17.698243Z","iopub.execute_input":"2025-06-06T06:07:17.698481Z","iopub.status.idle":"2025-06-06T06:07:18.088613Z","shell.execute_reply.started":"2025-06-06T06:07:17.698462Z","shell.execute_reply":"2025-06-06T06:07:18.087794Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch import nn\n\ndef cal_loss_batch(input_batch, target_batch,\n                   teacher_model: nn.Module,\n                   student_model: nn.Module,\n                   device: torch.device,\n                   mem_cof: float = 0.1,\n                   distil_temp: float = 5.0,\n                   alpha: float = 0.9):\n\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    with torch.no_grad():\n        teacher_output = teacher_model(input_batch)\n        teacher_logits = teacher_output.logits  # [B, T, V]\n\n    student_logits, x_emb, mem_output = student_model(input_batch)  # [B, T, V]\n    B, T, V = student_logits.shape\n\n    assert student_logits.shape == teacher_logits.shape, \\\n        f\"Shape mismatch: student={student_logits.shape}, teacher={teacher_logits.shape}\"\n\n    # Knowledge distillation loss\n    student_soft = F.log_softmax(student_logits.float() / distil_temp, dim=-1)\n    teacher_soft = F.softmax(teacher_logits.float() / distil_temp, dim=-1)\n    teacher_soft = torch.clamp(teacher_soft, min=1e-9)\n\n    distillation_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (distil_temp ** 2)\n    distillation_loss = torch.clamp(distillation_loss, max=100.0)\n\n    # Cross-entropy loss\n    ce_loss = F.cross_entropy(\n        student_logits.view(B * T, V),\n        target_batch.view(B * T),\n        ignore_index=-100,\n    )\n\n    # Memory loss\n    memory_loss = total_loss(inputs=x_emb, memory=student_model.memory,\n                             outputs=mem_output, targets=target_batch.float())\n\n    total = alpha * ce_loss + (1 - alpha) * distillation_loss + mem_cof * memory_loss\n\n\n    return total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T07:02:36.388893Z","iopub.execute_input":"2025-06-06T07:02:36.389229Z","iopub.status.idle":"2025-06-06T07:02:36.396750Z","shell.execute_reply.started":"2025-06-06T07:02:36.389204Z","shell.execute_reply":"2025-06-06T07:02:36.395637Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"def evaluate_model(student_model, teacher_model,train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, student_model,teacher_model ,  device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, student_model,teacher_model ,  device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\ndef calc_loss_loader(data_loader , student_model , teacher_model ,device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , teacher_model , student_model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T07:06:31.814173Z","iopub.execute_input":"2025-06-06T07:06:31.814469Z","iopub.status.idle":"2025-06-06T07:06:31.820773Z","shell.execute_reply.started":"2025-06-06T07:06:31.814446Z","shell.execute_reply":"2025-06-06T07:06:31.819835Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn.functional as F\n\ndef train_model(\n    teacher_model: nn.Module,\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    scaler = GradScaler()\n    torch.autograd.set_detect_anomaly(True)\n\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # Ensure all parameters require grad\n    for p in model.parameters():\n        p.requires_grad = True\n\n    # Load pretrained encoder weights into the memory module\n    ckpt = torch.load('/kaggle/input/v3-encoder-weight/memory_encoder(3).pth')\n    model.memory.compression.load_state_dict(ckpt[\"compression\"])\n    model.memory.W_cell.load_state_dict(ckpt[\"W_cell\"])\n    print(\"Encoder weights are loaded \")\n\n    # Build initial memory centroids via KMeans\n    with torch.no_grad():\n        all_concepts = []\n        for input_batch, _ in train_dataloader:\n            input_batch = input_batch.to(device)\n            x = model.embedding(input_batch)\n            x = model.memory.compression(x.mean(dim=1))\n            x = model.memory.W_cell(x)\n            all_concepts.append(x)\n        concept_pool = torch.cat(all_concepts, dim=0)\n\n        k = min(model.memory.memory_size, concept_pool.shape[0])\n        from sklearn.cluster import KMeans\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(concept_pool.cpu().numpy())\n        centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n        centroids = F.normalize(centroids, dim=-1)\n        n_centroids = centroids.size(0)\n\n        model.memory.key_memory.data[:n_centroids] = centroids\n        model.memory.value_memory.data[:n_centroids] = centroids\n        model.memory.cell_state.data[:n_centroids] = centroids\n\n    # Training loop\n    for epoch in range(num_epochs):\n        loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        model.train()\n\n        for inputs_batch, target_batch in loop:\n            inputs_batch = inputs_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            optimizer.zero_grad()\n\n            # 1) Compute loss under autocast\n            with autocast():\n                loss = cal_loss_batch(\n                    input_batch=inputs_batch,\n                    target_batch=target_batch,\n                    device=device,\n                    student_model=model,\n                    teacher_model=teacher_model\n                )\n\n    \n            # 3) Backward + gradient clipping + optimizer step\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            # 5) Periodic evaluation\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model,teacher_model ,  train_dataloader, eval_dataloader, device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                loop.set_postfix({\n                    'loss': f\"{loss.item():.4f}\",\n                    'step': global_step,\n                    'train_loss': f\"{train_loss:.4f}\",\n                    'val_loss': f\"{val_loss:.4f}\"\n                })\n\n        # 6) Generate a sample and save checkpoint at end of epoch\n        generate_and_print_sample(model, train_dataloader.dataset.tokenizer, device, start_context)\n        save_model_checkpoint(model, optimizer, epoch + 1)\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T07:06:32.414406Z","iopub.execute_input":"2025-06-06T07:06:32.414712Z","iopub.status.idle":"2025-06-06T07:06:32.427723Z","shell.execute_reply.started":"2025-06-06T07:06:32.414690Z","shell.execute_reply":"2025-06-06T07:06:32.426866Z"}},"outputs":[],"execution_count":107},{"cell_type":"markdown","source":"# Knowledge Distillation","metadata":{}},{"cell_type":"code","source":"\nteacher_name = \"deepseek-ai/deepseek-llm-7b-chat\"\nteacher_model = AutoModelForCausalLM.from_pretrained(teacher_name, device_map=\"auto\", torch_dtype=torch.float16)\nteacher_tokenizer = AutoTokenizer.from_pretrained(teacher_name)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:12:40.981992Z","iopub.execute_input":"2025-06-06T06:12:40.982316Z","iopub.status.idle":"2025-06-06T06:12:57.690947Z","shell.execute_reply.started":"2025-06-06T06:12:40.982292Z","shell.execute_reply":"2025-06-06T06:12:57.690126Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd9f11e651f42959ddf5161deb01a49"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Loss for Knowledge distillation\ndef distillation_loss(student_logits , teacher_logits  , labels , T = 2.0 , alpha = 0.7):\n    loss_kd =  F.kl_div(\n        F.log_softmax(stident_logits / T , dim=-1) , \n        F.softmax(teacher_logits / T , dim=-1),\n         reduction=\"batchmean\"\n    ) *(T**2 )\n\n    loss_ce = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n    return alpha * loss_kd +(1-alpha) * loss_ce ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:12:57.692597Z","iopub.execute_input":"2025-06-06T06:12:57.692965Z","iopub.status.idle":"2025-06-06T06:12:57.697939Z","shell.execute_reply.started":"2025-06-06T06:12:57.692932Z","shell.execute_reply":"2025-06-06T06:12:57.697041Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"teacher_model.config.vocab_size ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:04.237674Z","iopub.execute_input":"2025-06-06T06:13:04.237956Z","iopub.status.idle":"2025-06-06T06:13:04.243130Z","shell.execute_reply.started":"2025-06-06T06:13:04.237934Z","shell.execute_reply":"2025-06-06T06:13:04.242205Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"102400"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"teacher_tokenizer.vocab_size ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:05.054944Z","iopub.execute_input":"2025-06-06T06:13:05.055245Z","iopub.status.idle":"2025-06-06T06:13:05.060324Z","shell.execute_reply.started":"2025-06-06T06:13:05.055222Z","shell.execute_reply":"2025-06-06T06:13:05.059460Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"100000"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"teacher_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:13.887107Z","iopub.execute_input":"2025-06-06T06:13:13.887392Z","iopub.status.idle":"2025-06-06T06:13:13.894358Z","shell.execute_reply.started":"2025-06-06T06:13:13.887370Z","shell.execute_reply":"2025-06-06T06:13:13.893598Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(102400, 4096)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n)"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# MemoryGPT Model training  \n","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T14:27:39.765858Z","iopub.execute_input":"2025-06-05T14:27:39.766197Z","iopub.status.idle":"2025-06-05T14:27:39.781342Z","shell.execute_reply.started":"2025-06-05T14:27:39.766163Z","shell.execute_reply":"2025-06-05T14:27:39.780734Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# GPT Config ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nGPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,#126 \n# Context lengt\n\"emb_dim\": 768,\n# Embedding dimension\n\"n_heads\": 12,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False\n# Query-Key-Value bias\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T14:27:45.067970Z","iopub.execute_input":"2025-06-05T14:27:45.068300Z","iopub.status.idle":"2025-06-05T14:27:45.078045Z","shell.execute_reply.started":"2025-06-05T14:27:45.068271Z","shell.execute_reply":"2025-06-05T14:27:45.077203Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"teacher_tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T14:27:45.267680Z","iopub.execute_input":"2025-06-05T14:27:45.267944Z","iopub.status.idle":"2025-06-05T14:27:45.272752Z","shell.execute_reply.started":"2025-06-05T14:27:45.267923Z","shell.execute_reply":"2025-06-05T14:27:45.271966Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"100000"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"GPT_CONFIG_124M_Memory = {\n\"vocab_size\": teacher_model.config.vocab_size ,#100000,# 50257\n    # Vocabulary size\n\"context_length\": 126,\n# Context length\n\"emb_dim\": 128,\n# Embedding dimension\n\"n_heads\": 4,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False,\n'memory_dim':128,\n'max_slots' :1000,\n'memory_heads':2 ,\n\n# Query-Key-Value bias\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:37.356249Z","iopub.execute_input":"2025-06-06T06:13:37.356585Z","iopub.status.idle":"2025-06-06T06:13:37.360727Z","shell.execute_reply.started":"2025-06-06T06:13:37.356556Z","shell.execute_reply":"2025-06-06T06:13:37.359915Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def train_encoder(model , train_dataloader , device = 'cuda'):\n    memory = model.memory \n    for p in model.parameters():p.requires_grad = False \n    for p in model.memory.compression.parameters(): p.requires_grad = True\n    for p in model.memory.W_cell.parameters():p.requires_grad =  True \n    optimizer = torch.optim.Adam(\n    list(memory.compression.parameters()) + list(memory.W_cell.parameters()),\n    lr=1e-3\n)\n    global_step= -1 \n    print('tatal step ' , len(train_dataloader) * 5)\n    for _ in tqdm(range(5)):\n        \n        for input_batch , _  in train_dataloader:\n            input_batch =  input_batch.to(device)\n            x1 = model.embedding(input_batch)\n            B , _ , _  =  x1.shape\n            x2 =  model.embedding(input_batch)\n\n            q1 = memory.W_cell(memory.compression(x1.mean(dim=1)))\n            q2 =  memory.W_cell(memory.compression(x2.mean(dim=1)))\n\n            q1, q2 = F.normalize(q1, dim=-1), F.normalize(q2, dim=-1)\n\n            sim = torch.matmul(q1,q2.T)\n            loss = F.cross_entropy(sim/0.1  , torch.arange(B, device = sim.device))\n\n            # print(loss)\n            optimizer.zero_grad();loss.backward();optimizer.step()\n            global_step +=1 \n             # print(\n             #        f\"Epoch: {_+1} (step {global_step:06d}):\",\n    x, _ = next(iter(train_dataloader))\n    x =  x.to(device)\n    x = model.embedding(x)\n\n    q =  memory.W_cell(memory.compression(x.mean(1)))\n    q1 =  memory.W_cell(memory.compression(x.mean(1)))\n    avg_sim = F.cosine_similarity(q, q1, dim=-1).mean().item()\n    # print(avg_sim)\n    assert avg_sim > 0.95 , 'Encoder still drift too much'\n\n    ckpt = {\n        \"compression\": memory.compression.state_dict(),\n        \"W_cell\":      memory.W_cell.state_dict()\n    }\n    torch.save(ckpt, \"memory_encoder.pth\")\n    print(\"saved memory_encoder.pth\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:39.305910Z","iopub.execute_input":"2025-06-06T06:13:39.306210Z","iopub.status.idle":"2025-06-06T06:13:39.315024Z","shell.execute_reply.started":"2025-06-06T06:13:39.306186Z","shell.execute_reply":"2025-06-06T06:13:39.314091Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:41.246419Z","iopub.execute_input":"2025-06-06T06:13:41.246730Z","iopub.status.idle":"2025-06-06T06:13:41.250703Z","shell.execute_reply.started":"2025-06-06T06:13:41.246706Z","shell.execute_reply":"2025-06-06T06:13:41.249733Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"\ndevice =  'cuda' if torch.cuda.is_available() else \"cpu\"\nmodel =  GPTMemoryEnhanced(GPT_CONFIG_124M_Memory).to(device)\n\noptimizer =  torch.optim.AdamW(model.parameters() , lr=0.0004,weight_decay=0.01 )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:19:52.732053Z","iopub.execute_input":"2025-06-06T06:19:52.732401Z","iopub.status.idle":"2025-06-06T06:19:52.923412Z","shell.execute_reply.started":"2025-06-06T06:19:52.732375Z","shell.execute_reply":"2025-06-06T06:19:52.922740Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from collections import defaultdict\n\ndef get_param_group_summary(model):\n    groups = defaultdict(int)\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if \"embedding\" in name:\n            groups[\"embedding\"] += param.numel()\n        elif \"transformer\" in name:\n            groups[\"transformer_blocks\"] += param.numel()\n        elif \"memory\" in name or \"episodic\" in name or \"semantic\" in name:\n            groups[\"memory_modules\"] += param.numel()\n        elif \"norm\" in name:\n            groups[\"normalization\"] += param.numel()\n        elif \"lm_head\" in name or \"projection\" in name:\n            groups[\"output_projection\"] += param.numel()\n        else:\n            groups[\"other\"] += param.numel()\n    total = sum(groups.values())\n    for k, v in groups.items():\n        print(f\"{k:20s}: {v:,} parameters\")\n    print(f\"\\nTotal: {total:,}\")\nget_param_group_summary(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:43.386141Z","iopub.execute_input":"2025-06-06T06:13:43.386429Z","iopub.status.idle":"2025-06-06T06:13:43.394988Z","shell.execute_reply.started":"2025-06-06T06:13:43.386405Z","shell.execute_reply":"2025-06-06T06:13:43.394281Z"}},"outputs":[{"name":"stdout","text":"embedding           : 13,107,328 parameters\nmemory_modules      : 665,227 parameters\ntransformer_blocks  : 2,082,048 parameters\nnormalization       : 128 parameters\nother               : 32,896 parameters\noutput_projection   : 102,400 parameters\n\nTotal: 15,990,027\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"\n\nnum_epochs =  1\ntrain_ratio = 0.90\n\nfilename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\nwith open(filename , 'r') as f:\n    text_data =  json.load(f)\ntext_data = text_data[:150]\n\nsplit = int(train_ratio * len(text_data))\n\ntrain_data =  text_data[:split]\nval_data =  text_data[split:]\n\ntrain_dataloader =  create_dataloader_v1(data=train_data , batch_size=2 , max_length=GPT_CONFIG_124M_Memory['context_length'] ,tokenizer = teacher_tokenizer ,  shuffle=True , drop_last= True)\nval_dataloader = create_dataloader_v1(data=val_data , batch_size=2 , max_length=GPT_CONFIG_124M_Memory['context_length']  ,tokenizer  = teacher_tokenizer, shuffle=False , drop_last=False )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:45.546639Z","iopub.execute_input":"2025-06-06T06:13:45.546950Z","iopub.status.idle":"2025-06-06T06:13:46.240183Z","shell.execute_reply.started":"2025-06-06T06:13:45.546916Z","shell.execute_reply":"2025-06-06T06:13:46.239018Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:48.109028Z","iopub.execute_input":"2025-06-06T06:13:48.109330Z","iopub.status.idle":"2025-06-06T06:13:48.114489Z","shell.execute_reply.started":"2025-06-06T06:13:48.109307Z","shell.execute_reply":"2025-06-06T06:13:48.113618Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"80"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"train_encoder(model , train_dataloader )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:13:50.226176Z","iopub.execute_input":"2025-06-06T06:13:50.226490Z","iopub.status.idle":"2025-06-06T06:13:53.454712Z","shell.execute_reply.started":"2025-06-06T06:13:50.226464Z","shell.execute_reply":"2025-06-06T06:13:53.454021Z"}},"outputs":[{"name":"stdout","text":"tatal step  400\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fdb4b58ca854ef4addd6bed7719ffac"}},"metadata":{}},{"name":"stdout","text":"saved memory_encoder.pth\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import torch._dynamo\nimport logging \ndyno_logger =  logging.getLogger('torch._dynamo')\ndyno_logger.setLevel(logging.ERROR)\nfunction_logger =  logging.getLogger('torch._functorch')\nfunction_logger.setLevel(logging.ERROR)\ntorch._dynamo.config.suppress_errors = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T06:15:15.806293Z","iopub.execute_input":"2025-06-06T06:15:15.806677Z","iopub.status.idle":"2025-06-06T06:15:15.812488Z","shell.execute_reply.started":"2025-06-06T06:15:15.806644Z","shell.execute_reply":"2025-06-06T06:15:15.811340Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"compiled_model = torch.compile(model, mode=\"max-autotune\", backend=\"aot_eager\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T07:00:58.750169Z","iopub.execute_input":"2025-06-06T07:00:58.750473Z","iopub.status.idle":"2025-06-06T07:00:58.754986Z","shell.execute_reply.started":"2025-06-06T07:00:58.750450Z","shell.execute_reply":"2025-06-06T07:00:58.754129Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef cal_loss_batch(\n    input_batch, target_batch,\n    teacher_model: torch.nn.Module,\n    student_model: torch.nn.Module,\n    device: torch.device,\n    mem_cof: float = 0.1,\n    distil_temp: float = 5.0,\n    alpha: float = 0.86\n):\n    input_batch = input_batch.to(device)\n    target_batch = target_batch.to(device)\n\n    # 1) Teacher forward\n    with torch.no_grad():\n        teacher_output = teacher_model(input_batch)\n        teacher_logits = teacher_output.logits  # [B, T, V_teacher]\n\n    # 2) Student forward\n    student_logits, x_emb, mem_output = student_model(input_batch)  # [B, T, V_student]\n    B, T, V_student = student_logits.shape\n    V_teacher = teacher_logits.shape[-1]\n\n    # 3) Check vocab‐size match\n    assert V_student == V_teacher, f\"Vocab mismatch: student={V_student}, teacher={V_teacher}\"\n\n    # 4) CE loss\n    ce_loss = F.cross_entropy(\n        student_logits.view(B * T, V_student),\n        target_batch.view(B * T),\n        ignore_index=-100,\n    )\n\n    # 5) Distillation loss (KL)\n    s_logits = (student_logits.float() / distil_temp).log_softmax(dim=-1)  # [B,T,V]\n    t_probs = (teacher_logits.float() / distil_temp).softmax(dim=-1).clamp(min=1e-9)\n    distil_loss = F.kl_div(s_logits, t_probs, reduction='batchmean') * (distil_temp ** 2)\n    distil_loss = torch.clamp(distil_loss, max=100.0)\n\n    # 6) Memory loss\n    memory_loss = total_loss(inputs=x_emb, memory=student_model.memory,\n                             outputs=mem_output, targets=target_batch.float())\n\n    # 7) Combine\n    total = alpha * ce_loss + (1 - alpha) * distil_loss + mem_cof * memory_loss\n\n    # 8) Print for debugging\n    print(f\"CE: {ce_loss.item():.6f}, KD: {distil_loss.item():.6f}, MEM: {memory_loss.item():.6f}, TOT: {total.item():.6f}\")\n\n\n    return total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T07:00:34.505015Z","iopub.execute_input":"2025-06-06T07:00:34.505327Z","iopub.status.idle":"2025-06-06T07:00:34.513065Z","shell.execute_reply.started":"2025-06-06T07:00:34.505304Z","shell.execute_reply":"2025-06-06T07:00:34.512057Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\nfrom tqdm.notebook import tqdm\nimport torch\n\ndef train_model(\n    teacher_model: torch.nn.Module,\n    model: torch.nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    scaler = GradScaler()\n    torch.autograd.set_detect_anomaly(True)\n\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # Ensure all params require grad\n    for param in model.parameters():\n        param.requires_grad = True\n\n    # Load memory‐encoder weights\n    ckpt = torch.load('/kaggle/input/v3-encoder-weight/memory_encoder(3).pth')\n    model.memory.compression.load_state_dict(ckpt[\"compression\"])\n    model.memory.W_cell.load_state_dict(ckpt[\"W_cell\"])\n    print(\"Encoder weights are loaded\")\n\n    # Build initial memory via KMeans (unchanged)\n    with torch.no_grad():\n        all_concepts = []\n        for input_batch, _ in train_dataloader:\n            input_batch = input_batch.to(device)\n            x = model.embedding(input_batch)\n            x = model.memory.compression(x.mean(dim=1))\n            x = model.memory.W_cell(x)\n            all_concepts.append(x)\n        concept_pool = torch.cat(all_concepts, dim=0)\n\n        from sklearn.cluster import KMeans\n        k = min(model.memory.memory_size, concept_pool.size(0))\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(concept_pool.cpu().numpy())\n        centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)\n        centroids = torch.nn.functional.normalize(centroids, dim=-1)\n        n_centroids = centroids.size(0)\n\n        model.memory.key_memory.data[:n_centroids] = centroids\n        model.memory.value_memory.data[:n_centroids] = centroids\n        model.memory.cell_state.data[:n_centroids] = centroids\n\n    # TRAINING LOOP\n    for epoch in range(num_epochs):\n        progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        model.train()\n\n        for inputs_batch, target_batch in progress:\n            inputs_batch = inputs_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            optimizer.zero_grad()\n\n            # 1) Compute combined loss\n            with autocast():\n                loss = cal_loss_batch(\n                    input_batch=inputs_batch,\n                    target_batch=target_batch,\n                    device=device,\n                    student_model=model,\n                    teacher_model=teacher_model\n                )\n\n            # 2) If loss is None, skip backward\n            if loss is None:\n                continue\n\n            # 3) Backward + clip + step\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            # 4) Periodic evaluation\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                progress.set_postfix({\n                    'loss': f\"{loss.item():.4f}\",\n                    'step': global_step,\n                    'train_loss': f\"{train_loss:.4f}\",\n                    'val_loss': f\"{val_loss:.4f}\"\n                })\n\n        # 5) At epoch end: generate sample + save checkpoint\n        generate_and_print_sample(model, train_dataloader.dataset.tokenizer, device, start_context)\n        save_model_checkpoint(model, optimizer, epoch + 1)\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T07:00:35.745595Z","iopub.execute_input":"2025-06-06T07:00:35.745934Z","iopub.status.idle":"2025-06-06T07:00:35.758142Z","shell.execute_reply.started":"2025-06-06T07:00:35.745912Z","shell.execute_reply":"2025-06-06T07:00:35.757224Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"\n\n\n\nprint('start training')\ntrain_losses , val_losses , token_seen =  train_model(\n    teacher_model =teacher_model,\n    \n    model=compiled_model.to(device) , \n    train_dataloader=train_dataloader, \n    device=device, \n    eval_freq=5 , \n    eval_dataloader=val_dataloader , \n    optimizer=optimizer, \n    eval_iter=3,  \n    num_epochs= 10, \n    start_context='### Instruction: What are the three primary colors? .n### Response:'\n)\nprint(model.memory.get_memory_metrics())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T07:06:37.919884Z","iopub.execute_input":"2025-06-06T07:06:37.920177Z"}},"outputs":[{"name":"stdout","text":"start training\n🚀 Total training steps: 800\nEncoder weights are loaded \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/80 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7b4f5aba06445ca8a2960e06ba2a41"}},"metadata":{}},{"name":"stdout","text":"\n[Prompt]: ### Instruction: What are the three primary colors? .n### Response:\n\n[Generated]: FE indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference名誉名誉名誉名誉名誉名誉名誉名誉 greatest greatest greatest mime mimepapapapapapapapapapapapapapapapapapa Dod sil Travels Travels \\\\ \\\\ \\\\ \\\\ \\\\ \\\\名誉名誉名誉 ways ways ways ways ways ways LIB\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/80 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ca87a5bfbc4b389098ebeac0f89b66"}},"metadata":{}},{"name":"stdout","text":"\n[Prompt]: ### Instruction: What are the three primary colors? .n### Response:\n\n[Generated]: sil sil reliable reliable reliable reliable reliable reliable reliable reliable reliable*}[! запазва запазва запазва запазва запазваChaChaChaChaCha臀部 Гю indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference dropdown indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference indifference \"< \"< \"< \"< \"< \"< \"< \"< \"< \"< \"< \"<\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/80 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"526026d897a64d869fa9b5165481f248"}},"metadata":{}},{"name":"stdout","text":"\n[Prompt]: ### Instruction: What are the three primary colors? .n### Response:\n\n[Generated]: 会让会让 Introdu Introdu Introdu Introdu Goal Goal Goal Goal Goal Goal Goal Goal Goal Goal Goalaring texlive texlive texlive texlive texlive texlive texlive dye.机动车 uncertainty uncertainty Heating融化 preve preve preve preve preve preve preve preve preve preve Heating, compilaci compilaci reliable*}[!Region снаря снаря снаря the Heating Heating Heating Heating Heating Heating,WalletWalletWallet\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/80 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a163607515b40d1a8496fe8182ea24f"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"!pip install torch --upgrade\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T07:07:10.035257Z","iopub.execute_input":"2025-06-05T07:07:10.035764Z","iopub.status.idle":"2025-06-05T07:09:55.181667Z","shell.execute_reply.started":"2025-06-05T07:07:10.035719Z","shell.execute_reply":"2025-06-05T07:09:55.180564Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torch\n  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.3.1 (from torch)\n  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch) (75.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nDownloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cufile-cu12, nvidia-cudnn-cu12, nvidia-cuda-nvrtc-cu12, torch\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"model.memory.model_save(path =  'Memory_saved.pth')\nmodel.memory.model_save(path = 'memory_saved.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# Assume `memory` is your EfiBioSemanticMemory_V2 instance, already initialized\n# and `W_cell` (compression) is frozen or pretrained so that random_input→embedding.\nmemory = model.memory\nmemory.to(device)\nmemory.eval()\nmemory.hit_count  = torch.tensor(0).to(device) # if you have a method; otherwise manually zero out hit_count, write_count, query_count\nmemory.write_count  = torch.tensor(0).to(device) \nmemory.query_count = torch.tensor(0).to(device) \n# Helper to run a single step and report\ndef step_test(x):\n    x= x.unsqueeze(0).unsqueeze(0)\n    print(x.shape)\n    out, retrieved, topk_idx, attn_weights = memory(x , training =  False ) # x shape [D]; expand to [1,1,D]\n    max_sim = float(F.cosine_similarity(memory.key_memory[topk_idx.squeeze()], memory.key_memory[topk_idx.squeeze()], dim=-1).mean())\n    return max_sim\n\n# 1) Same vector twice\nvec1 = torch.randn(memory.input_dim, device=memory.key_memory.device)\nsim1 = step_test(vec1)\nsim2 = step_test(vec1)\n\n# 2) Two distinct vectors\nvec2 = torch.randn(memory.input_dim, device=memory.key_memory.device)\nsim3 = step_test(vec2)\nsim4 = step_test(vec2)\n\nprint(\" sims: first_pass(vec1) =\", f\"{sim1:.4f}\",\n      \"| second_pass(vec1) =\", f\"{sim2:.4f}\")\nprint(\" sims: first_pass(vec2) =\", f\"{sim3:.4f}\",\n      \"| second_pass(vec2) =\", f\"{sim4:.4f}\")\n\nprint(\" final metrics:\",\n      f\"hit_count={memory.hit_count.item()}\",\n      f\"write_count={memory.write_count.item()}\",\n      f\"query_count={memory.query_count.item()}\")\n\n# Expected outcome:\n# - sim1 < hit_threshold (no slot existed) → write_count=1\n# - sim2  ≃ 0.9–1.0 (slot now exists) → hit_count=1\n# - sim3 < hit_threshold (new) → write_count=2\n# - sim4  ≃ 0.9–1.0 → hit_count=2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T15:12:39.331726Z","iopub.execute_input":"2025-06-02T15:12:39.332044Z","iopub.status.idle":"2025-06-02T15:12:39.531780Z","shell.execute_reply.started":"2025-06-02T15:12:39.332016Z","shell.execute_reply":"2025-06-02T15:12:39.531021Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.0303, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(0, device='cuda:0')\nWriting Happen\nWriting Happen\ntorch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.9465, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(1, device='cuda:0')\nUpdate Happen\n average update_gate: 0.6290084719657898  std: 0.001736109028570354\ntorch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.3093, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(1, device='cuda:0')\nWriting Happen\nWriting Happen\ntorch.Size([1, 1, 128])\nhit_threshold 0.61\nmax scores  tensor(0.9538, device='cuda:0', grad_fn=<SqueezeBackward1>)\nhit count tensor(2, device='cuda:0')\nUpdate Happen\n average update_gate: 0.8370599150657654  std: 0.010752998292446136\n sims: first_pass(vec1) = nan | second_pass(vec1) = 1.0000\n sims: first_pass(vec2) = nan | second_pass(vec2) = 1.0000\n final metrics: hit_count=2 write_count=2 query_count=4\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"model.memory.write_count ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.416079Z","iopub.status.idle":"2025-05-21T17:14:19.416328Z","shell.execute_reply":"2025-05-21T17:14:19.416229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# If you loaded from a JSON file\n# with open(\"loss_history.json\", \"r\") as f:\n#     data = json.load(f)\n#     train_losses = data[\"train_loss\"]\n#     val_losses = data[\"val_loss\"]\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\nplt.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\nplt.xlabel(\"Evaluation Step\")\npri\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")  # Save the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.425933Z","iopub.status.idle":"2025-05-21T17:14:19.426202Z","shell.execute_reply":"2025-05-21T17:14:19.426098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\n\n# Load model\nmodel = GPTMQModel2(GPT_CONFIG_124M)\n# model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_7.pt\"))\ncheckpoint = torch.load(\"checkpoint_epoch_7.pt\")\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\nmodel.eval().to(device)\n\n# Tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Utility functions\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n# Sampling-based generate function (uses your logic)\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\nf\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n# High-level text generation function\ndef generate_response(prompt, model, tokenizer, max_new_tokens=100, context_size=128, temperature=1.0, top_k=50):\n    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n    generated_ids = generate(\n        model=model,\n        idx=input_ids,\n        max_new_tokens=max_new_tokens,\n        context_size=context_size,\n        temperature=temperature,\n        top_k=top_k\n    )\n    return token_ids_to_text(generated_ids, tokenizer)\n\n# Try it out\n# prompt = \"### Instruction:\\nExplain what is deep learning.\\n\\n### Response:\\n <bot>\"\nprompt = \"\"\"\n\n'### Instruction :Give three tips for staying healthy ### Response:'\n\"\"\".strip()\n\n\noutput = generate_response(prompt, model, tokenizer)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.427072Z","iopub.status.idle":"2025-05-21T17:14:19.427368Z","shell.execute_reply":"2025-05-21T17:14:19.427264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is generated\n        if idx_next.item() == end_token_id:\n            break\n\n    return idx\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\nraw_output = generate_response(prompt, model, tokenizer)\ncleaned_output = truncate_after_n_bullets(raw_output)\nprint(cleaned_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.428166Z","iopub.status.idle":"2025-05-21T17:14:19.428492Z","shell.execute_reply":"2025-05-21T17:14:19.428345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_response(\n    prompt, model, tokenizer,\n    temperature=0.8,  # better balance\n    top_k=40,         # a bit narrower selection\n    max_new_tokens=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T17:14:19.429419Z","iopub.status.idle":"2025-05-21T17:14:19.429775Z","shell.execute_reply":"2025-05-21T17:14:19.429611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device).unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is in the generated output\n        if end_token_id in idx_next:\n            break\n\n    return idx\n\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\n\n# 🔁 Input prompt\nprompt = \"### Instruction: What are the three primary colors? \\n### Response:\"\n\n# 🔁 Tokenize input\ninput_ids = text_to_token_ids(prompt, tokenizer).to(device)\n\n# 🔁 Generate output tokens\noutput_ids = generate(\n    model=model,\n    idx=input_ids,\n    max_new_tokens=100,\n    context_size=128,\n    temperature=0.7,\n    top_k=40\n)\n\n# 🔁 Decode and postprocess\noutput_text = tokenizer.decode(output_ids[0].tolist())\n\n# ✂️ Truncate after 3 bullets (optional)\nfinal_output = truncate_after_n_bullets(output_text)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.421013Z","iopub.status.idle":"2025-04-24T09:34:44.421333Z","shell.execute_reply":"2025-04-24T09:34:44.421177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}