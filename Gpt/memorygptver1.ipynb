{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339249,"sourceType":"datasetVersion","datasetId":7093845},{"sourceId":11378548,"sourceType":"datasetVersion","datasetId":7124129},{"sourceId":11378997,"sourceType":"datasetVersion","datasetId":7124489}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math \nimport torch \nimport torch.nn as nn \nimport transformers \nfrom tqdm.notebook import tqdm\n# Memory Network\nimport torch.nn.functional as F \nfrom typing import Tuple , Optional\nimport torch.bin \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:06.381567Z","iopub.execute_input":"2025-04-26T04:59:06.381881Z","iopub.status.idle":"2025-04-26T04:59:11.223895Z","shell.execute_reply.started":"2025-04-26T04:59:06.381857Z","shell.execute_reply":"2025-04-26T04:59:11.223063Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:11.225001Z","iopub.execute_input":"2025-04-26T04:59:11.225321Z","iopub.status.idle":"2025-04-26T04:59:11.229260Z","shell.execute_reply.started":"2025-04-26T04:59:11.225298Z","shell.execute_reply":"2025-04-26T04:59:11.228184Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# RMS NORM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim)) \n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        norm_x = self._norm(x.float()).type_as(x) \n        return norm_x * self.scale \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:11.231074Z","iopub.execute_input":"2025-04-26T04:59:11.231377Z","iopub.status.idle":"2025-04-26T04:59:11.245833Z","shell.execute_reply.started":"2025-04-26T04:59:11.231344Z","shell.execute_reply":"2025-04-26T04:59:11.244956Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Embedding Layer","metadata":{}},{"cell_type":"code","source":"import torch \n\nclass EmbeddingLayer(torch.nn.Module):\n    def __init__(self , vocab_size , embedding_dim):\n        super().__init__()\n\n        self.embedding_layer= torch.nn.Embedding(vocab_size , embedding_dim)\n\n    def forward(self , input_tokens):\n        return self.embedding_layer(input_tokens)\n        \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:11.246973Z","iopub.execute_input":"2025-04-26T04:59:11.247249Z","iopub.status.idle":"2025-04-26T04:59:11.259505Z","shell.execute_reply.started":"2025-04-26T04:59:11.247223Z","shell.execute_reply":"2025-04-26T04:59:11.258684Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# FeedForward Layer ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n      \n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n            GELU(),\n            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n        )\n    def forward(self, x ):\n        return self.layers(x)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:11.260391Z","iopub.execute_input":"2025-04-26T04:59:11.260681Z","iopub.status.idle":"2025-04-26T04:59:11.272563Z","shell.execute_reply.started":"2025-04-26T04:59:11.260655Z","shell.execute_reply":"2025-04-26T04:59:11.271799Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Normalization Layer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch \n\nclass LayerNorm(nn.Module):\n    def __init__(self , emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale  = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self , x):\n        mean = x.mean(dim= -1, keepdim = True)\n        var = x.var(dim =-1, keepdim = True)\n        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n        return self.scale * norm_x + self.shift \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:11.273992Z","iopub.execute_input":"2025-04-26T04:59:11.274213Z","iopub.status.idle":"2025-04-26T04:59:11.284818Z","shell.execute_reply.started":"2025-04-26T04:59:11.274194Z","shell.execute_reply":"2025-04-26T04:59:11.283999Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# RoPE Embdding ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport torch \nfrom dataclasses import dataclass\n\n\nclass NRopE: # RopE in Numpy \n    def rotate_2d(self,vec , theta_p):\n        cos_theta  , sin_theta  = np.cos(theta_p) , np.sin(theta_p)\n        rotat_vec = np.array([[cos_theta , -sin_theta],\n                    [sin_theta ,cos_theta]])\n        \n        return rotat_vec @ vec\n\n\n    def RoPe(self,x , p , theta = 10000):\n        d = len(x)\n        x_rotate =  np.zeros_like(x)\n        for i in range(0 , d , 2):\n            if i +1< d:\n                theta_p = (theta **(-2*(i//2)))**p \n                roted_pair = self.rotate_2d(x[i:i+1] , theta_p)    \n                x_rotate[i:i+1] = roted_pair\n\n        return x_rotate\n\n\n\n@dataclass\nclass TRopE(torch.nn.Module): # RopE in torch \n    def __init__(self, dim:int ,theta:float = 10000):\n        self.dim = dim \n        self.theta = theta \n        self.freq =  torch.pow(self.theta ,-torch.arange(0 ,dim  , 2)/dim )\n        torch.nn.Parameter('freq' , self.freq)\n\n    def forward(self, x:torch.Tensor , pos:torch.Tensor):\n        batch_size , seq_len, dim = x.shape\n        assert dim ==self.dim ,\"Error dim must be same\"\n        theta_p = torch.einsum(\"n,d->nd\" , pos, self.freq.to(x.device))\n        cos_theta  , sin_theta = torch.cos(theta_p) , torch.sin(theta_p)\n        x_even , x_odd =  x[... , ::2] , x[... , 1::2]\n        x_rotated =  torch.empty_like(x)\n        x_rotated[...,::2] =  x_even * cos_theta - x_odd * sin_theta\n        x_rotated[...,1::2] =  x_even * sin_theta + x_odd * cos_theta\n\n        return x_rotated\n\n\n\n\n\n\n\ndef precompute_freq_cis(  dim:int , end:int , theta:float = 10000.0):\n        \"\"\"dim : dimentions \n        end: end index   \n        \"\"\"\n        freqs =  1/(theta **(torch.arange(0 , dim , 2)[:dim//2].float() / dim))\n        t =  torch.arange(end, device=freqs.device)\n        freqs = torch.outer(t , freqs).float()\n        freqs_cis =  torch.polar(torch.ones_like(freqs), freqs)\n        return freqs_cis \n\n\ndef reshape_for_broadcast(freq_cis  , x):\n        \"\"\" reshape the freqcies to match x dimentions \"\"\"\n        ndim=  x.ndim\n        assert 0<=1<ndim \n        assert freq_cis.shape == (x.shape[1], x.shape[-1]), f\"Expected {(x.shape[1], x.shape[-1])}, got {freq_cis.shape}\" \n        shape = [d if i == 1 or i ==  ndim -1 else 1 for i , d in enumerate(x.shape)]\n        return freq_cis.view(*shape)\n\n\ndef apply_rotary_embedding( xq:torch.Tensor ,xk:torch.Tensor ,  freq_cis:torch.Tensor):\n\n            xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n            xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n\n\n            freq_cies =  reshape_for_broadcast(freq_cis , xq_)\n    \n\n            xq_out = torch.view_as_real(xq_* freq_cies).flatten(3)\n            \n            xk_out = torch.view_as_real(xk_*freq_cies).flatten(3)\n\n\n            return  xq_out.type_as(xq)   ,  xk_out.type_as(xq) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:12.361619Z","iopub.execute_input":"2025-04-26T04:59:12.361935Z","iopub.status.idle":"2025-04-26T04:59:12.375121Z","shell.execute_reply.started":"2025-04-26T04:59:12.361911Z","shell.execute_reply":"2025-04-26T04:59:12.374227Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# MultiHead & MultiQuery Attention Layer ","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention_V2(nn.Module):\n    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n        super().__init__()\n        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n        self.d_in =d_in\n        self.d_out = d_out\n        self.dropout = nn.Dropout(dropout)\n        self.num_heads  = num_heads\n        self.head_dim = d_out // num_heads\n        self.out_proj  = nn.Linear(d_out , d_out)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens , d_in = x.shape\n        keys = self.w_key(x)\n        queries  = self.w_query(x)\n        values = self.w_value(x)\n        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(2, 3)\n        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n        attn_scores.masked_fill(mask_bool , -torch.inf)\n        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n        attn_weights = self.dropout(attn_weights)\n        context_vector = (attn_weights  @ values).transpose(1, 2)\n        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n        context_vector = self.out_proj(context_vector)\n        return context_vector\n\n\n\n\ndef apply_rotary_embedding(xq:torch.Tensor , xk:torch.Tensor , freq_cies:torch.Tensor):\n\n    assert xq.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n    assert xk.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1] , -1, 2))\n\n    freq_cies = reshape_for_broadcast(freq_cies , xq_)\n\n    xq_out = torch.view_as_real(xq_ * freq_cies ).flatten(3)\n\n    xk_out = torch.view_as_real(xk_ * freq_cies).flatten(3)\n\n    return xq_out.type_as(xq) ,  xk_out.type_as(xk)\n\n\n\n\nclass MultiQueryAttentionBlock(nn.Module):\n    def __init__(self, d_model:int , h:int , dropout:float , seq_len:int , qkv_bias =  False ):\n        super().__init__()\n        self.d_model  = d_model \n\n        self.seq_len=  seq_len\n\n        assert d_model % h == 0, \"d_model is must be divided by th head\"\n        self.dropout = nn.Dropout(dropout)\n\n        self.h = h  \n\n        self.d_k = d_model // h \n\n        self.w_qkv =  nn.Linear(d_model , d_model +2 * self.d_k )\n\n        self.w_o = nn.Linear(d_model  , d_model)\n\n        freq_cies = precompute_freq_cis(dim=self.d_k , end=self.seq_len * 2 )\n\n        self.register_buffer('freq_cies' , freq_cies , persistent= False )\n\n    def generate_causal_mask(self, seq_len, device):\n        # shape: (1, 1, seq_len, seq_len)\n        return torch.tril(torch.ones((1, 1, seq_len, seq_len), device=device)).bool()\n\n    @staticmethod\n    def attention(q, k  , v,mask  , dropout):\n        d_k = q.shape[-1]\n\n        attention_score =  (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n\n        if mask is not None :\n            if mask.dim() == 2:\n                      mask = mask.unsqueeze(1).unsqueeze(2)\n            elif mask.dim() == 3:\n                     mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n        \n        attention_score = attention_score.softmax(dim=-1)\n\n        if dropout is not None :\n            attention_score = dropout(attention_score)\n\n        context_vector =  attention_score @ v\n\n        return context_vector  , attention_score\n    \n\n\n    def forward(self, q, mask= None):\n        if mask is None:\n            mask = self.generate_causal_mask(self.seq_len , device = q.device)\n        qkv =  self.w_qkv(q)\n\n        query , key, value =  torch.split(qkv , [self.d_model  , self.d_k , self.d_k], dim=-1)\n\n        query = query.view(query.shape[0] , -1 , self.h , self.d_k).transpose(1, 2)\n\n        key =  key.unsqueeze(1)\n\n        value =  value.unsqueeze(1)\n\n        seq_len =  q.size(1)\n\n        freq_cies = self.freq_cies[:query.shape[1]].to(q.device)\n\n        # freq_cies =  self.freq_cies[:seq_len].to(q.device)\n\n        query , key = apply_rotary_embedding(query , key , freq_cies)\n\n        x , self.attention_score = MultiQueryAttentionBlock.attention(q = query,k =  key,v= value ,mask=mask , dropout= self.dropout)\n\n        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h* self.d_k)\n\n        x = self.w_o(x)\n\n        return x \n    \n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:12.866564Z","iopub.execute_input":"2025-04-26T04:59:12.866853Z","iopub.status.idle":"2025-04-26T04:59:12.882218Z","shell.execute_reply.started":"2025-04-26T04:59:12.866831Z","shell.execute_reply":"2025-04-26T04:59:12.881369Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n\nclass MemorySummaryWriter(nn.Module):\n    def __init__(self, hidden_dim: int, compress_dim: int , num_heads :int =  4 , dropout_rate:float=  0.1):\n        super().__init__()\n\n        self.num_heads = num_heads \n        self.compress_dim = compress_dim\n        self.dropout =  nn.Dropout(dropout_rate)\n\n        assert hidden_dim % num_heads == 0 , \"hidden dim must divisible by num_heads\"\n        self.importance_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dim , hidden_dim // num_heads) , \n                nn.GELU(), \n                nn.Linear(hidden_dim // num_heads, 1)\n            ) for _ in range(num_heads)\n        ])\n\n        self.compresser = nn.Sequential(\n            nn.Linear(hidden_dim * num_heads , hidden_dim * 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate), \n            nn.Linear(hidden_dim * 2, compress_dim),\n            nn.LayerNorm(compress_dim),\n            nn.GELU()\n        )\n    def forward(self , x:torch.Tensor , mask:Optional[torch.Tensor] = None):\n\n        bs, seq_len , d = x.shape \n\n        head_weights =  []\n\n        for head in self.importance_heads:\n            head_weights.append(head(x)) # [B, S , 1]\n\n        attn_weights =  torch.cat(head_weights, dim= -1) # [B , S, num_heads]\n\n        if mask is not None:\n            attn_weights = attn_weights.masked_fill(mask.unsqueeze(-1)==0,float('-inf'))\n\n        attn_weights =  F.softmax(attn_weights , dim= 1 ) # Normalize across the Seq [B , S , num_heads ]\n\n        summary =  torch.einsum('bth, btd->bhd', attn_weights , x )\n# Replace reshape with more robust flattening\n        summary = summary.contiguous().view(bs, -1)  # Safer flattening\n        # pass to the compresser \n        compressed_emb = self.compresser(summary)\n        # compressed_emb =  self.output_projection(compressed_emb)\n\n        return compressed_emb ,  attn_weights\n\n\n\n# use summary writter after the transformer block \n\ndef init_weight_router(route_layer, preferred='semantic'):\n    with torch.no_grad():\n        route_layer.bias.fill_(-1.0)  # Set all to -1\n        idx = {'episodic': 0, 'semantic': 1}[preferred]\n        route_layer.bias[idx] = 1.0   # Prefer this one\n\n\nclass MemoryRouter(nn.Module):\n    \"\"\"\n        The Router Help to select which memory to use [Episodic , semantic ]\n\n    \"\"\"\n\n    def __init__(self,input_dim:int ,temperature:float= 1.0 , dropout:float = 0.1 ):\n        super().__init__()\n        \"\"\"\n            input_dim:input tensor dimention , \n            episodic_capacity : episodic memory size \n            semantic_capcity : semantic memory size \n            temprature :select the memory \n            dropout: dropout rate for Dropout layer \n\n\n        \"\"\"\n\n        self.input_dim =input_dim\n        # self.episodic_memory = episodic_capacity \n        # self.semantic_memory  = semantic_capacity \n        # self.context_lenth =  context_length \n        self.temprature = temperature \n        self.num_memories =  2\n        self.memories_names =  ['episodic', 'semantic' ]\n        self.training = True \n        self.dropout = nn.Dropout(dropout)\n\n        self.router = nn.Sequential(\n            nn.Linear(input_dim , input_dim *2 ),\n            nn.GELU(),\n            nn.Linear(input_dim * 2 , self.num_memories )    # Ouput the num of memories 3 \n        )\n     \n        init_weight_router(self.router[-1]  , preferred='semantic')\n\n\n        self.register_buffer('memory_counts' , torch.zeros(self.num_memories)) # Use to track how many time each memory is used \n        self.register_buffer('total_routed' , torch.tensor(0))  # counter for how many time the router decisions \n\n\n\n\n    def forward(self , x:torch.Tensor , current_context:Optional[torch.Tensor] =  None ) -> Tuple[torch.Tensor , torch.Tensor, dict ]:\n\n       \"\"\"x:input tensor\n        current_context: current context or previous memory\n\n        return memory wights , meta deta, write indices \n       \"\"\"\n       batch_size, seq_len , _ =  x.shape \n\n       route_logits =  self.router(self.dropout(x)) /  self.temprature \n       memory_weights =  F.softmax(route_logits , dim=-1)\n\n     \n\n    \n       if self.training:\n           self._update_stats(memory_weights)\n           capacity_utilization  = self.memory_counts / self.total_routed \n           memory_weights = memory_weights * (1.0-capacity_utilization)\n       meta_data = {\n           'usage':self._get_memory_usage(), \n           'weights':memory_weights.mean(dim= (0,1)).detach(),\n       }\n\n       return memory_weights , meta_data \n     \n\n    def _update_stats(self, memories_weights: torch.Tensor):\n        decisions =  torch.argmax(memories_weights , dim=-1).flatten()\n        counts  =  torch.bincount(decisions ,minlength=self.num_memories)\n        self.memory_counts += counts \n        self.total_routed += decisions.numel()\n\n    def _get_memory_usage(self):\n        if self.total_routed == 0:\n            return {name:0.0 for name in self.memories_names}\n        usage = self.memory_counts /  self.total_routed\n        return {name:usage[i].item() for i , name in enumerate (self.memories_names)}\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:14.806251Z","iopub.execute_input":"2025-04-26T04:59:14.806563Z","iopub.status.idle":"2025-04-26T04:59:14.819525Z","shell.execute_reply.started":"2025-04-26T04:59:14.806537Z","shell.execute_reply":"2025-04-26T04:59:14.818685Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nclass EpisodicMemoryCell(nn.Module):\n    def __init__(self, episodic_memory_dim, input_dim, num_slots=100, comp_ratio=0.25, memory_compression_ratio:int =0.5 , temp=0.1 , apply_compression= True ):\n        super().__init__()\n        self.num_slots_dim = num_slots\n        self.temp = temp\n        self.episodic_memory_dim =  episodic_memory_dim\n        self.comp_dim = int(episodic_memory_dim * comp_ratio)\n        self.memory_compression_dim = int(episodic_memory_dim * memory_compression_ratio)\n        # Learnable initial memory\n        self.register_buffer(\"access_count\", torch.zeros(num_slots))\n        self.register_buffer(\"last_accessed\", torch.zeros(num_slots))\n        self.register_buffer(\"memory_strength\", torch.ones(num_slots))\n        self.register_buffer(\"slot_importance\", torch.zeros(num_slots))\n        self.episodic_memory =  nn.Parameter(torch.randn(self.num_slots_dim , episodic_memory_dim))\n        self.num_slots = num_slots \n        self.register_buffer(\"episodic_memory_age\", torch.zeros(num_slots))\n        self.fill_count = 0 \n        self.memory_compress = nn.Sequential(\n            nn.Linear(episodic_memory_dim, self.memory_compression_dim),\n            nn.RMSNorm(self.memory_compression_dim),\n            nn.GELU(),\n            nn.Linear(self.memory_compression_dim, episodic_memory_dim)\n        )\n        self.alpha = nn.Parameter(torch.ones(1)) \n        self.compress = nn.Sequential(\n            nn.Linear(input_dim, episodic_memory_dim),\n            nn.RMSNorm(episodic_memory_dim),\n            nn.GELU(),\n            nn.Linear(episodic_memory_dim, self.comp_dim)\n        )\n        self.decompress = nn.Sequential(\n            nn.Linear(self.comp_dim, episodic_memory_dim),\n            nn.RMSNorm(episodic_memory_dim),\n            nn.GELU()\n        )\n        self.write_gate = nn.Sequential(\n            nn.Linear(self.comp_dim+ episodic_memory_dim, 1),\n            nn.Sigmoid()\n        )\n        self.apply_flag =  apply_compression \n        self.memory_projection = nn.Linear(self.episodic_memory_dim , input_dim)\n        self.W4 = nn.Parameter(torch.randn( episodic_memory_dim , self.comp_dim))\n\n    def forward(self, x: torch.Tensor, prev_memory: torch.Tensor = None, memory_recall:bool =  True):\n        bs, s, _ = x.shape\n        compressed_x = self.compress(x)\n\n        if prev_memory is None:\n            prev_memory , prev_memory_attn_weights  = self.retrive_prev_experience(x=x)\n\n        cell_state = torch.sigmoid(prev_memory + F.linear(compressed_x, self.W4))\n\n        cell_state = F.layer_norm(cell_state, (self.episodic_memory_dim,))\n\n\n        mem_exp = self.episodic_memory.unsqueeze(0).expand(bs, -1, -1)\n\n    \n        comp_exp = compressed_x.unsqueeze(1).expand(-1, self.num_slots, -1, -1).mean(dim=2) \n        gate_input = torch.cat([mem_exp, comp_exp], dim=-1)  \n\n        gate = self.write_gate(gate_input).squeeze(-1)\n        with torch.no_grad():\n            self.episodic_memory_age.data = 0.9 * self.episodic_memory_age + 0.1 *  torch.ones_like(self.episodic_memory_age)\n        decay = torch.exp(-self.episodic_memory_age / self.temp).unsqueeze(0)\n        write_weights = F.softmax((gate * decay) / self.temp, dim=-1)\n        cell_state_mean = cell_state.mean(dim=1)\n\n        with torch.no_grad():\n            memory_target =  torch.einsum(\n\n                'bs,bd->sd' , write_weights , cell_state_mean\n            )\n           \n        delta =  memory_target - self.episodic_memory  \n        learned_gate = gate.mean(dim=0 ).unsqueeze(-1)\n        update_memory=  self.episodic_memory +self.alpha + learned_gate * delta\n\n        with torch.no_grad():\n            # Replace in-place update with cloned copy\n            new_memory = self.episodic_memory.data.clone()\n            new_memory =new_memory +  self.alpha + learned_gate * delta\n            self.episodic_memory.data.copy_(new_memory)\n        return   self.memory_projection(prev_memory)\n    def retrive_prev_experience(self, x:torch.Tensor):\n        compress_x =  self.compress(x)\n        decompress_x =  self.decompress(compress_x)\n        query  =  F.normalize(decompress_x, dim=-1)\n\n        memory = F.normalize(self.episodic_memory , dim=-1)\n        attn_scores = torch.matmul(query,memory.T) / self.temp \n\n        attn_weights =  F.softmax(attn_scores , dim=-1)        \n        \n        attended_memory = torch.matmul(attn_weights, memory)  # (B, S, D)\n        return attended_memory , attn_weights \n\n\n    def get_memory(self):\n        return self.episodic_memory\n    \n    def is_full(self):\n        return self.fill_count >= self.num_slots\n \n    def replay(self, top_k=5, compress=True):\n        \n        # Get top-k important memory indices (e.g., based on slot importance or access_count)\n        importance_scores = self.slot_importance if hasattr(self, \"slot_importance\") else self.access_count\n        topk_indices = torch.topk(importance_scores, k=top_k, largest=True).indices\n    \n        # Select memories\n        selected_memories = self.episodic_memory[topk_indices]  # Shape: (top_k, episodic_memory_dim)\n    \n        # # Optionally compress replayed memories\n        # if compress:\n        #     compressed = self.compress(selected_memories.unsqueeze(0))  # Add batch dim\n        #     print(compressed.shape)\n        #     return compressed.squeeze(0)  # Shape: (top_k, compressed_dim)\n        # else:\n        #     return selected_memories  # Shape: (top_k, episodic_memory_dim)\n\n        return selected_memories\n\n\n\n    def memory_regularization_loss(self, lambda_reg=0.01):\n        \"\"\" Apply regularization on memory to prevent overfitting. \"\"\"\n        norm_loss = torch.mean(torch.norm(self.episodic_memory, p=2))  # Regularization on key memory\n        return lambda_reg * norm_loss\n    def _update_memory_metrics(self, write_weights, attn_weights=None):\n        \"\"\"Update memory usage statistics\"\"\"\n        # Track write operations\n        self.access_count += write_weights.sum(dim=0)\n        self.last_accessed += 1\n        self.last_accessed[write_weights.sum(dim=0) > 0] = 0  # Reset counter for accessed slots\n        \n        # Update memory strength (exponential decay)\n        self.memory_strength = 0.9 * self.memory_strength + 0.1 * write_weights.mean(dim=0)\n        \n        # Track read operations if available\n        if attn_weights is not None:\n            read_counts = attn_weights.sum(dim=[0,1])  # Sum across batch and sequence\n            self.access_count += read_counts\n            self.slot_importance = 0.95 * self.slot_importance + 0.05 * (write_weights + read_counts).mean(dim=0)\n\n    def get_memory_metrics(self):\n        \"\"\"Return comprehensive memory statistics\"\"\"\n        return {\n            # Basic stats\n            'total_accesses': self.access_count.sum().item(),\n            'mean_access_count': self.access_count.float().mean().item(),\n            'most_used_slot': self.access_count.argmax().item(),\n            'least_used_slot': self.access_count.argmin().item(),\n            \n            # Temporal stats\n            'mean_age': self.episodic_memory_age.float().mean().item(),\n            'oldest_slot': self.episodic_memory_age.argmax().item(),\n            \n            # Memory quality metrics\n            'mean_memory_strength': self.memory_strength.mean().item(),\n            'importance_distribution': self.slot_importance.detach(),\n            'active_slots': int((self.memory_strength > 0).sum().item()) ,\n            # Diversity metrics\n            'memory_similarity': self._calculate_memory_similarity(),\n            'unique_memories': self._calculate_unique_memories()\n        }\n\n    def _calculate_memory_similarity(self):\n        \"\"\"Calculate cosine similarity between memory slots\"\"\"\n        with torch.no_grad():\n            norms = F.normalize(self.episodic_memory, p=2, dim=-1)\n            similarity = torch.mm(norms, norms.T)\n            return {\n                'mean_similarity': similarity.mean().item(),\n                'max_similarity': similarity.max().item(),\n                'min_similarity': similarity.min().item()\n            }\n\n    def _calculate_unique_memories(self, threshold=0.8):\n        \"\"\"Count unique memories based on similarity threshold\"\"\"\n        with torch.no_grad():\n            norms = F.normalize(self.episodic_memory, p=2, dim=-1)\n            similarity = torch.mm(norms, norms.T)\n            unique_count = (similarity < threshold).sum(dim=1).gt(0).sum()\n            return unique_count.item()\n\n    def memory_health_check(self):\n        \"\"\"Diagnostic check for memory system\"\"\"\n        metrics = self.get_memory_metrics()\n        print(metrics)\n        return {\n            'potential_issues': [\n                (\"High similarity\", metrics['memory_similarity']['mean_similarity'] > 0.75),\n                (\"Low uniqueness\", metrics['unique_memories'] < self.num_slots//2),\n                (\"Memory leakage\", metrics['total_accesses'] > 1e6)\n            ],\n            'recommended_actions': [\n                (\"Consolidate memories\", metrics['memory_similarity']['mean_similarity'] > 0.75),\n                (\"Increase diversity\", metrics['unique_memories'] < self.num_slots//3),\n                (\"Purge old memories\", metrics['mean_age'] > 1000)\n            ]\n        }\n\n    def reset_usage_stats(self):\n        \"\"\"Reset transient statistics while preserving memories\"\"\"\n        self.access_count.fill_(0)\n        self.last_accessed.fill_(0)\n        self.slot_importance.fill_(0)\n    def prune_infrequent(self , access_threshold:int = 10):\n        infrequent_mask  = self.access_count  < access_threshold \n    \n        self.episodic_memory.data[infrequent_mask] = 0\n        self.episodic_memory_age +=1 \n        self.access_count[infrequent_mask] = 0\n\n    \n    def episodic_loss(self, lambda_sparsity=0.1, lambda_stability=0.5):\n        \"\"\"\n        Combines:\n        - Memory regularization\n        - Access sparsity\n        - Temporal stability\n        - Slot diversity\n        \"\"\"\n        # Base regularization (existing)\n        reg_loss = self.memory_regularization_loss()\n        \n        # Access pattern sparsity (encourage efficient slot usage)\n        access_prob = F.softmax(self.access_count.float(), dim=0)\n        sparsity_loss = lambda_sparsity * torch.sum(access_prob * torch.log(access_prob + 1e-8))\n        \n        # Temporal stability (prevent catastrophic forgetting)\n        age_weight = torch.sigmoid(-self.episodic_memory_age/self.max_age)\n        stability_loss = lambda_stability * torch.mean(\n            age_weight * torch.norm(self.episodic_memory - self.episodic_memory.detach(), dim=-1)\n        )\n        \n        # Slot diversity (anti-collapse)\n        similarity = F.cosine_similarity(\n            self.episodic_memory.unsqueeze(1),\n            self.episodic_memory.unsqueeze(0),\n            dim=-1\n        )\n        diversity_loss = torch.mean(torch.triu(similarity, diagonal=1)**2)\n        \n        return reg_loss + sparsity_loss + stability_loss + diversity_loss\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T04:59:16.636759Z","iopub.execute_input":"2025-04-26T04:59:16.637168Z","iopub.status.idle":"2025-04-26T04:59:16.661173Z","shell.execute_reply.started":"2025-04-26T04:59:16.637142Z","shell.execute_reply":"2025-04-26T04:59:16.660419Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nclass BioSemanticMemoryCell(nn.Module):\n    def __init__(self, semantic_memory_dim: int, input_dim: int, num_concepts: int = 1000,\n                 comp_ratio: float = 0.25, temp: float = 0.1, top_k: int = 5, expand_rate: float = 0.1):\n        super().__init__()\n        self.semantic_memory_dim = semantic_memory_dim\n        self.input_dim = input_dim\n        self.num_concepts = num_concepts\n        self.temp = temp\n        self.top_k = top_k\n        self.expand_rate = expand_rate\n        self.comp_dim = int(semantic_memory_dim * comp_ratio)\n        self.register_buffer('memory_age', torch.zeros(num_concepts))\n        self.register_buffer('access_count', torch.zeros(num_concepts))\n        self.register_buffer('concept_energy', torch.ones(num_concepts))\n        self.energy_threshold = nn.Parameter(torch.tensor(0.1))\n        self.max_age = 1000\n        self.age_embedding = nn.Embedding(self.max_age, semantic_memory_dim)\n        \n        # Memory importance estimation\n        self.importance_net = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        \n        # Adaptive forgetting parameters\n        self.decay_rate = nn.Parameter(torch.tensor(0.9))\n        self.consolidation_threshold = 0.7\n        \n        # Pre-allocate memory with buffer\n        self.key_memory  =  nn.Parameter(torch.randn(num_concepts , semantic_memory_dim))\n        self.value_memory  =  nn.Parameter(torch.randn(num_concepts , semantic_memory_dim))\n        self.cell_state  =  nn.Parameter(torch.randn(num_concepts , semantic_memory_dim))\n        nn.init.xavier_normal_(self.key_memory)\n        nn.init.xavier_normal_(self.value_memory)\n        nn.init.xavier_normal_(self.cell_state)  # Initialize cell_state similarly\n\n\n        # Compression network with optimized layers\n        self.compression = nn.Sequential(\n            nn.Linear(input_dim, semantic_memory_dim),\n            nn.LayerNorm(semantic_memory_dim),\n            nn.GELU(),\n            nn.Linear(semantic_memory_dim, self.comp_dim),)\n\n        # Vectorized gating mechanisms\n        self.update_gate = nn.Sequential(\n            nn.Linear(3 * semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        self.age = nn.Parameter(torch.zeros(num_concepts, dtype=torch.long), requires_grad=False)\n        self.memory_projection = nn.Linear(self.semantic_memory_dim, self.input_dim)\n\n        # Shared forget gate parameters\n        self.forget_gate = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 3),  # 3 outputs for cell/key/value\n            nn.Sigmoid()\n        )\n\n        # Projection with pre-allocated weights\n        self.W_cell = nn.Linear(self.comp_dim, semantic_memory_dim, bias=False)\n        self.synaptic_scale = nn.Parameter(torch.tensor(0.1))\n        \n        # Memory management\n        self.sparsity = nn.Parameter(torch.tensor(0.5))\n        self.register_buffer('usage', torch.zeros(num_concepts))\n        # self.register_buffer('active_concepts', torch.tensor(num_concepts))\n\n\n    def forward(self, x: torch.Tensor, training=False):\n        bs , seq_len , _ =  x.shape    \n        compressed = self.compression(x.mean(dim=1))\n        projected = self.W_cell(compressed)\n        self.active_concepts = int(self.num_concepts * self.sparsity)\n\n        # Compute similarity scores\n        age_idx = self.memory_age[:self.active_concepts].clamp(0, self.max_age - 1).long()\n        age_embed = self.age_embedding(age_idx)  # [C, D]\n\n        # Add age signal to key\n        age_enriched_key = self.key_memory[:self.active_concepts] + age_embed[:self.active_concepts]\n        \n        # Dot similarity with age-aware key\n        sim = torch.mm(projected, age_enriched_key.T) / self.temp\n        # sim = torch.matmul(projected, age_enriched_key.transpose(-1, -2)) / self.temp\n\n        \n        # Top-K selection\n        topk_scores, topk_idx = torch.topk(sim, self.top_k, dim=1)\n        self._update_energy_levels()\n        \n        # Memory interaction\n        batch_indices = torch.arange(x.size(0), device=x.device)[:, None]\n        keys = self.key_memory[topk_idx]\n        values = self.value_memory[topk_idx]\n        cells = self.cell_state[topk_idx]\n        self._update_memory_metadata(topk_idx)\n        self.usage[topk_idx] = (self.usage[topk_idx] + 1).clamp(max=1)\n\n        # Vectorized gating\n        gate_input = torch.cat([\n            keys,\n            cells,\n            projected.unsqueeze(1).expand(-1, self.top_k, -1)\n        ], dim=-1)\n        update_gates = self.update_gate(gate_input.view(-1, 3 * self.semantic_memory_dim))\n        update_gates = update_gates.view(-1, self.top_k)\n\n        # Efficient cell updates\n        decay_factor = self._adaptive_decay(topk_idx)\n        decay_factor = decay_factor.unsqueeze(-1)  # Shape: [16, 5, 1]\n  \n        cell_updates = decay_factor * torch.sigmoid(cells + projected.unsqueeze(1))\n        delta = update_gates.unsqueeze(-1) * cell_updates\n        \n        if self.training:\n            self._consolidate_important_memories()\n\n        with torch.no_grad():\n            # Replace in-place updates with cloned copies\n            new_cell = self.cell_state.data.clone()\n            new_key = self.key_memory.data.clone()\n            new_value = self.value_memory.data.clone()\n            \n            # Update clones instead of original parameters\n            new_cell[topk_idx] += (delta * self.synaptic_scale).mean(0)\n            new_key[topk_idx] += F.normalize(cells, dim=-1)\n            new_value[topk_idx] += F.normalize(cells, dim=-1)\n            \n            # Copy back to parameters\n            self.cell_state.data.copy_(new_cell)\n            self.key_memory.data.copy_(new_key)\n            self.value_memory.data.copy_(new_value)\n            \n        # Normalize using out-of-place operations\n        self.key_memory.data[topk_idx] = F.normalize(self.key_memory.data[topk_idx], dim=-1)\n        self.value_memory.data[topk_idx] = F.normalize(self.value_memory.data[topk_idx], dim=-1)\n        # Output with fused operations\n        output = (values * F.softmax(topk_scores, dim=1).unsqueeze(-1)).sum(1)\n\n        # Conditional expansion\n        \n        if self.training:\n            self._consolidate_important_memories()\n        if  self._needs_expansion():\n            self._expand_memory()\n        output = output.unsqueeze(1)\n        output = output.repeat(1, seq_len , 1)\n        output = self.memory_projection(output)\n        self.age += 1\n        return output\n\n    def _consolidate_important_memories(self):\n        consolidation_factor = torch.where(self.concept_energy > self.consolidation_threshold,\n                                           torch.tensor(1.2),\n                                           torch.tensor(1.0))\n        self.key_memory *= consolidation_factor.unsqueeze(-1)\n        self.value_memory *= consolidation_factor.unsqueeze(-1)\n    def _adaptive_decay(self, memory_idx):\n        # Calculate the adaptive decay based on importance (energy and access count)\n        energy = self.concept_energy[memory_idx]\n        access = self.access_count[memory_idx]\n        \n        # The decay is stronger for less accessed and less energetic memories\n        decay = torch.exp(-self.decay_rate * (1 - energy) * (1 - access))\n        \n        return decay\n    def _update_energy_levels(self):\n        \"\"\"Energy-based memory retention\"\"\"\n        # Calculate memory importance using the importance network\n        importance = self.importance_net(self.key_memory).squeeze()  # [batch_size, num_concepts]\n\n        # Energy decay with importance factor and previous energy levels\n        self.concept_energy = torch.clamp(\n            self.concept_energy * self.decay_rate + importance * 0.1,  # Decay + importance influence\n            0, 1  # Bound energy between 0 and 1\n        )\n\n        # Identify low-energy memories\n        deactivated = self.concept_energy < 0.1  # Memory slots with energy below threshold\n\n        # Update active concepts count (deactivate low-energy ones)\n        self.active_concepts = self.active_concepts - deactivated.sum()\n\n        # Update memory values in-place using .data (avoid reassigning nn.Parameter directly)\n        with torch.no_grad():\n            self.key_memory.data[deactivated] = 0\n            self.value_memory.data[deactivated] = 0\n            self.cell_state.data[deactivated] = 0\n\n        # Optionally, if you want to keep track of the inactive ones for future use:\n        self.inactive_concepts = deactivated.sum()\n\n\n    def purge_inactive_memories(self):\n        # Use registered buffers properly\n        active_mask = self.usage > self.threshold\n        self.key_memory = self.key_memory[active_mask]\n        self.value_memory = self.value_memory[active_mask]\n        self.cell_state = self.cell_state[active_mask]\n        self.active_concepts = active_mask.sum()\n        def _expand_memory(self):\n            \"\"\"Efficient memory expansion with pre-allocation\"\"\"\n            new_size = int(self.num_concepts * (1 + self.expand_rate))\n            if new_size > self.key_memory.size(0):\n                # Double memory allocation for amortized O(1) expansion\n                new_size = max(new_size, self.key_memory.size(0) * 2)\n                self._reallocate_memory(new_size)\n            \n            self.active_concepts = int(self.active_concepts * (1 + self.expand_rate))\n\n    def _reallocate_memory(self, new_size):\n        \"\"\"Efficient memory reallocation\"\"\"\n        new_key = torch.empty(new_size, self.semantic_memory_dim, device=self.key_memory.device)\n        new_value = torch.empty_like(new_key)\n        new_cell = torch.zeros_like(new_key)\n        \n        new_key[:self.key_memory.size(0)] = self.key_memory\n        new_value[:self.value_memory.size(0)] = self.value_memory\n        new_cell[:self.cell_state.size(0)] = self.cell_state\n        \n        self.key_memory = new_key\n        self.value_memory = new_value\n        self.cell_state = new_cell\n\n    def _needs_expansion(self):\n        \"\"\"Efficient expansion check\"\"\"\n        return (self.usage[:self.active_concepts] > 0.8).float().mean() > 0.5\n\n    def get_memory(self):\n        return self.key_memory[:self.active_concepts], \\\n               self.value_memory[:self.active_concepts], \\\n               self.cell_state[:self.active_concepts]\n\n    def _update_memory_metadata(self, used_indices):\n        \"\"\"Track memory usage patterns\"\"\"\n        # Update access statistics\n        self.access_count[used_indices] += 1\n        \n        # Update memory age (non-accessed memories age faster)\n        self.memory_age += 1\n        self.memory_age[used_indices] *= 0.5\n        \n        # Update energy levels\n        self.concept_energy[used_indices] += 0.1\n        self.concept_energy = torch.clamp(self.concept_energy * 0.95, 0, 1)\n    def _consolidate_important_memories(self):\n        \"\"\"Memory consolidation mechanism\"\"\"\n        # Find important memories\n        importance = self.importance_net(self.key_memory).squeeze()\n        consolidate_mask = importance > self.consolidation_threshold\n        \n        # Stabilize important memories\n        if consolidate_mask.any():\n            self.key_memory.data[consolidate_mask] = F.normalize(\n                self.key_memory[consolidate_mask], dim=-1\n            )\n            self.value_memory.data[consolidate_mask] = (\n                0.9 * self.value_memory[consolidate_mask] + \n                0.1 * self.value_memory[consolidate_mask].mean(dim=0)\n            )\n    def get_memory_metrics(self):\n        \"\"\"Return memory health statistics\"\"\"\n        return {\n            'active_concepts': self.active_concepts.item(),\n            'energy_mean': self.concept_energy.mean().item(),\n            'access_count': self.access_count.sum().item(),\n            'age_distribution': torch.histc(self.memory_age, bins=10),\n            'importance_distribution': self.importance_net(self.key_memory).detach()\n        }\n    \n    def replay(self, n_samples):\n        \"\"\"Periodically replay important memories\"\"\"\n        idx = torch.topk(self.usage, n_samples).indices\n        return self.key_memory[idx], self.value_memory[idx]\n\n\n    def semantic_loss(self, lambda_energy=0.3, lambda_semantic=0.2):\n        \"\"\"\n        Combines:\n        - Energy maintenance\n        - Semantic coherence\n        - Importance sparsity\n        - Adaptive consolidation\n        \"\"\"\n        # Energy preservation (prevent premature forgetting)\n        energy_loss = lambda_energy * torch.mean(\n            (self.concept_energy - self.energy_threshold).clamp(min=0)**2\n        )\n        \n        # Semantic coherence (cluster similar concepts)\n        key_norm = F.normalize(self.key_memory, dim=-1)\n        value_norm = F.normalize(self.value_memory, dim=-1)\n        alignment_loss = torch.mean(1 - (key_norm * value_norm).sum(dim=-1))\n        \n        # Importance sparsity (focus on key concepts)\n        importance = self.importance_net(self.key_memory).squeeze()\n        sparsity_loss = torch.mean(importance**2)\n        \n        # Consolidation stability\n        consolidated = (self.concept_energy > self.consolidation_threshold).float()\n        consolidation_loss = torch.mean(\n            consolidated * torch.norm(self.key_memory - self.key_memory.detach(), dim=-1)\n        )\n        \n        return energy_loss + alignment_loss + sparsity_loss + consolidation_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:39:20.569237Z","iopub.execute_input":"2025-04-26T05:39:20.569567Z","iopub.status.idle":"2025-04-26T05:39:20.597535Z","shell.execute_reply.started":"2025-04-26T05:39:20.569543Z","shell.execute_reply":"2025-04-26T05:39:20.596720Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n# 1. Instantiate your refactored cell\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncell = BioSemanticMemoryCell(\n    semantic_memory_dim=64,\n    input_dim=64,\n    num_concepts=100,\n    comp_ratio=0.25,\n    temp=0.1,\n    top_k=5,\n    expand_rate=0.1,\n    usage_threshold=0.1\n).to(device)\ncell.train()\n\n# Helper to get (top-slot idx, L2-distance) for a given input\ndef top_idx_and_dist(x):\n    comp = cell.compression(x.mean(1))\n    q = cell.W_cell(comp)[0]\n    keys, _, _ = cell.get_memory()\n    usage = cell.access_count\n    idx = usage.argmax().item()\n    dist = (q - keys[idx]).norm().item()\n    return idx, dist\n\n# ——————————————————————————————\n# 2. Single-concept retention test\nx = torch.randn(1, 10, 64, device=device) + 3.0\nidx0, d0 = top_idx_and_dist(x)\n\noptimizer = torch.optim.Adam(cell.parameters(), lr=1e-4)\nfor _ in range(200):\n    optimizer.zero_grad()\n    _ = cell(x, training=True)\n    comp = cell.compression(x.mean(1))\n    q = cell.W_cell(comp)[0]\n    keys, _, _ = cell.get_memory()\n    loss = (q - keys[idx0]).pow(2).sum()\n    loss.backward()\n    optimizer.step()\n\nidx1, d1 = top_idx_and_dist(x)\nprint(f\"Retention: init dist={d0:.4f}, final dist={d1:.4f}\")\nassert d1 < 0.5 * d0, \"Retention failed!\"\n\n# ——————————————————————————————\n# 3. Two-concept interference test\nxA = torch.randn(1, 10, 64, device=device) + 5.0\nxB = torch.randn(1, 10, 64, device=device) - 5.0\nidxA0, dA0 = top_idx_and_dist(xA)\nidxB0, dB0 = top_idx_and_dist(xB)\n\n# Train A then B\nfor _ in range(100):\n    _ = cell(xA, training=True)\nfor _ in range(100):\n    _ = cell(xB, training=True)\n\nidxA1, dA1 = top_idx_and_dist(xA)\nidxB1, dB1 = top_idx_and_dist(xB)\nprint(f\"Interference A: init dist={dA0:.4f}, final dist={dA1:.4f}\")\nprint(f\"Interference B: init dist={dB0:.4f}, final dist={dB1:.4f}\")\nassert dA1 < 0.7 * dA0, \"Cluster A overwritten!\"\nassert dB1 < 0.7 * dB0, \"Cluster B not retained!\"\n\n# ——————————————————————————————\n# 4. Forgetting-curve test\nM = 200\ndistances = []\n# Use x again and the same idx0 slot\nfor step in range(M + 1):\n    _, d = top_idx_and_dist(x)\n    distances.append(d)\n    filler = torch.randn(1, 10, 64, device=device)\n    _ = cell(filler, training=False)\n\n# Plot\nplt.plot(distances)\nplt.xlabel(\"Decay steps\")\nplt.ylabel(\"Distance to original slot\")\nplt.title(\"Forgetting curve\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:39:22.755674Z","iopub.execute_input":"2025-04-26T05:39:22.755967Z","iopub.status.idle":"2025-04-26T05:39:22.801560Z","shell.execute_reply.started":"2025-04-26T05:39:22.755944Z","shell.execute_reply":"2025-04-26T05:39:22.800417Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-4aa557c53ee7>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-918255f6631a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# 2. Select active slots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_concepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_concepts\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musage\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_concepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_age\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_concepts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_age\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mage_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'active' is not defined"],"ename":"NameError","evalue":"name 'active' is not defined","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:32:42.522989Z","iopub.execute_input":"2025-04-26T05:32:42.523296Z","iopub.status.idle":"2025-04-26T05:32:54.381065Z","shell.execute_reply.started":"2025-04-26T05:32:42.523273Z","shell.execute_reply":"2025-04-26T05:32:54.380213Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"class MemorySystem(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        \n        # Initialize memory components\n        self.memory_writer = MemorySummaryWriter(\n            hidden_dim=cfg['emb_dim'],\n            compress_dim=cfg['compress_dim'],\n            num_heads=cfg['memory_router_head']\n        )\n        self.memory_router = MemoryRouter(\n            input_dim=cfg['emb_dim'],\n            temperature=cfg['temperature'],\n            dropout=cfg['drop_rate']\n        )\n        self.consalidate_dim =  nn.Linear(cfg['episodic_memory_dim'] ,  cfg['semantic_memory_dim'])\n        # Memory subsystems\n        self.episodic_memory = EpisodicMemoryCell(\n            episodic_memory_dim=cfg['episodic_memory_dim'],\n            input_dim=cfg['emb_dim'],\n            num_slots=cfg['num_slots'],\n            comp_ratio=cfg['comp_ratio'],\n            memory_compression_ratio=cfg['memory_compression_ratio'],\n            temp=cfg['epic_temp'],\n            apply_compression=cfg['apply_compression']\n        )\n        \n        self.semantic_memory = BioSemanticMemoryCell(\n            semantic_memory_dim=cfg['semantic_memory_dim'],\n            input_dim=cfg['emb_dim'],\n            num_concepts=cfg['num_concepts'],\n            comp_ratio=cfg['semantic_comp_ratio'],\n            temp=cfg['semantic_temp'],\n            top_k=cfg['top_k'],\n            expand_rate=cfg['expand_rate']\n        )\n        self.usage_history = []\n        self.overflow_threshold = cfg.get('overflow_threshold', 0.9)\n        # Adaptive gating\n        self.gate_norm = nn.LayerNorm(cfg['emb_dim'])\n        self.temperature = nn.Parameter(torch.tensor(1.0))\n        self.output_projection = nn.Linear(cfg['compress_dim'], cfg['emb_dim'])\n        # In MemorySystem __init__:\n    def forward(self, x: torch.Tensor):\n        bs, seq_len, dim = x.shape\n        \n        # Memory routing\n        router_weights, meta_data = self.memory_router(x)\n        router_weights = F.softmax(router_weights / (self.temperature + 1e-6), dim=-1)\n        \n        # Memory writing\n        summary_out, _ = self.memory_writer(x)\n        summary_out = self.output_projection(summary_out)\n        summary_out = summary_out.unsqueeze(1).expand(-1, seq_len, -1)\n        \n        # Memory retrieval\n        episodic_out = self.episodic_memory(summary_out)\n        semantic_out = self.semantic_memory(summary_out)\n        \n        # Memory consolidation check\n        if self.episodic_memory.is_full():\n            self._consolidate_memory()\n            \n        # Adaptive memory fusion\n        episodic_gate = router_weights[..., 0].unsqueeze(-1)\n        semantic_gate = router_weights[..., 1].unsqueeze(-1)\n        \n        combined = self.gate_norm(\n            episodic_gate * episodic_out + \n            semantic_gate * semantic_out\n        )\n        current_usage = self.episodic_memory.get_memory_metrics()['active_slots'] / self.cfg['num_slots']\n        if current_usage > self.overflow_threshold:\n            self._adaptive_consolidation()\n        \n        return semantic_out , episodic_out , combined\n\n    def _consolidate_memory(self):\n        \"\"\"Transfer knowledge from episodic to semantic memory\"\"\"\n        with torch.no_grad():\n            episodic_data = self.episodic_memory.replay(\n                top_k=self.cfg['consolidation_top_k'],\n                compress=True\n            )\n            self.semantic_memory(episodic_data.unsqueeze(1))\n    def _adaptive_consolidation(self):\n            \"\"\"Smart consolidation based on memory content\"\"\"\n            # Preserve important memories\n            importance = self.episodic_memory.slot_importance\n            preserve_mask = importance > torch.quantile(importance, 0.8)\n            \n            # Consolidate less important memories\n            consolidate_data = self.episodic_memory.replay(\n                top_k=int(self.cfg['num_slots'] * 0.2),\n                compress=True\n            )\n            consolidate_data =  self.consalidate_dim(consolidate_data) \n\n            self.semantic_memory(consolidate_data.unsqueeze(1))\n            \n            # Reset consolidated slots\n        \n            self.episodic_memory.prune_infrequent(\n                access_threshold=self.cfg['episodic_prune_thresh'] * 2\n            )\n    def _consolidate_memory(self):\n         \"\"\"Transfer knowledge from episodic to semantic memory\"\"\"\n         with torch.no_grad():\n            episodic_data = self.episodic_memory.replay(\n                top_k=self.cfg['consolidation_top_k'],\n                compress=True\n            )\n            \n            # Add dimension alignment projection\n            consolidation_proj = nn.Linear(\n                episodic_data.size(-1),\n                self.cfg['emb_dim']\n            ).to(episodic_data.device)\n            projected_data = consolidation_proj(episodic_data)\n            \n            # Use proper sequence dimensions\n            self.semantic_memory(projected_data.unsqueeze(1))\n    def maintain_memory(self):\n        \"\"\"Perform routine memory maintenance\"\"\"\n        # Episodic maintenance\n        self.episodic_memory.prune_infrequent(\n            access_threshold=self.cfg['episodic_prune_thresh']\n        )\n        \n        # Semantic maintenance\n        self.semantic_memory.purge_inactive_memories()\n        \n        return {\n            'episodic': self.episodic_memory.get_memory_metrics(),\n            'semantic': self.semantic_memory.get_memory_metrics()\n        }\n\n    def memory_regularization_loss(self):\n        \"\"\"Combine memory regularization terms\"\"\"\n        return (\n            self.cfg['episodic_reg_weight'] * self.episodic_memory.memory_regularization_loss() +\n            self.cfg['semantic_reg_weight'] * self.semantic_memory.memory_regularization_loss()\n            \n        )\n    \n    def memory_loss(self, \n                       episodic_weight=1.0, \n                       semantic_weight=0.7,\n                       temp_scale=0.1):\n            \"\"\"\n            Unified memory loss with temperature-scaled balancing\n            \"\"\"\n            # Component losses\n            episodic_loss = self.episodic_memory.episodic_loss()\n            semantic_loss = self.semantic_memory.semantic_loss()\n            \n            # Adaptive temperature scaling\n            t = torch.sigmoid(self.temperature) * temp_scale\n            balance = torch.softmax(torch.stack([episodic_loss/t, semantic_loss/t]), dim=0)\n            \n            # Final weighted loss\n            return (\n                episodic_weight * balance[0] * episodic_loss +\n                semantic_weight * balance[1] * semantic_loss\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:44.751410Z","iopub.execute_input":"2025-04-26T05:01:44.751709Z","iopub.status.idle":"2025-04-26T05:01:44.765220Z","shell.execute_reply.started":"2025-04-26T05:01:44.751687Z","shell.execute_reply":"2025-04-26T05:01:44.764279Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Transformer Block","metadata":{}},{"cell_type":"code","source":"\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention_V2(\n        d_in=cfg[\"emb_dim\"],\n        d_out=cfg[\"emb_dim\"],\n        context_length=cfg[\"context_length\"],\n        num_heads=cfg[\"n_heads\"],\n        dropout=cfg[\"drop_rate\"],\n        qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n    #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut # Add the original input back\n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut #C\n        return x\n\n\n\nclass TransformerBlock_v2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention =  MultiQueryAttentionBlock(d_model=cfg['emb_dim'], h=cfg['n_heads'] , dropout=cfg['drop_rate'], seq_len=  cfg['context_length'] ,qkv_bias=cfg['qkv_bias'])\n\n        self.feed_forward = FeedForward(cfg)\n\n        self.layernorm1 =  LayerNorm(cfg['emb_dim'])\n    \n        self.layernorm2 =  LayerNorm(cfg['emb_dim'])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x , mask= None):\n\n        attention_output =  self.attention(self.layernorm1(x) , mask =  mask)\n\n        ff_output =  self.feed_forward(self.layernorm2(x))\n\n        return x + self.drop_out(ff_output) + self.drop_out(attention_output)\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.memory = MemorySystem(cfg=cfg)\n        self.feed_forward = FeedForward(cfg=cfg)\n        \n        self.norm1 = LayerNorm(cfg['emb_dim'])\n        self.norm2 = LayerNorm(cfg['emb_dim'])\n        self.norm3 = LayerNorm(cfg['emb_dim'])\n        \n        # Memory gate\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] * 2, cfg['emb_dim']),\n            nn.Sigmoid()\n        )\n        \n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        attn_out = self.attention(self.norm1(x), mask=mask)\n        x = x + self.dropout(attn_out)\n        \n        norm_x = self.norm2(x)\n        epic_out , semantic_out , memory_out = self.memory(norm_x)\n        \n        gate_input = torch.cat([norm_x, memory_out], dim=-1)\n        memory_gate = self.memory_gate(gate_input)\n        x = x + memory_gate * memory_out\n        \n        ff_out = self.feed_forward(self.norm3(x))\n        x = x + self.dropout(ff_out)\n        \n        return x\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg, shared_memory=None):\n        super().__init__()\n        # Core components\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.ffn = FeedForward(cfg=cfg)\n        \n        # Memory system (shared across blocks)\n        self.memory = shared_memory or MemorySystem(cfg=cfg)\n        \n        # Normalization layers\n        self.pre_ln_attn = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_mem = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_ffn = RMSNorm(cfg['emb_dim'])\n        \n        # Adaptive memory gating\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'], 1),\n            nn.Sigmoid()\n        )\n        \n        # Memory residual weights\n        self.mem_alpha = nn.Parameter(torch.tensor(0.5))\n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        # Attention phase\n        resid = x\n        x = self.pre_ln_attn(x)\n        x = resid + self.dropout(self.attention(x, mask=mask))\n        \n        # Memory phase\n        resid_mem = x\n        x_mem = self.pre_ln_mem(x)\n        print('x shape ', x.shape)\n        _, _, memory_out = self.memory(x_mem)\n        \n        # Adaptive gating\n        gate = self.memory_gate(x_mem)\n        x = resid_mem + self.mem_alpha * gate * memory_out\n        \n        # FFN phase\n        resid_ffn = x\n        x = self.pre_ln_ffn(x)\n        x = resid_ffn + self.dropout(self.ffn(x))\n        \n        return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:46.731162Z","iopub.execute_input":"2025-04-26T05:01:46.731437Z","iopub.status.idle":"2025-04-26T05:01:46.745254Z","shell.execute_reply.started":"2025-04-26T05:01:46.731416Z","shell.execute_reply":"2025-04-26T05:01:46.744282Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# GPTQModel","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int, embdding_layer: nn.Embedding):\n        super().__init__()\n        self.weight = embdding_layer.weight  # share weights with input embedding\n        self.bias = nn.Parameter(torch.zeros(vocab_size))  # learnable bias\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\n\n\n\nclass GPTMQModel2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim = cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel1(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks =  nn.ModuleList([\n            TransformerBlockWithMemory(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n        \n        # Shared memory system across layers\n        self.shared_memory = MemorySystem(cfg=cfg)\n        \n        # Transformer blocks with shared memory\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlockWithMemory(\n                cfg=cfg,\n                shared_memory=self.shared_memory if cfg['share_memory'] else None\n            ) for _ in range(cfg['n_layers'])\n        ])\n        \n        # Final projections\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.projection = ProjectionLayer(\n            cfg['emb_dim'], \n            cfg['vocab_size'], \n            self.embedding.embeddings\n        )\n        self.memory_retention_alpha = nn.Parameter(torch.tensor(0.9))\n\n        # Memory loss coefficient\n        self.mem_loss_coef = cfg.get('mem_loss_coef', 0.3)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n        \n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)\n            x = self.memory_retention_alpha * x + (1 - self.memory_retention_alpha) * x.detach()\n            \n        x = self.final_norm(x)\n        logits = self.projection(x)\n        \n        return logits\n    \n    def get_memory_loss(self):\n        \"\"\"Get combined memory regularization loss\"\"\"\n        return self.mem_loss_coef * self.shared_memory.memory_loss()\n    \n    def transformer_parameters(self):\n        return [p for n, p in self.named_parameters() if 'transformer_blocks' in n and p.requires_grad]\n    \n    def memory_parameters(self):\n        return [p for n, p in self.named_parameters() if 'memory_modules' in n and p.requires_grad]\n    \n    def embedding_parameters(self):\n        return [p for n, p in self.named_parameters() if 'embedding' in n and p.requires_grad]\n    \n    def norm_parameters(self):\n        return [p for n, p in self.named_parameters() if 'normalization' in n and p.requires_grad]\n    \n    def output_parameters(self):\n        return [p for n, p in self.named_parameters() if 'output_projection' in n and p.requires_grad]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:47.314074Z","iopub.execute_input":"2025-04-26T05:01:47.314326Z","iopub.status.idle":"2025-04-26T05:01:47.328300Z","shell.execute_reply.started":"2025-04-26T05:01:47.314306Z","shell.execute_reply":"2025-04-26T05:01:47.327596Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"\n\n\n\ndef cal_loss_batch(input_batch , target_batch , model:torch.nn.Module , device:torch.device):\n    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader , model , device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:48.995977Z","iopub.execute_input":"2025-04-26T05:01:48.996267Z","iopub.status.idle":"2025-04-26T05:01:49.002134Z","shell.execute_reply.started":"2025-04-26T05:01:48.996245Z","shell.execute_reply":"2025-04-26T05:01:49.001110Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Text Generation Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\n\n\n\ndef text_to_token_ids(text,  tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\n\ndef token_ids_to_text(tokens , tokenizer):\n    flat  = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n    \ndef generate_and_sample(model  , idx , context_size ,max_new_tokens ):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n            print(logits.shape)\n        logits  = logits[:, -1  , :]\n        print(logits.shape)\n        probs  = torch.softmax(logits  , dim=-1)\n        print(probs)\n        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n        print(idx_next)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx \n\n# def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n#     for _ in range(max_new_tokens):\n#         idx_cond = idx[:, -context_size:]  # shape: [1, current_seq_len]\n\n#         # Create causal mask dynamically\n#         seq_len = idx_cond.size(1)\n#         causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n#         causal_mask = causal_mask.unsqueeze(0)  # [1, seq_len, seq_len]\n\n#         with torch.no_grad():\n#             logits = model(idx_cond, mask=causal_mask)  # <--- pass mask here\n\n#         logits = logits[:, -1, :]  # only take the last token logits\n\n#         # Apply top-k sampling if needed\n#         if top_k is not None:\n#             top_logits, _ = torch.topk(logits, top_k)\n#             min_val = top_logits[:, -1]\n#             logits = torch.where(\n#                 logits < min_val,\n#                 torch.tensor(float('-inf')).to(logits.device),\n#                 logits\n#             )\n\n#         # Temperature sampling\n#         if temperature > 0.0:\n#             logits = logits / temperatureallowed_special=allowed\n#             probs = torch.softmax(logits, dim=-1)\n#             idx_next = torch.multinomial(probs, num_samples=1)\n#         else:\n#             idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n#         idx = torch.cat((idx, idx_next), dim=1)\n#     return idx\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None, end_token_id=None):\n    model.eval()\n    max_seq_len = context_size\n    full_causal_mask = torch.tril(torch.ones(max_seq_len, max_seq_len)).to(idx.device).unsqueeze(0)\n\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = full_causal_mask[:, :seq_len, :seq_len]\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n        logits = logits[:, -1, :]  # Take last token logits\n\n        # Top-k sampling\n        if top_k is not None:\n            top_values, _ = torch.topk(logits, top_k)\n            threshold = top_values[:, -1].unsqueeze(1)\n            logits = logits.masked_fill(logits < threshold, float('-inf'))\n        logits = logits[:, -1, :]\n        logits = logits - logits.max(dim=-1, keepdim=True).values  # Stability trick\n        probs = torch.softmax(logits / temperature, dim=-1)\n        probs = torch.clamp(probs, min=1e-8)  # Prevent zero probs\n        idx_next = torch.multinomial(probs, num_samples=1)\n\n        # logits = logits / temperature\n        # logits = torch.clamp(logits, -100, 100)\n        # probs = torch.softmax(logits, dim=-1)\n\n        # idx_next = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        if end_token_id is not None and idx_next.item() == end_token_id:\n            break\n\n    return idx\n\n\n\n\n\ndef real_time_generation(model, initial_input, context_size, temperature, top_k=None, device=\"cpu\"):\n    # Tokenize the initial input and prepare the model context\n    idx = torch.tensor(initial_input).unsqueeze(0).to(device)  # Assuming initial_input is tokenized\n    \n    print(\"Starting real-time generation...\")\n    \n    # Start generating tokens in real-time\n    for new_token in generate(model, idx, max_new_tokens=50, context_size=context_size, temperature=temperature, top_k=top_k, device=device):\n        print(f\"Generated token: {new_token.item()}\")  # Or decode it back to a word\n        \n        # You can check for user input here and update idx with the new input\n        # For instance, wait for the user to input a prompt to append to the context\n        user_input = input(\"Enter new input (or press enter to continue generation): \")\n        \n        if user_input:\n            # Tokenize the new user input and append it to the context\n            user_input_tokens = torch.tensor(tokenize(user_input)).unsqueeze(0).to(device)\n            idx = torch.cat((idx, user_input_tokens), dim=1)  # Append the new tokens to the context\n        else:\n            # Continue generating if no new user input\n            continue\n\n# Function to tokenize input (adjust depending on your tokenizer)\ndef tokenize(text):\n    # Assuming you have a tokenizer function available\n    return [ord(c) for c in text]  # Dummy example: ord() converts char to token id\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:49.571136Z","iopub.execute_input":"2025-04-26T05:01:49.571439Z","iopub.status.idle":"2025-04-26T05:01:49.583225Z","shell.execute_reply.started":"2025-04-26T05:01:49.571418Z","shell.execute_reply":"2025-04-26T05:01:49.582334Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Dataset and DataLoader ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef generate_prompt(sample):\n    # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n    \n\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer =  tokenizer\n\n        all_tokens = []\n        allowed = {'<|endoftext|>'}\n        for sample in data:\n            prompt = generate_prompt(sample)\n            tokens = tokenizer.encode(prompt , allowed_special=allowed)\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n    return inputs, targets\n\ndef create_dataloader_v1(data, batch_size=4,\n    max_length=256, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n    dataset = Dataset_V1(data, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=collate_fn)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:50.910463Z","iopub.execute_input":"2025-04-26T05:01:50.910785Z","iopub.status.idle":"2025-04-26T05:01:50.956979Z","shell.execute_reply.started":"2025-04-26T05:01:50.910757Z","shell.execute_reply":"2025-04-26T05:01:50.956388Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Dataset And DataLoader for Psycology Dataset ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport tiktoken\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass Dataset_v2(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n        self.input_ids = []\n\n        all_tokens = []\n        for sample in data:\n            tokens = tokenizer.encode(sample)  \n            all_tokens.extend(tokens)\n\n        # Split the tokens into chunks of size max_length with stride\n        for i in range(0, len(all_tokens) - self.max_length, self.stride):\n            input_chunk = all_tokens[i:i + self.max_length]\n            target_chunk = all_tokens[i + 1:i + self.max_length + 1]\n            self.input_ids.append((torch.tensor(input_chunk), torch.tensor(target_chunk)))\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, index):\n        return self.input_ids[index]\n\ndef collect_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0) \n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  \n    return inputs, targets\n\ndef create_dataloader_v2(data, batch_size=4, max_length=1024, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")  \n    dataset = Dataset_v2(data, tokenizer, max_length, stride) \n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=collect_fn)\n    return dataloader\n\ndef load_txt_file(filepath):\n    with open(filepath, 'r') as f:\n        text = f.read()\n    return text\n\ndef split_into_chunks(text, chunk_size=1024, overlap=200):\n\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        chunks.append(chunk)\n\n    return chunks\n\n\n\nfile =  '/kaggle/input/datasetcleaned/cleaned_books.txt'\nload_text =  load_txt_file(file)\nchunk = split_into_chunks(load_text)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-26T05:01:52.292162Z","iopub.execute_input":"2025-04-26T05:01:52.292468Z","iopub.status.idle":"2025-04-26T05:01:54.595350Z","shell.execute_reply.started":"2025-04-26T05:01:52.292441Z","shell.execute_reply":"2025-04-26T05:01:54.594686Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Train Script ","metadata":{}},{"cell_type":"code","source":"\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\n\n\ndef evaluate_model(model, train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,   # Increase generation length if needed\n            context_size=126,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n\n        # Trim everything before the generation\n        generated_only = decoded_text[len(start_context):].strip()\n\n        # Stop at endoftext token if present\n        end_marker = \"<|endoftext|>\"\n        if end_marker in generated_only:\n            generated_only = generated_only.split(end_marker)[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\ndef save_model_checkpoint(model, optimizer, epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch\n    }\n    torch.save(checkpoint, path.format(epoch))\ndef after_save_load():\n    checkpoint = torch.load(\"checkpoint_epoch_7.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n\n\ndef train_model(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n    # scheduler = get_cosine_schedule_with_warmup(optimizer,\n    #                                         num_warmup_steps=500,\n    #                                         num_training_steps=total_steps)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n        save_model_checkpoint(model , optimizer , epoch+1)\n\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:54.596390Z","iopub.execute_input":"2025-04-26T05:01:54.596635Z","iopub.status.idle":"2025-04-26T05:01:56.527959Z","shell.execute_reply.started":"2025-04-26T05:01:54.596591Z","shell.execute_reply":"2025-04-26T05:01:56.527225Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# MemoryGPT Model training  \n","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:56.529106Z","iopub.execute_input":"2025-04-26T05:01:56.529440Z","iopub.status.idle":"2025-04-26T05:01:56.533279Z","shell.execute_reply.started":"2025-04-26T05:01:56.529409Z","shell.execute_reply":"2025-04-26T05:01:56.532380Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torch \nfrom torch.utils.data  import Dataset , DataLoader \nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np \nfrom tqdm import tqdm \nfrom torch.optim import AdamW\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass GPTTrainingPipeline:\n    def __init__(self, model  , config ):\n\n        self.model  = model  \n\n        self.config = config \n        self.device =  config['device'] \n        self.optimizer = AdamW([\n            {'params': model.transformer_parameters(), 'lr': config['transformer_lr']},\n            {'params': model.memory_parameters(), 'lr': config['memory_lr']},\n        ])\n\n\n    \n\n        self.model.to(self.device)\n\n        total_steps = config['epochs'] * config['steps_per_epoch']\n    \n        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            self.optimizer,\n            max_lr=[\n                config['transformer_lr'],\n                config['memory_lr'],\n            ],\n            total_steps=total_steps\n        )\n\n    \n   \n\n\n    class Dataset_V1(Dataset):\n        def __init__(self, data, tokenizer, max_length, stride):\n            self.max_length = max_length\n            self.input_ids = []\n            self.target_ids = []\n            self.tokenizer =  tokenizer\n\n            all_tokens = []\n            allowed = {'<|endoftext|>'}\n            for sample in data:\n                prompt = self.generate_prompt(sample)\n                tokens = tokenizer.encode(prompt , allowed_special=allowed)\n                all_tokens.extend(tokens)\n\n            for i in range(0, len(all_tokens) - max_length, stride):\n                input_chunk = all_tokens[i: i + max_length]\n                target_chunk = all_tokens[i + 1: i + max_length + 1]\n                self.input_ids.append(torch.tensor(input_chunk))\n                self.target_ids.append(torch.tensor(target_chunk))\n\n        def __len__(self):\n            return len(self.input_ids)\n\n        def __getitem__(self, idx):\n            return self.input_ids[idx], self.target_ids[idx]\n        def generate_prompt(self,sample):\n        # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n            return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n        \n    def collate_fn(self, batch):\n        inputs, targets = zip(*batch)\n        inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n        targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n        return inputs, targets\n\n    def create_dataloader_v1(self,data, batch_size=4,\n        max_length=256, stride=128, shuffle=True, drop_last=True):\n        tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n        dataset =self.Dataset_V1(data, tokenizer, max_length, stride) #B\n        dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=self.collate_fn)\n        return dataloader\n  \n\n\n    def train_epoch(self,train_loader ):\n        self.model.train()\n        total_loss = 0 \n        memory_metrics = []\n\n        for batch_idx, (inputs , targets ) in enumerate(tqdm(\n            train_loader\n        )):\n            inputs , targets =  inputs.to(self.device)  , targets.to(self.device)\n\n            self.optimizer.zero_grad()\n            logits =  self.model(inputs)\n\n            task_loss  =  torch.nn.functional.cross_entropy(\n                logits.view(-1 , logits.size(-1)) , \n                targets.view(-1),\n                ignore_index=self.tokenizer.pad_token_id)\n            memory_loss =  self.model.get_memory_loss()\n            total_batch_loss =  task_loss + memory_loss \n            total_batch_loss.backward()\n\n            clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])\n            self.optimizer.step()\n            self.scheduler.step()\n\n            if batch_idx % self.config['memory_maintenance_interval'] == 0:\n                with torch.no_grad():\n                    self.model.shared_memory.maintain_memory()\n                    metrics =  self.model.shared_memory.get_memory_metrics()\n                    memory_metrics.append(metrics)\n\n            total_loss += total_batch_loss.item()\n            del logits , task_loss , memory_loss \n            torch.cuda.empty_cache()\n\n        return {\n            'avg_loss':total_loss / len(train_loader) , \n            'memory_metric':self._aggregate_memory_metrics(memory_metrics)\n        }\n\n    def validate(self, val_loader ):\n        self.model.eval()\n        total_loss =  0 \n        memory_metrics= []\n\n        with torch.no_grad():\n            for inputs  ,targets in val_loader:\n                inputs , targets  = inputs.to(self.device) , targets.to(self.device)\n\n                logits =  self.model(inputs )\n                task_loss = torch.nn.functional.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    targets.view(-1),\n                    ignore_index=self.tokenizer.pad_token_id\n                )\n                total_loss += task_loss.item()\n                metrics =  self.model.shared_memory.get_memory_metrics()\n                memory_metrics.append(metrics)\n\n\n        return {\n            'avg_loss':total_loss / len(val_loader) , \n            'memory_metrics':self.__aggregate_memory_metrics(memory_metrics)\n        }\n    \n    def _aggregate_memory_metrics(self, metrics_list):\n        aggregated = {\n            'active_slots': np.mean([m['active_slots'] for m in metrics_list]),\n            'utilization': np.mean([m['active_slots'] / self.config['num_slots'] for m in metrics_list]),\n            'energy_mean': np.mean([m['energy_mean'] for m in metrics_list]),\n            'consolidation_rate': np.mean([m.get('consolidation_rate', 0) for m in metrics_list])\n        }\n        return aggregated\n    \n\n    def train(self, train_texts, val_texts, epochs=10):\n        train_loader = self.create_dataloader_v1(train_texts, self.config['batch_size'])\n        val_loader = self.create_dataloader_v1(val_texts, self.config['batch_size'])\n        \n        best_val_loss = float('inf')\n        \n        for epoch in range(epochs):\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            train_stats = self.train_epoch(train_loader)\n            val_stats = self.validate(val_loader)\n            \n            print(f\"Train Loss: {train_stats['avg_loss']:.4f}\")\n            print(f\"Val Loss: {val_stats['avg_loss']:.4f}\")\n            print(\"Memory Stats:\")\n            print(f\"  Active Slots: {train_stats['memory_metrics']['active_slots']:.1f}\")\n            print(f\"  Utilization: {train_stats['memory_metrics']['utilization']:.2%}\")\n            print(f\"  Avg Energy: {train_stats['memory_metrics']['energy_mean']:.2f}\")\n            \n            # Save best model\n            if val_stats['avg_loss'] < best_val_loss:\n                best_val_loss = val_stats['avg_loss']\n                self.save_checkpoint(f\"best_model.pth\")\n            \n            # Save periodic checkpoint\n            if epoch % self.config['checkpoint_interval'] == 0:\n                self.save_checkpoint(f\"checkpoint_epoch{epoch}.pth\")\n\n    def save_checkpoint(self, path):\n        torch.save({\n            'model_state': self.model.state_dict(),\n            'memory_state': self.model.shared_memory.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'scheduler': self.scheduler.state_dict(),\n            'config': self.config\n        }, path)\n\n    def load_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state'])\n        self.model.shared_memory.load_state_dict(checkpoint['memory_state'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.scheduler.load_state_dict(checkpoint['scheduler'])\n\nconfig = {\n    'emb_dim': 768,\n    'num_slots': 1000,\n    'transformer_lr': 5e-5,\n    'memory_lr': 1e-4,\n    'gate_lr': 1e-3,\n    'batch_size': 8,\n    'seq_length': 256,\n    'max_grad_norm': 1.0,\n    'memory_maintenance_interval': 50,\n    'checkpoint_interval': 2,\n    'total_steps': 10000,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n     'epochs': 5,\n    'steps_per_epoch': 1000,\n}\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:56.574152Z","iopub.execute_input":"2025-04-26T05:01:56.574350Z","iopub.status.idle":"2025-04-26T05:01:56.664084Z","shell.execute_reply.started":"2025-04-26T05:01:56.574333Z","shell.execute_reply":"2025-04-26T05:01:56.663202Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# GPT Config ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nGPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,#126 \n# Context lengt\n\"emb_dim\": 768,\n# Embedding dimension\n\"n_heads\": 12,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False\n# Query-Key-Value bias\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:58.700807Z","iopub.execute_input":"2025-04-26T05:01:58.701110Z","iopub.status.idle":"2025-04-26T05:01:58.711632Z","shell.execute_reply.started":"2025-04-26T05:01:58.701087Z","shell.execute_reply":"2025-04-26T05:01:58.710842Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def train_memory_model(\n    model: nn.Module,\n    train_dataloader: DataLoader,\n    device: torch.device,\n    eval_dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    max_grad_norm: float = 1.0,\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    memory_metrics = []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # Create TensorBoard writer\n    # writer = SummaryWriter(log_dir='runs/memory_experiment')\n\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for batch_idx, (inputs_batch, target_batch) in enumerate(train_dataloader):\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            \n            # Forward pass with memory\n            logits = model(inputs_batch)\n            \n            # Calculate loss with memory regularization\n            loss = cal_loss_batch(inputs_batch, target_batch, model, device)\n            loss += model.memory_system.episodic_memory_cell.memory_regularization_loss()\n            loss += model.memory_system.semantic_memory_cell.memory_regularization_loss()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n\n            # Memory maintenance\n            if batch_idx % 100 == 0:\n                model.memory_system.maintain_memory()\n\n            # Logging and evaluation\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                # Evaluate and get memory metrics\n                train_loss, val_loss, mem_metrics = evaluate_memory_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n                \n                # Record metrics\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                memory_metrics.append(mem_metrics)\n                \n                # TensorBoard logging\n                writer.add_scalar('Loss/Train', train_loss, global_step)\n                writer.add_scalar('Loss/Val', val_loss, global_step)\n                log_memory_metrics(writer, mem_metrics, global_step)\n                \n                print(f\"Step {global_step}:\")\n                print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n                print_memory_health(mem_metrics)\n\n            # Generate samples with current memory state\n            if global_step % (eval_freq*2) == 0:\n                generate_with_memory(\n                    model, \n                    train_dataloader.dataset.tokenizer, \n                    device, \n                    start_context,\n                    writer,\n                    global_step\n                )\n\n        # Save model and memory state\n        save_memory_checkpoint(model, optimizer, epoch+1, global_step)\n\n    writer.close()\n    return train_losses, val_losses, track_tokens_seen, memory_metrics\n\ndef evaluate_memory_model(model, train_loader, eval_loader, device, eval_iter):\n    model.eval()\n    train_loss, val_loss = 0, 0\n    memory_stats = []\n    \n    with torch.no_grad():\n        # Evaluate on training set\n        for i, (inputs, targets) in enumerate(train_loader):\n            if i >= eval_iter: break\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            train_loss += cal_loss_batch(inputs, targets, model, device).item()\n            \n        # Evaluate on validation set\n        for i, (inputs, targets) in enumerate(eval_loader):\n            if i >= eval_iter: break\n            inputs, targets = inputs.to(device), targets.to(device)\n         \n            logits =model(inputs)\n            val_loss += cal_loss_batch(inputs, targets, model, device).item()\n            \n        # Get memory health metrics\n        mem_health = model.memory_system.maintain_memory()\n        \n    model.train()\n    return train_loss/eval_iter, val_loss/eval_iter, mem_health\n\ndef log_memory_metrics(writer, metrics, step):\n    # Episodic memory metrics\n    writer.add_scalar('Memory/Episodic/UsedSlots', \n                     metrics['episodic']['active_slots'], step)\n    writer.add_scalar('Memory/Episodic/MeanSimilarity', \n                     metrics['episodic']['memory_similarity']['mean_similarity'], step)\n    \n    # Semantic memory metrics\n    writer.add_scalar('Memory/Semantic/ActiveConcepts', \n                     metrics['semantic']['active_concepts'], step)\n    writer.add_scalar('Memory/Semantic/EnergyMean', \n                     metrics['semantic']['energy_mean'], step)\n\ndef print_memory_health(metrics):\n    print(\"Memory Health:\")\n    print(f\"  Episodic: {metrics['episodic']['active_slots']} active slots\")\n    print(f\"    Similarity: {metrics['episodic']['memory_similarity']['mean_similarity']:.3f}\")\n    print(f\"  Semantic: {metrics['semantic']['active_concepts']} concepts\")\n    print(f\"    Energy: {metrics['semantic']['energy_mean']:.3f}\")\n\ndef generate_with_memory(model, tokenizer, device, start_context, writer=None, step=None):\n    model.eval()\n    with torch.no_grad():\n        # Generate with current memory state\n        input_ids = tokenizer.encode(start_context, return_tensors='pt').to(device)\n        \n        # Generate text with memory context\n        outputs = model.generate(\n            input_ids,\n            max_length=100,\n            temperature=0.8,\n            do_sample=True,\n            memory_context=model.memory_system.get_memory_state()\n        )\n        \n        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(\"\\nGenerated Text with Memory:\")\n        print(text)\n        \n        if writer:\n            writer.add_text(\"Generated Text\", text, step)\n    \n    model.train()\n\ndef save_memory_checkpoint(model, optimizer, epoch, step):\n    state = {\n        'epoch': epoch,\n        'step': step,\n        'model_state': model.state_dict(),\n        'optimizer_state': optimizer.state_dict(),\n        'memory_system': {\n            'episodic': model.memory_system.episodic_memory_cell.get_memory(),\n            'semantic': model.memory_system.semantic_memory_cell.get_memory(),\n            'router': model.memory_system.memory_router.state_dict()\n        }\n    }\n    torch.save(state, f\"memory_checkpoint_{epoch}_{step}.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:58.858967Z","iopub.execute_input":"2025-04-26T05:01:58.859175Z","iopub.status.idle":"2025-04-26T05:01:58.873548Z","shell.execute_reply.started":"2025-04-26T05:01:58.859157Z","shell.execute_reply":"2025-04-26T05:01:58.872811Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# model = GPTMQModel2(GPT_CONFIG_124M)\n\n\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# model.to(device)\n# optimizer  =  torch.optim.AdamW(model.parameters() , lr = 0.0004  , weight_decay= 0.01)\n# num_epochs = 1\n# train_ratio = 0.90\n\n# text_data = chunk\n# print(len(text_data))\n# split = int(train_ratio * len(text_data))\n# print(split)\n# train_data= text_data[:split]\n# val_data = text_data[split:]\n# # train_dataloader = create_dataloader_v1(txt= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True , stride=GPT_CONFIG_124M['context_length'])\n# # val_dataloader = create_dataloader_v1(txt= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False , stride=GPT_CONFIG_124M['context_length'])\n# train_dataloader = create_dataloader_v2(data= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True )\n# val_dataloader = create_dataloader_v2(data= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False )\n# start_context = '### Instruction :Give three tips for staying healthy ### Response:'\n# print('start trainning')\n# train_losses , val_losses  , token_seen = train_model(\n#     model= model , train_dataloader= train_dataloader , \n#     eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n#     eval_iter=3 , start_context=start_context, num_epochs=2\n# )\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:59.080566Z","iopub.execute_input":"2025-04-26T05:01:59.080841Z","iopub.status.idle":"2025-04-26T05:01:59.084425Z","shell.execute_reply.started":"2025-04-26T05:01:59.080811Z","shell.execute_reply":"2025-04-26T05:01:59.083687Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n        # --- GPTModel ---\n        \"vocab_size\": 50257,\n        'emb_dim': 768, \n        \"n_layers\": 12,                # Base embedding dimension\n        'n_heads': 12,\n        'drop_rate': 0.1,\n        'context_length': 126,\n        'qkv_bias': True,\n        'share_memory': True,  \n        # --- Memory Writer ---\n        'compress_dim': 64,             # Compressed embedding dimension\n        'temperature': 0.7,              # Softmax temp for sharper router weights\n        'memory_router_head': 4,        # Router head count\n\n        # --- Episodic Memory ---\n        'episodic_memory_dim': 768,     # Matches transformer emb_dim for residual\n        'num_slots': 500,               # Number of episodic memory slots\n        'comp_ratio': 0.25,             # Controls compression granularity\n        'memory_compression_ratio': 0.5, # Compression for storage efficiency\n        'epic_temp': 0.1,                      # Matching probability threshold\n        'apply_compression': True,      # Enable latent compression (like an autoencoder)\n        'episodic_prune_thresh' : 0.01, \n        # --- Semantic Memory ---\n        'semantic_memory_dim': 768,     # Same as emb_dim for residual compatibility\n        'num_concepts': 1000,           # Concept space size (can be large)\n        'semantic_comp_ratio': 0.25,    # Compression ratio per concept\n        'semantic_temp': 0.1,                    # Softmax temp for semantic retrieval\n        'top_k': 5,                     # Top-k memory vectors to retrieve\n        'expand_rate': 0.1,             # Optional growth for dynamic slot expansion\n\n        # --- General Memory Flags ---\n        'memory_type': 'hybrid',        # Use both episodic + semantic\n        'use_memory_router': True,\n        'memory_update_method': 'gated',# Learn to update selectively\n        'memory_decay': 0.95,           # Decay old memories slowly\n        'memory_norm': True,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:01:59.425944Z","iopub.execute_input":"2025-04-26T05:01:59.426183Z","iopub.status.idle":"2025-04-26T05:01:59.430816Z","shell.execute_reply.started":"2025-04-26T05:01:59.426163Z","shell.execute_reply":"2025-04-26T05:01:59.430003Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Initialize components\n    model = GPTMQMemoryModel(cfg=GPT_CONFIG_124M )\n    filename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\n    train_ratio = 0.90\n\n    with open(filename , 'r') as f:\n        text_data = json.load(f)\n    \n    text_data = text_data[:50]\n    print(len(text_data))\n    split = int(train_ratio * len(text_data))\n    print(split)\n    train_data= text_data[:split]\n    val_data = text_data[split:]\n    pipeline = GPTTrainingPipeline(model, config)\n    \n    \n    pipeline.train(train_data, val_data, epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:02:00.880973Z","iopub.execute_input":"2025-04-26T05:02:00.881276Z","iopub.status.idle":"2025-04-26T05:02:04.630517Z","shell.execute_reply.started":"2025-04-26T05:02:00.881250Z","shell.execute_reply":"2025-04-26T05:02:04.629322Z"}},"outputs":[{"name":"stdout","text":"50\n45\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-62b7a3aefc8b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-3b8b6ee9b15a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_texts, val_texts, epochs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mval_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-3b8b6ee9b15a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             task_loss  =  torch.nn.functional.cross_entropy(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-b22f1bef805e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tokens, mask)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_retention_alpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_retention_alpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-e693c37417c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mresid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_ln_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Memory phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-2a3e46c681b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, mask)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfreq_cies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiQueryAttentionBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-2a3e46c681b0>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(q, k, v, mask, dropout)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                      \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mattention_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mattention_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (126) must match the size of tensor b (256) at non-singleton dimension 3"],"ename":"RuntimeError","evalue":"The size of tensor a (126) must match the size of tensor b (256) at non-singleton dimension 3","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:48:09.976577Z","iopub.execute_input":"2025-04-24T09:48:09.976849Z","iopub.status.idle":"2025-04-24T09:48:09.982052Z","shell.execute_reply.started":"2025-04-24T09:48:09.976828Z","shell.execute_reply":"2025-04-24T09:48:09.981223Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"<generator object Module.parameters at 0x7ca4d17cba70>"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"# model = GPTMQModel2(GPT_CONFIG_124M)\nmodel = GPTMQMemoryModel(cfg=GPT_CONFIG_124M )\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\noptimizer  =  torch.optim.AdamW(model.parameters() , lr = 0.0004  , weight_decay= 0.01)\nnum_epochs = 1\ntrain_ratio = 0.90\nfilename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\n\nwith open(filename , 'r') as f:\n    text_data = json.load(f)\n\ntext_data = text_data[:50]\nprint(len(text_data))\nsplit = int(train_ratio * len(text_data))\nprint(split)\ntrain_data= text_data[:split]\nval_data = text_data[split:]\n# train_dataloader = create_dataloader_v1(txt= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True , stride=GPT_CONFIG_124M['context_length'])\n# val_dataloader = create_dataloader_v1(txt= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False , stride=GPT_CONFIG_124M['context_length'])\ntrain_dataloader = create_dataloader_v1(data= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True )\nval_dataloader = create_dataloader_v1(data= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False )\nstart_context = '### Instruction :Give three tips for staying healthy ### Response:'\nprint('start trainning')\ntrain_losses, val_losses, track_tokens_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=10\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T05:02:12.640971Z","iopub.execute_input":"2025-04-26T05:02:12.641293Z","iopub.status.idle":"2025-04-26T05:03:12.835470Z","shell.execute_reply.started":"2025-04-26T05:02:12.641267Z","shell.execute_reply":"2025-04-26T05:03:12.834124Z"}},"outputs":[{"name":"stdout","text":"50\n45\nstart trainning\n🚀 Total training steps: 310\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"x shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nEpoch: 1 (step 000000): Train Loss: 243.0607, Val Loss: 246.2433\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nEpoch: 1 (step 000005): Train Loss: 184.5903, Val Loss: 175.2049\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nEpoch: 1 (step 000010): Train Loss: 57.2633, Val Loss: 55.0253\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nEpoch: 1 (step 000015): Train Loss: 38.0707, Val Loss: 39.0728\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\nx shape  torch.Size([2, 126, 768])\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n    self.ctx_run(self.run)\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n    self.do_execute(\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-32-f3ec56ebe94f>\", line 27, in <cell line: 27>\n    train_losses, val_losses, track_tokens_seen = train_model(\n  File \"<ipython-input-24-7c116f97df48>\", line 81, in train_model\n    loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n  File \"<ipython-input-20-80d5e89efea2>\", line 4, in cal_loss_batch\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3479, in cross_entropy\n    return torch._C._nn.cross_entropy_loss(\n (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:110.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  0%|          | 0/10 [00:58<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-f3ec56ebe94f>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mstart_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'### Instruction :Give three tips for staying healthy ### Response:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start trainning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m train_losses, val_losses, track_tokens_seen = train_model(\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-7c116f97df48>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, device, eval_dataloader, optimizer, eval_freq, eval_iter, start_context, num_epochs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."],"ename":"RuntimeError","evalue":"Function 'LogSoftmaxBackward0' returned nan values in its 0th output.","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"from collections import defaultdict\n\ndef get_param_group_summary(model):\n    groups = defaultdict(int)\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if \"embedding\" in name:\n            groups[\"embedding\"] += param.numel()\n        elif \"transformer\" in name:\n            groups[\"transformer_blocks\"] += param.numel()\n        elif \"memory\" in name or \"episodic\" in name or \"semantic\" in name:\n            groups[\"memory_modules\"] += param.numel()\n        elif \"norm\" in name:\n            groups[\"normalization\"] += param.numel()\n        elif \"lm_head\" in name or \"projection\" in name:\n            groups[\"output_projection\"] += param.numel()\n        else:\n            groups[\"other\"] += param.numel()\n    total = sum(groups.values())\n    for k, v in groups.items():\n        print(f\"{k:20s}: {v:,} parameters\")\n    print(f\"\\nTotal: {total:,}\")\nget_param_group_summary(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:47:03.421280Z","iopub.execute_input":"2025-04-24T09:47:03.421571Z","iopub.status.idle":"2025-04-24T09:47:03.430581Z","shell.execute_reply.started":"2025-04-24T09:47:03.421551Z","shell.execute_reply":"2025-04-24T09:47:03.429796Z"}},"outputs":[{"name":"stdout","text":"memory_modules      : 10,305,011 parameters\nembedding           : 38,725,376 parameters\ntransformer_blocks  : 72,061,464 parameters\nnormalization       : 768 parameters\noutput_projection   : 50,257 parameters\n\nTotal: 121,142,876\n","output_type":"stream"}],"execution_count":60},{"cell_type":"raw","source":"def count_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n# your custom GPT, MemoryLLM, etc.\ntotal_params = count_trainable_parameters(model)\nprint(f\"Trainable parameters: {total_params:,}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T09:45:21.381944Z","iopub.execute_input":"2025-04-24T09:45:21.382287Z","iopub.status.idle":"2025-04-24T09:45:21.388203Z","shell.execute_reply.started":"2025-04-24T09:45:21.382238Z","shell.execute_reply":"2025-04-24T09:45:21.387376Z"}}},{"cell_type":"code","source":"from collections import defaultdict\n\ndef modulewise_param_count(model):\n    module_params = defaultdict(int)\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            module = name.split('.')[0]  # or custom parsing\n            module_params[module] += param.numel()\n    \n    for module, count in sorted(module_params.items(), key=lambda x: -x[1]):\n        print(f\"{module:<20} : {count:,} parameters\")\n\nmodulewise_param_count(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:45:21.656375Z","iopub.execute_input":"2025-04-24T09:45:21.656704Z","iopub.status.idle":"2025-04-24T09:45:21.664403Z","shell.execute_reply.started":"2025-04-24T09:45:21.656679Z","shell.execute_reply":"2025-04-24T09:45:21.663432Z"}},"outputs":[{"name":"stdout","text":"transformer_blocks   : 72,061,464 parameters\nembedding            : 38,597,376 parameters\nshared_memory        : 10,433,010 parameters\nprojection           : 50,257 parameters\nfinal_norm           : 768 parameters\nmemory_retention_alpha : 1 parameters\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"def train_model_restart(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    checkpoint_path: str = None\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step, start_epoch = 0, -1, 0\n\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # 🔁 Load from checkpoint if provided\n    if checkpoint_path is not None:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)\n\n        # ⬇️ Reduce learning rate by half when resuming\n        for param_group in optimizer.param_groups:\n            old_lr = param_group['lr']\n            param_group['lr'] = old_lr * 0.5\n            print(f\"🔧 Reduced LR: {old_lr:.6f} ➜ {param_group['lr']:.6f}\")\n\n        print(f\"✅ Resuming training from Epoch {start_epoch}\")\n\n    # ⚙️ Reinitialize scheduler after changing LR\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=500,\n        num_training_steps=total_steps\n    )\n\n    for epoch in tqdm(range(start_epoch, num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n\n        save_model_checkpoint(model, optimizer, epoch + 1, global_step, tokens_seen)\n\n    return train_losses, val_losses, track_tokens_seen\n\ndef save_model_checkpoint(model, optimizer, epoch, global_step=None, tokens_seen=None, path=\"checkpoint_epoch.pt\"):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    if global_step is not None:\n        checkpoint['global_step'] = global_step\n    if tokens_seen is not None:\n        checkpoint['tokens_seen'] = tokens_seen\n\n    torch.save(checkpoint, f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\")\n    print(f\"💾 Saved checkpoint at epoch {epoch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.412648Z","iopub.status.idle":"2025-04-24T09:34:44.413047Z","shell.execute_reply":"2025-04-24T09:34:44.412872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"        checkpoint = torch.load('/kaggle/working/checkpoint_epoch_6.pt', map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.413954Z","iopub.status.idle":"2025-04-24T09:34:44.414329Z","shell.execute_reply":"2025-04-24T09:34:44.414156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses , val_losses  , token_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=2\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.415232Z","iopub.status.idle":"2025-04-24T09:34:44.415568Z","shell.execute_reply":"2025-04-24T09:34:44.415454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nloss_history = {\n    \"train_loss\": train_losses,\n    \"val_loss\": val_losses,\n    \"tokens_seen\": token_seen\n}\n\nwith open(\"loss_history.json\", \"w\") as f:\n    json.dump(loss_history, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.416549Z","iopub.status.idle":"2025-04-24T09:34:44.416821Z","shell.execute_reply":"2025-04-24T09:34:44.416720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# If you loaded from a JSON file\n# with open(\"loss_history.json\", \"r\") as f:\n#     data = json.load(f)\n#     train_losses = data[\"train_loss\"]\n#     val_losses = data[\"val_loss\"]\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\nplt.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\nplt.xlabel(\"Evaluation Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")  # Save the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.417430Z","iopub.status.idle":"2025-04-24T09:34:44.417738Z","shell.execute_reply":"2025-04-24T09:34:44.417597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\n\n# Load model\nmodel = GPTMQModel2(GPT_CONFIG_124M)\n# model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_7.pt\"))\ncheckpoint = torch.load(\"checkpoint_epoch_7.pt\")\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\nmodel.eval().to(device)\n\n# Tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Utility functions\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n# Sampling-based generate function (uses your logic)\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\nf\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n# High-level text generation function\ndef generate_response(prompt, model, tokenizer, max_new_tokens=100, context_size=128, temperature=1.0, top_k=50):\n    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n    generated_ids = generate(\n        model=model,\n        idx=input_ids,\n        max_new_tokens=max_new_tokens,\n        context_size=context_size,\n        temperature=temperature,\n        top_k=top_k\n    )\n    return token_ids_to_text(generated_ids, tokenizer)\n\n# Try it out\n# prompt = \"### Instruction:\\nExplain what is deep learning.\\n\\n### Response:\\n <bot>\"\nprompt = \"\"\"\n\n'### Instruction :Give three tips for staying healthy ### Response:'\n\"\"\".strip()\n\n\noutput = generate_response(prompt, model, tokenizer)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.418499Z","iopub.status.idle":"2025-04-24T09:34:44.418773Z","shell.execute_reply":"2025-04-24T09:34:44.418675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is generated\n        if idx_next.item() == end_token_id:\n            break\n\n    return idx\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\nraw_output = generate_response(prompt, model, tokenizer)\ncleaned_output = truncate_after_n_bullets(raw_output)\nprint(cleaned_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.419352Z","iopub.status.idle":"2025-04-24T09:34:44.419579Z","shell.execute_reply":"2025-04-24T09:34:44.419486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_response(\n    prompt, model, tokenizer,\n    temperature=0.8,  # better balance\n    top_k=40,         # a bit narrower selection\n    max_new_tokens=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.420138Z","iopub.status.idle":"2025-04-24T09:34:44.420420Z","shell.execute_reply":"2025-04-24T09:34:44.420314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device).unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is in the generated output\n        if end_token_id in idx_next:\n            break\n\n    return idx\n\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\n\n# 🔁 Input prompt\nprompt = \"### Instruction: What are the three primary colors? \\n### Response:\"\n\n# 🔁 Tokenize input\ninput_ids = text_to_token_ids(prompt, tokenizer).to(device)\n\n# 🔁 Generate output tokens\noutput_ids = generate(\n    model=model,\n    idx=input_ids,\n    max_new_tokens=100,\n    context_size=128,\n    temperature=0.7,\n    top_k=40\n)\n\n# 🔁 Decode and postprocess\noutput_text = tokenizer.decode(output_ids[0].tolist())\n\n# ✂️ Truncate after 3 bullets (optional)\nfinal_output = truncate_after_n_bullets(output_text)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.421013Z","iopub.status.idle":"2025-04-24T09:34:44.421333Z","shell.execute_reply":"2025-04-24T09:34:44.421177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}