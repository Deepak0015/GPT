{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11339249,"sourceType":"datasetVersion","datasetId":7093845},{"sourceId":11378548,"sourceType":"datasetVersion","datasetId":7124129},{"sourceId":11378997,"sourceType":"datasetVersion","datasetId":7124489}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math \nimport torch \nimport torch.nn as nn \nimport transformers \nfrom tqdm.notebook import tqdm\n# Memory Network\nimport torch.nn.functional as F \nfrom typing import Tuple , Optional\nimport torch.bin \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:59:54.496326Z","iopub.execute_input":"2025-05-03T12:59:54.496600Z","iopub.status.idle":"2025-05-03T12:59:56.887790Z","shell.execute_reply.started":"2025-05-03T12:59:54.496579Z","shell.execute_reply":"2025-05-03T12:59:56.886962Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:59:56.888882Z","iopub.execute_input":"2025-05-03T12:59:56.889287Z","iopub.status.idle":"2025-05-03T12:59:56.893017Z","shell.execute_reply.started":"2025-05-03T12:59:56.889257Z","shell.execute_reply":"2025-05-03T12:59:56.892280Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# RMS NORM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim)) \n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        norm_x = self._norm(x.float()).type_as(x) \n        return norm_x * self.scale \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:59:56.894373Z","iopub.execute_input":"2025-05-03T12:59:56.894681Z","iopub.status.idle":"2025-05-03T12:59:56.909821Z","shell.execute_reply.started":"2025-05-03T12:59:56.894656Z","shell.execute_reply":"2025-05-03T12:59:56.909145Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Embedding Layer","metadata":{}},{"cell_type":"code","source":"import torch \n\nclass EmbeddingLayer(torch.nn.Module):\n    def __init__(self , vocab_size , embedding_dim):\n        super().__init__()\n\n        self.embedding_layer= torch.nn.Embedding(vocab_size , embedding_dim)\n\n    def forward(self , input_tokens):\n        return self.embedding_layer(input_tokens)\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:59:57.055650Z","iopub.execute_input":"2025-05-03T12:59:57.055867Z","iopub.status.idle":"2025-05-03T12:59:57.060634Z","shell.execute_reply.started":"2025-05-03T12:59:57.055848Z","shell.execute_reply":"2025-05-03T12:59:57.059928Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# FeedForward Layer ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n      \n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n            GELU(),\n            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n        )\n    def forward(self, x ):\n        return self.layers(x)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:59:57.596225Z","iopub.execute_input":"2025-05-03T12:59:57.596504Z","iopub.status.idle":"2025-05-03T12:59:57.601725Z","shell.execute_reply.started":"2025-05-03T12:59:57.596480Z","shell.execute_reply":"2025-05-03T12:59:57.600869Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Normalization Layer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn \nimport torch \n\nclass LayerNorm(nn.Module):\n    def __init__(self , emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale  = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self , x):\n        mean = x.mean(dim= -1, keepdim = True)\n        var = x.var(dim =-1, keepdim = True)\n        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n        return self.scale * norm_x + self.shift \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T12:59:58.956282Z","iopub.execute_input":"2025-05-03T12:59:58.956598Z","iopub.status.idle":"2025-05-03T12:59:58.961714Z","shell.execute_reply.started":"2025-05-03T12:59:58.956571Z","shell.execute_reply":"2025-05-03T12:59:58.960877Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# RoPE Embdding ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport torch \nfrom dataclasses import dataclass\n\n\nclass NRopE: # RopE in Numpy \n    def rotate_2d(self,vec , theta_p):\n        cos_theta  , sin_theta  = np.cos(theta_p) , np.sin(theta_p)\n        rotat_vec = np.array([[cos_theta , -sin_theta],\n                    [sin_theta ,cos_theta]])\n        \n        return rotat_vec @ vec\n\n\n    def RoPe(self,x , p , theta = 10000):\n        d = len(x)\n        x_rotate =  np.zeros_like(x)\n        for i in range(0 , d , 2):\n            if i +1< d:\n                theta_p = (theta **(-2*(i//2)))**p \n                roted_pair = self.rotate_2d(x[i:i+1] , theta_p)    \n                x_rotate[i:i+1] = roted_pair\n\n        return x_rotate\n\n\n\n@dataclass\nclass TRopE(torch.nn.Module): # RopE in torch \n    def __init__(self, dim:int ,theta:float = 10000):\n        self.dim = dim \n        self.theta = theta \n        self.freq =  torch.pow(self.theta ,-torch.arange(0 ,dim  , 2)/dim )\n        torch.nn.Parameter('freq' , self.freq)\n\n    def forward(self, x:torch.Tensor , pos:torch.Tensor):\n        batch_size , seq_len, dim = x.shape\n        assert dim ==self.dim ,\"Error dim must be same\"\n        theta_p = torch.einsum(\"n,d->nd\" , pos, self.freq.to(x.device))\n        cos_theta  , sin_theta = torch.cos(theta_p) , torch.sin(theta_p)\n        x_even , x_odd =  x[... , ::2] , x[... , 1::2]\n        x_rotated =  torch.empty_like(x)\n        x_rotated[...,::2] =  x_even * cos_theta - x_odd * sin_theta\n        x_rotated[...,1::2] =  x_even * sin_theta + x_odd * cos_theta\n\n        return x_rotated\n\n\n\n\n\n\n\ndef precompute_freq_cis(  dim:int , end:int , theta:float = 10000.0):\n        \"\"\"dim : dimentions \n        end: end index   \n        \"\"\"\n        freqs =  1/(theta **(torch.arange(0 , dim , 2)[:dim//2].float() / dim))\n        t =  torch.arange(end, device=freqs.device)\n        freqs = torch.outer(t , freqs).float()\n        freqs_cis =  torch.polar(torch.ones_like(freqs), freqs)\n        return freqs_cis \n\n\ndef reshape_for_broadcast(freq_cis  , x):\n        \"\"\" reshape the freqcies to match x dimentions \"\"\"\n        ndim=  x.ndim\n        assert 0<=1<ndim \n        assert freq_cis.shape == (x.shape[1], x.shape[-1]), f\"Expected {(x.shape[1], x.shape[-1])}, got {freq_cis.shape}\" \n        shape = [d if i == 1 or i ==  ndim -1 else 1 for i , d in enumerate(x.shape)]\n        return freq_cis.view(*shape)\n\n\ndef apply_rotary_embedding( xq:torch.Tensor ,xk:torch.Tensor ,  freq_cis:torch.Tensor):\n\n            xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n            xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n\n\n            freq_cies =  reshape_for_broadcast(freq_cis , xq_)\n    \n\n            xq_out = torch.view_as_real(xq_* freq_cies).flatten(3)\n            \n            xk_out = torch.view_as_real(xk_*freq_cies).flatten(3)\n\n\n            return  xq_out.type_as(xq)   ,  xk_out.type_as(xq) \n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:00.601539Z","iopub.execute_input":"2025-05-03T13:00:00.601813Z","iopub.status.idle":"2025-05-03T13:00:00.613631Z","shell.execute_reply.started":"2025-05-03T13:00:00.601792Z","shell.execute_reply":"2025-05-03T13:00:00.612911Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# MultiHead & MultiQuery Attention Layer ","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention_V2(nn.Module):\n    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n        super().__init__()\n        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n        self.d_in =d_in\n        self.d_out = d_out\n        self.dropout = nn.Dropout(dropout)\n        self.num_heads  = num_heads\n        self.head_dim = d_out // num_heads\n        self.out_proj  = nn.Linear(d_out , d_out)\n        self.register_buffer(\n            'mask',\n            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens , d_in = x.shape\n        keys = self.w_key(x)\n        queries  = self.w_query(x)\n        values = self.w_value(x)\n        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n        queries = queries.transpose(1, 2)\n        keys = keys.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(2, 3)\n        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n        attn_scores.masked_fill(mask_bool , -torch.inf)\n        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n        attn_weights = self.dropout(attn_weights)\n        context_vector = (attn_weights  @ values).transpose(1, 2)\n        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n        context_vector = self.out_proj(context_vector)\n        return context_vector\n\n\n\n\ndef apply_rotary_embedding(xq:torch.Tensor , xk:torch.Tensor , freq_cies:torch.Tensor):\n\n    assert xq.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n    assert xk.shape[-1] % 2 == 0 , 'Embeddig dimension must be even for complex paring'\n\n\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1] , -1, 2))\n\n    freq_cies = reshape_for_broadcast(freq_cies , xq_)\n\n    xq_out = torch.view_as_real(xq_ * freq_cies ).flatten(3)\n\n    xk_out = torch.view_as_real(xk_ * freq_cies).flatten(3)\n\n    return xq_out.type_as(xq) ,  xk_out.type_as(xk)\n\n\n\n\nclass MultiQueryAttentionBlock(nn.Module):\n    def __init__(self, d_model:int , h:int , dropout:float , seq_len:int , qkv_bias =  False ):\n        super().__init__()\n        self.d_model  = d_model \n\n        self.seq_len=  seq_len\n\n        assert d_model % h == 0, \"d_model is must be divided by th head\"\n        self.dropout = nn.Dropout(dropout)\n\n        self.h = h  \n\n        self.d_k = d_model // h \n\n        self.w_qkv =  nn.Linear(d_model , d_model +2 * self.d_k )\n\n        self.w_o = nn.Linear(d_model  , d_model)\n\n        freq_cies = precompute_freq_cis(dim=self.d_k , end=self.seq_len * 2 )\n\n        self.register_buffer('freq_cies' , freq_cies , persistent= False )\n\n    def generate_causal_mask(self, seq_len, device):\n        # shape: (1, 1, seq_len, seq_len)\n        return torch.tril(torch.ones((1, 1, seq_len, seq_len), device=device)).bool()\n\n    @staticmethod\n    def attention(q, k  , v,mask  , dropout):\n        d_k = q.shape[-1]\n\n        attention_score =  (q @ k.transpose(-2,-1)) / math.sqrt(d_k)\n\n        if mask is not None :\n            if mask.dim() == 2:\n                      mask = mask.unsqueeze(1).unsqueeze(2)\n            elif mask.dim() == 3:\n                     mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n        \n        attention_score = attention_score.softmax(dim=-1)\n\n        if dropout is not None :\n            attention_score = dropout(attention_score)\n\n        context_vector =  attention_score @ v\n\n        return context_vector  , attention_score\n    \n\n\n    def forward(self, q, mask= None):\n        if mask is None:\n            mask = self.generate_causal_mask(self.seq_len , device = q.device)\n        qkv =  self.w_qkv(q)\n\n        query , key, value =  torch.split(qkv , [self.d_model  , self.d_k , self.d_k], dim=-1)\n\n        query = query.view(query.shape[0] , -1 , self.h , self.d_k).transpose(1, 2)\n\n        key =  key.unsqueeze(1)\n\n        value =  value.unsqueeze(1)\n\n        seq_len =  q.size(1)\n\n        freq_cies = self.freq_cies[:query.shape[1]].to(q.device)\n\n        # freq_cies =  self.freq_cies[:seq_len].to(q.device)\n\n        query , key = apply_rotary_embedding(query , key , freq_cies)\n\n        x , self.attention_score = MultiQueryAttentionBlock.attention(q = query,k =  key,v= value ,mask=mask , dropout= self.dropout)\n\n        x = x.transpose(1,2).contiguous().view(x.shape[0] , -1, self.h* self.d_k)\n\n        x = self.w_o(x)\n\n        return x \n    \n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:01.161430Z","iopub.execute_input":"2025-05-03T13:00:01.161701Z","iopub.status.idle":"2025-05-03T13:00:01.178125Z","shell.execute_reply.started":"2025-05-03T13:00:01.161681Z","shell.execute_reply":"2025-05-03T13:00:01.177259Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Memory Network ","metadata":{}},{"cell_type":"code","source":"\n\n\nclass EfiBioSemanticMemory_V2(nn.Module):\n    def __init__(self, input_dim:int ,semantic_memory_dim, max_slots:int = 1000 , compress_dim:int =  128 , top_k:int = 5 , num_heads:int =  4 ):\n        super().__init__()\n\n        self.input_dim = input_dim \n        self.max_slots =  max_slots \n        self.compress_dim =  compress_dim \n        self.top_k =  top_k \n        self.num_heads =  num_heads \n        self.semantic_memory_dim = semantic_memory_dim\n        self.memory_size =  semantic_memory_dim \n\n\n\n        self.key_memory =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.value_memory = nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.cell_state =  nn.Parameter(torch.randn(max_slots , compress_dim))\n        self.register_buffer('active_mask' , torch.zeros(max_slots , dtype= torch.bool))\n        self.active_mask[:semantic_memory_dim] = True  \n\n\n        # Meta data parameter \n        self.register_buffer('age', torch.zeros(max_slots))\n        self.register_buffer('usage', torch.zeros(max_slots))\n        self.register_buffer('concept_energy', torch.ones(max_slots))\n        self.register_buffer('memory_age', torch.zeros(max_slots))\n        self.register_buffer('access_count', torch.zeros(max_slots))\n        self.register_buffer(\"_memory_version\", torch.tensor(0))\n        self.concept_energy[:semantic_memory_dim] =  0.2\n\n        #stats params\n        self.register_buffer(\"step_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"query_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"novel_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"write_count\", torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\"hit_count\", torch.zeros(1, dtype=torch.long)) \n\n        # Threshold Parameter \n        self.consolidation_threshold = nn.Parameter(torch.tensor(100.0))\n        self.energy_threshold = nn.Parameter(torch.tensor(0.3))\n        self.decay_rate = nn.Parameter(torch.tensor(0.999))\n        # self.novelty_threshold = nn.Parameter(torch.tensor(0.2))\n        self.novelty_threshold = 0.2 * (1 - (self.memory_size / self.max_slots))\n        self.register_buffer(\"prune_age_threshold\", torch.tensor(100))\n        self.register_buffer(\"neurogenesis_threshold\", torch.tensor(0.9))\n        self.register_buffer(\"new_slot_maturation_steps\", torch.tensor(50)) \n        self.synaptic_scale = nn.Parameter(torch.tensor(0.1))\n        self.sparsity = nn.Parameter(torch.tensor(0.5))\n        self.sim_thershold =  nn.Parameter(torch.tensor(0.4))\n\n        \n        # Networks \n        self.important_net = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        \n\n        self.update_gate = nn.Sequential(\n            nn.Linear(3 * compress_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        for layer in self.update_gate:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n                nn.init.constant_(layer.bias, 0.1) \n        self.forgot_gate = nn.Sequential(\n            nn.Linear(semantic_memory_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 3),\n            nn.Sigmoid()\n        )\n        self.compression = nn.Sequential(\n            nn.Linear(input_dim, semantic_memory_dim),\n            nn.RMSNorm(semantic_memory_dim),\n            nn.GELU(),\n            nn.Linear(semantic_memory_dim, self.compress_dim)\n        )\n\n        self.decompression = nn.Sequential(\n            nn.Linear(self.compress_dim, input_dim),\n            nn.GELU()\n        )\n\n        self.W_cell = nn.Linear(self.compress_dim, semantic_memory_dim, bias=False)\n        self.memory_projection = nn.Linear(self.semantic_memory_dim, self.input_dim)\n\n        self.attn = nn.MultiheadAttention(\n            embed_dim=semantic_memory_dim,\n            num_heads=num_heads,\n            batch_first=False\n        )\n        nn.init.kaiming_uniform_(self.key_memory, mode='fan_out')\n        nn.init.xavier_normal_(self.value_memory)\n        nn.init.xavier_normal_(self.cell_state)\n\n    def _get_active_memory(self):\n\n        \"\"\"\n            Get the active memries slot \n        \"\"\"\n        idx = torch.nonzero(self.active_mask, as_tuple=False).squeeze(1)\n        assert idx.numel() > 0, \"No active memory slots\"\n       \n        return  (\n            self.key_memory[idx] , \n            self.value_memory[idx], \n            self.cell_state[idx]\n\n        )\n        \n    @property \n    def active_capacity(self):\n        return torch.sum(self.active_mask).item()/ self.max_slots\n    \n    def _retrive_memory(self ,query:torch.Tensor , batch_size:int , seq_len:int):\n\n        k_active , v_active , c_active =  self._get_active_memory()\n\n        k  = k_active.unsqueeze(1).expand(-1 , batch_size ,-1)\n        v = v_active.unsqueeze(1).expand(-1, batch_size , -1)\n        assert k.size(1) == batch_size\n\n        attn_output , attn_weights = self.attn(\n            query.unsqueeze(0), k ,v , need_weights =  True \n        )\n        attn_output = attn_output + torch.randn_like(attn_output) * 0.1\n        retrived =  attn_output.squeeze(0)\n        attn_score =  F.cosine_similarity(query.unsqueeze(1), k_active.unsqueeze(0), dim=-1)\n\n        active_indices =  torch.where(self.active_mask)[0]\n        topk_values , topk_idx =  attn_score.topk(\n            min(self.top_k , len(active_indices)) , dim=-1\n        )\n\n        topk_idx =  active_indices[topk_idx]\n\n        # Project the output to the out \n        out =  self.memory_projection(retrived)\n        out = out.unsqueeze(1).repeat(1, seq_len, 1)\n        return  out , retrived , topk_idx , attn_weights\n\n\n    def _adaptive_decay(self, memory_idx):\n\n \n\n        energy = self.concept_energy[memory_idx]\n        access = self.usage[memory_idx]\n\n        decay = torch.exp((1-self.decay_rate) * (1-energy) * (1-access))\n        return decay  \n    \n\n    def _update_memory(self, topk_idx , cells:torch.Tensor , projected:torch.Tensor , update_gates):\n        projected =  projected.unsqueeze(1).expand(-1,cells.size(1), -1)\n        active_indices =  torch.where(self.active_mask)[0]\n        new_slot_mask  = torch.isin(topk_idx, active_indices[-10:])\n        bs , _ ,_ = projected.shape\n        cell_input = cells + projected \n        cell_input =  torch.sigmoid(cell_input)\n\n        decay_factor =  self._adaptive_decay(topk_idx)\n        decay_factor =  decay_factor.unsqueeze(-1)\n        cell_updates =  decay_factor * cell_input \n        delta = update_gates.unsqueeze(-1) * cell_updates\n        cell_updates[new_slot_mask]*= 0.1\n        # assert cell_updates.shape == (bs, topk_idx, self.compress_dim)\n        # assert new_slot_mask.unsqueeze(-1).shape == (bs, topk_idx, 1)\n        cell_updates = cell_updates * torch.where(new_slot_mask.unsqueeze(-1), 0.1, 1.0)\n\n        batch_size, top_num , dim = delta.shape  \n        flat_topk_idx =  topk_idx.view(-1)\n\n        flat_delta =  (delta * self.synaptic_scale).view(-1, dim)\n        flat_cells =  F.normalize(cells, dim=-1).view(-1, dim)\n\n        with torch.no_grad():\n            self.cell_state.data.index_put_(\n                (flat_topk_idx , ),  \n                flat_delta , \n                accumulate=True \n            )\n            self.key_memory.data.index_add_(0, flat_topk_idx, flat_cells)\n            self.value_memory.data.index_add_(0, flat_topk_idx, flat_cells)\n\n            self.age.data += 1\n            self.age.data[flat_topk_idx] = 0 \n\n        decay =  self.decay_rate ** self.age.unsqueeze(-1)\n        # In _update_memory()\n        assert flat_topk_idx.max() < self.key_memory.size(0), \\\n            f\"Index {flat_topk_idx.max()} >= {self.key_memory.size(0)}\"\n        with torch.no_grad():\n            self.cell_state.data =  self.cell_state * decay\n            self.key_memory.data = self.key_memory * decay \n            self.value_memory.data =  self.value_memory  * decay \n\n            self.key_memory.data[flat_topk_idx] = torch.tanh(F.normalize(self.key_memory.data[flat_topk_idx], dim=-1))\n            self.value_memory.data[flat_topk_idx] = torch.tanh(F.normalize(self.value_memory.data[flat_topk_idx], dim=-1))\n            self.cell_state.data[flat_topk_idx] = torch.tanh(self.cell_state.data[flat_topk_idx])\n\n\n    def _get_low_energy_slots(self, candidate_indices):\n        if len(candidate_indices) == 0:\n            return candidate_indices \n        candidate_energy =  self.concept_energy[candidate_indices]\n\n        sorted_indices   =  torch.argsort(candidate_energy)\n        return candidate_indices[sorted_indices]\n    \n    @torch.no_grad()\n    def _write_new_concept(self, new_concepts:torch.Tensor ):\n        \n\n        \"\"\"\n        Reinitialized the slots that have been deactivated  \n\n        \"\"\"\n        batch_size  =  new_concepts.size(0)\n        active_mask =  self.active_mask\n        candidate =  torch.where(self.active_mask & (self.concept_energy < self.energy_threshold))[0]\n        candidate =self._get_low_energy_slots(candidate)\n        num_reuse = min(len(candidate), batch_size)\n\n        if num_reuse > 0:\n                reuse_idx = candidate[:num_reuse]\n                batch_indices = torch.arange(num_reuse) \n                self.usage.data[reuse_idx] = 0.0\n                self.age.data[reuse_idx] = 0.0\n                self.memory_age[reuse_idx] = 0.0\n                self.concept_energy.data[reuse_idx] = 0.5\n                decay =  torch.sigmoid(self.concept_energy[reuse_idx])\n                # Write data to memories\n                old_key = self.key_memory[reuse_idx]\n                old_val  = self.value_memory[reuse_idx]\n                old_cell =  self.cell_state[reuse_idx]\n                new_data = new_concepts[batch_indices]\n                new_key =  new_concepts[:num_reuse]\n                alpha =  0.5 \n                decay = torch.sigmoid(self.concept_energy[reuse_idx]).unsqueeze(-1)\n        \n                self.key_memory.data[reuse_idx] = F.normalize(\n                    0.3 * old_key + 0.7 * new_data * (1 - decay), \n                    dim=-1\n                )\n                self.value_memory.data[reuse_idx] = F.normalize(\n                    0.3 * old_val + 0.7 * new_data * (1 - decay), \n                    dim=-1\n                )\n                gate_input = torch.cat([\n                old_cell,\n                old_key,\n                new_data], dim=-1)\n\n                update_gate = self.update_gate(gate_input)\n                self.cell_state.data[reuse_idx] = F.normalize(\n                update_gate * new_data + (1 - update_gate) * old_cell,\n                dim=-1\n            )\n\n                \n        #         self.cell_state.data[reuse_idx] = 0.5 * self.cell_state[reuse_idx] + (1-decay) * new_concepts\n        remaining =  batch_size - num_reuse \n        if remaining  > 0 and self.memory_size < self.max_slots:\n            add = min(remaining , self.max_slots - self.memory_size)\n        \n\n            start = self.memory_size \n            end =  start+ add \n            new_idx =  torch.arange(start, end ,device= self.key_memory.device)\n            new_keys = new_concepts[num_reuse :num_reuse+add]\n            assert new_idx.numel() == new_keys.size(0), (\n            f\"Shape mismatch: {new_idx.numel()} vs {new_keys.size(0)}\"\n        )\n            assert new_idx.max() < self.key_memory.size(0), (\n            f\"Memory index {new_idx.max()} exceeds max slots {self.key_memory.size(0)}\"\n        )\n            self.key_memory[new_idx] =  new_keys\n            self.value_memory[new_idx] = new_keys.clone()\n            self.cell_state.data[new_idx] = new_keys.clone()\n            self.usage.data[new_idx]= 0\n            self.age.data[new_idx] = 0 \n            self.concept_energy.data[new_idx] = 0.5\n            self.access_count.data[new_idx] = 0 \n            self.active_mask.data[new_idx] =  True \n\n\n            self.memory_size = end\n        assert self.memory_size <= self.max_slots\n        assert torch.all(self.active_mask[:self.memory_size])\n                \n\n    \n    def _update_energy_level(self):\n        importance = self.important_net(self.key_memory).squeeze()\n        assert torch.all(self.concept_energy >= 0)\n        assert torch.all(self.concept_energy <= 1.01)  \n        new_energy =  (\n            self.decay_rate * self.concept_energy + 0.3 * self.usage + 0.1 * importance *(1-self.concept_energy)\n                    )\n        \n        deactivated  = ~self.active_mask \n\n        valid_deactivate =  deactivated.nonzero().squeeze()\n        valid_deactivate = valid_deactivate[(valid_deactivate>=0)&(valid_deactivate < self.memory_size)]\n        with torch.no_grad():\n            self.concept_energy.data =  torch.clamp(new_energy  , 0, 1)\n\n            if valid_deactivate.numel() >0:\n                self.key_memory.data[valid_deactivate] *=0.01\n                self.value_memory.data[valid_deactivate]*=0.01\n                self.cell_state.data[valid_deactivate]*=0.1\n\n        self.active_concepts =  torch.sum(self.active_mask).clamp(min=0 , max=self.memory_size)\n\n\n    def forward(self,x:torch.Tensor, training = True ):\n        bs , seq_len , _ =  x.shape  \n        self.step_count+=1 \n        self.query_count+=bs\n        compressed =  self.compression(x.mean(dim=1))\n        projected =  self.W_cell(compressed)\n        with torch.set_grad_enabled(training):\n            out , retrived, topk_idx  , attn_w = self._retrive_memory(projected , bs, seq_len )\n        max_scores , _ =  attn_w.max(dim=-1)\n        max_scores = max_scores.squeeze(-1)\n        hits =  (max_scores>0.5).sum()\n        self.hit_count +=  hits\n        self.novelty_threshold = 0.2 * (1 - self.active_capacity)\n        novel_mask =  max_scores < self.novelty_threshold\n        self.novel_count+=novel_mask.sum()\n        key_active, _ , _ =  self._get_active_memory()\n        if novel_mask.any():\n            novel_projection  = projected[novel_mask]\n            sim_scores  = F.cosine_similarity(novel_projection.unsqueeze(1), key_active.unsqueeze(0), dim=-1)\n            is_novel = sim_scores.max(dim=-1).values < self.sim_thershold\n            write_mask  =is_novel\n            if write_mask.any():\n                new_concepts = novel_projection[write_mask].detach()\n                self.write_count += new_concepts.size(0)\n                assert new_concepts.size(0) <= self.max_slots - self.memory_size, \\\n            \"Exceeding maximum memory capacity\"\n                self._write_new_concept(new_concepts)\n\n     \n        with torch.no_grad():\n            self.usage.data *= 0.95\n            self.usage.data[topk_idx] +=0.1\n            self.usage.data.clamp(0,1)\n            self.usage.data = 0.9 * self.usage.data \n            self.usage.data.scatter_add_(0, topk_idx.flatten(), torch.ones_like(topk_idx, dtype=torch.float).flatten())\n            self.usage.data.clamp_(max=1.0)\n    # feed one more entirely new concept → should reuse slot 1\n        if training:\n\n            self._update_energy_level()\n        keys= self.key_memory[topk_idx]\n        value = self.value_memory[topk_idx]\n        cells = self.cell_state[topk_idx]\n        # self._update_memory_metadata(topk_idx)\n\n        gate_input = torch.cat([\n            keys, cells, projected.unsqueeze(1).expand(-1, self.top_k , -1)\n        ], dim= -1) \n\n        update_gates =  self.update_gate(gate_input.view(-1, 3 *self.semantic_memory_dim))\n\n        update_gates = update_gates.view(bs, self.top_k)\n\n        self._update_memory(topk_idx=topk_idx, cells=cells ,projected=projected , update_gates=update_gates)\n        # if training:\n            # self._consolidate_important_memories()\n            # self.neurogenesis()\n        self._memory_version +=1 \n        return out , retrived ,topk_idx ,  attn_w \n    \n    @torch.no_grad()\n    def _gradual_influence_increase(self):\n        \"\"\"\n        Gradually increase the influence of newly added memory slots based on their age and access.\n        \"\"\"\n        new_slots_mask  = (self.age <= self.new_slot_maturation_steps) & self.active_mask \n        if not torch.any(new_slots_mask):\n            return  \n        \n        age_normalized = self.age[new_slots_mask] / self.new_slot_maturation_steps\n        usage_normalized =  self.usage[new_slots_mask]\n\n        growth_factor =torch.sigmoid((age_normalized + usage_normalized) * 3 ).unsqueeze(-1)\n\n        self.key_memory[new_slots_mask] = F.normalize(self.key_memory[new_slots_mask] * (1 + growth_factor * 0.5),\n        dim=-1)\n        self.value_memory[new_slots_mask] =  F.normalize(self.value_memory[new_slots_mask]* (1+growth_factor * 0.3) ,dim=-1 )\n\n        energy_boost = torch.clamp(0.1 * growth_factor.squeeze(), max=0.15)\n        self.concept_energy.data[new_slots_mask] = torch.clamp(\n        self.concept_energy[new_slots_mask] + energy_boost,\n        min=0.3,\n        max=0.7\n    )\n\n        self.age.data[new_slots_mask] += 1 \n\n        \n    def _consolidate_important_memories(self):\n        print('Consalidate Happen')\n        importance =  self.important_net(self.key_memory).squeeze()\n        consolidate_mask  = importance > 0.1\n        if consolidate_mask.any():\n          with torch.no_grad():\n            self.key_memory.data[consolidate_mask] = F.normalize(\n                self.key_memory[consolidate_mask] , dim=-1\n            )\n            mean_value = self.semantic_value_memory[consolidate_mask].mean(dim=0)\n           \n            self.semantic_value_memory.data[consolidate_mask] = (\n                    0.9 * self.semantic_value_memory[consolidate_mask] +\n                    0.1 * mean_value\n                )\n            self.concept_energy.data[consolidate_mask] = torch.clamp(self.concept_energy[consolidate_mask] + 0.05, 0, 1)\n\n    @torch.no_grad()\n    def prune_memories(self):\n        print(\"Memory Prunig Happen\")\n        prune_condidate =  (self.age > self.prune_age_threshold) & (self.usage < 0.01)\n        if prune_condidate.any():\n            self.key_memory.data[prune_condidate] *=  0.1\n            self.value_memory.data[prune_condidate]*=0.01\n            self.cell_state.data[prune_condidate] *= 0.01\n            self.age.data[prune_condidate] = 0 \n            self.usage[prune_condidate] = 0 \n            self.concept_energy.data[prune_condidate] = 0.1\n\n    @torch.no_grad()\n    def _prune_slots(self):\n            mask = self.age > self.prune_age_threshold\n            self.key_memory.data[mask] *= 0.01\n            self.value_memory.data[mask] *= 0.01\n            self.cell_state.data[mask] *= 0.01\n            self.concept_energy.data[mask] = 0\n\n\n    def replay_consolidation(self, x: torch.Tensor):\n        print('Memory Replay Happening ')\n        active_key , active_value, _ = self._get_active_memory()\n        if  self.training and random.random() < 0.2: \n            high_energy_mask = self.concept_energy > 0.8\n            if high_energy_mask.sum() > 0:\n                replay_keys = active_key[high_energy_mask]\n                replay_values = active_value[high_energy_mask]\n                \n                replay_input = self.decompression(replay_values.mean(dim=0, keepdim=True))\n                return replay_input\n        return x\n\n    def get_reusable_slots(self ,num_needed:int):\n       \n\n        age_score =  1-torch.sigmoid(self.age / 100) # old age \n        energy_score = (1-  self.concept_energy ) *2 \n        usage_score = 1 - self.usage \n        reuse_scores = (0.4 * energy_score  + 0.3 * age_score + 0.3 * usage_score \n                       )\n\n        mask =(self.concept_energy  < self.energy_threshold) & (self.age< 100)\n        reuse_scores[~mask]= -float('inf')\n\n        topk_scores  , candidates = torch.topk(reuse_scores, min(num_needed, self.memory_size))\n        return candidates \n\n            \n    def _reinitialize_slot(self, idx):\n        \"\"\"Reset a slot to initial state\"\"\"\n        print(\"Reinitialize Slots \")\n        with torch.no_grad():\n            scale = 0.1 + 0.05 * torch.rand(1, device=idx.device)\n            self.key_memory.data[idx] = torch.randn_like(self.key_memory[idx]) * scale\n            self.value_memory.data[idx] = torch.randn_like(self.value_memory[idx]) * scale\n            self.cell_state.data[idx] =  0.2 * self.cell_state.data[idx].mean(dim=0)\n            \n            # Reset metadata\n            self.concept_energy.data[idx] = 0.3 + 0.2 * torch.rand_like(self.concept_energy[idx])\n            self.usage.data[idx] =  0.1 * torch.rand_like(self.usage[idx])\n            self.age.data[idx] = 0\n            self.memory_age.data[idx] = 0\n            self.access_count.data[idx] = 0\n            self.active_mask.data[idx] = True \n\n\n\n\n\n    def _consalidate_new_slots(self):\n        new_slot_indices =  torch.arange(self.memory_size - 10 , self.memory_size )\n        new_slot_energy = self.concept_energy[new_slot_indices]\n        with torch.no_grad():\n            self.concept_energy.data[new_slot_indices] = torch.clamp(\n                new_slot_energy + 0.1 * self.age[new_slot_indices] , 0 ,1\n            )\n            self.key_memory.data[new_slot_indices] *=  0.1\n            self.value_memory.data[new_slot_indices] *= 0.1\n            self.age.data[new_slot_indices] +=1\n            self.access_count[new_slot_indices] +=1 \n\n    def _update_memory_metadata(self, used_indices):\n       \n        self.access_count[used_indices] += 1\n        \n        self.memory_age += 1\n        self.memory_age[used_indices] = 0\n        with torch.no_grad():\n            self.concept_energy.data[used_indices] += 0.1\n            self.concept_energy.data= torch.clamp(self.concept_energy * 0.95, 0, 1)\n    @torch.no_grad()\n    def neurogenesis(self):\n        print(\"Neurogenesis happen\")\n        if self.max_slots > self.memory_size:\n            usage_rate =  (self.usage > 0.1).float().mean()\n            if usage_rate > self.neurogenesis_threshold:\n                reusable =  self.get_reusable_slots()\n                num_reuse =  reusable.numel()\n                if num_reuse > 0:\n                    with torch.no_grad():\n                        device =  self.key_memory.device \n                        self._reinitialize_slot(idx=reusable)\n                        \n                new_slots =  min(10 -  num_reuse , self.max_slots- self.memory_size)\n                if new_slots > 0:\n                    start_idx =self.memory_size\n                    end_idx =  start_idx  + new_slots \n                    new_indices  = torch.arange(start_idx , end_idx, device = device )\n                    self.key_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.value_memory.data[new_indices] = torch.randn(new_slots, self.compress_dim, device=device) * 0.1\n                    self.cell_state.data[new_indices] = 0\n                    self.concept_energy.data[new_indices] = 0.5\n                    self.usage.data[new_indices] = 0.0\n                    self.age.data[new_indices] = 0.0\n                    self.access_count.data[new_indices] = 0.0\n                    self.active_mask.data[new_indices] = True\n                    self.memory_size += new_slots\n\n                self._gradual_influence_increase()\n                    \n\n\n    @torch.no_grad()\n    def _merge_similar_slots(self):\n        print(\"Merging the similar slots \")\n        device = self.key_memory.device\n        active_idx = self.active_mask.nonzero(as_tuple=True)[0]\n        if active_idx.numel() < 2:\n            return\n\n        act_keys = F.normalize(self.key_memory[active_idx], dim=1)  \n        sim_mtx  = act_keys @ act_keys.T                         \n\n        merge_thr = torch.clamp(0.95 - 0.01*self.memory_age[active_idx].mean(), 0.7, 0.9)\n\n        rows, cols = torch.triu_indices(act_keys.size(0), act_keys.size(0), 1, device=device)\n        mask_pairs = sim_mtx[rows, cols] > merge_thr\n        if not mask_pairs.any():\n            return\n\n        # 4) union–find over those pairs\n        parent = torch.arange(act_keys.size(0), device=device)\n        def find(x):\n            orig_x = x\n            while parent[x] != x:\n                parent[x] = parent[parent[x]]\n                x = parent[x]\n            parent[orig_x] = x  # Full path compression\n            return x\n\n        def union(u, v):\n            ru, rv = find(u), find(v)\n            if ru != rv:\n                parent[rv] = ru\n\n        uv = torch.stack([rows[mask_pairs], cols[mask_pairs]], dim=1)\n        for u, v in uv:\n            union(u.item(), v.item())\n\n        # 5) cluster IDs, counts\n        comp_ids = torch.tensor([find(i) for i in range(act_keys.size(0))], device=device)\n        uniq, inv, counts = torch.unique(comp_ids, return_inverse=True, return_counts=True)\n\n        to_merge = uniq[counts >= 2]\n        if to_merge.numel() == 0:\n            return\n\n        # 7) build a mask of all slots belonging to merge‐worthy clusters\n        cluster_mask = torch.isin(inv, to_merge)      \n        cluster_ids  = inv[cluster_mask]           \n\n        D = act_keys.size(1)\n        C = uniq.size(0)\n        sums_keys   = torch.zeros((C, D), device=device)\n        sums_vals   = torch.zeros((C, D), device=device)\n        sums_energy = torch.zeros((C,), device=device)\n        sums_keys.scatter_add_(0,\n            cluster_ids.unsqueeze(-1).expand(-1, D),\n            act_keys[cluster_mask]\n        )\n        sums_vals.scatter_add_(0,\n            cluster_ids.unsqueeze(-1).expand(-1, D),\n            self.value_memory[active_idx][cluster_mask]\n        )\n        sums_energy.scatter_add_(0,\n            cluster_ids,\n            self.concept_energy[active_idx][cluster_mask]\n        )\n        counts_clamped = counts[to_merge].unsqueeze(-1).clamp(min=1).to(device)\n        means_keys   = sums_keys   / counts_clamped\n        means_vals   = sums_vals   / counts_clamped\n        cmask =  counts <= 2\n        means_energy = sums_energy / counts[cmask]\n\n        positions = torch.arange(act_keys.size(0), device=device)\n        first_pos = torch.full((C,), act_keys.size(0), device=device, dtype=torch.long)\n        first_pos.scatter_reduce_(0, inv, positions, reduce=\"amin\")\n        keep_slots  = active_idx[first_pos] \n        with torch.no_grad():\n            self.key_memory.data[keep_slots] = means_keys\n            self.value_memory.data[keep_slots] = means_vals\n            self.concept_energy.data[keep_slots] = means_energy\n\n        all_active = torch.arange(act_keys.size(0), device=device)\n        merged_mask = torch.isin(inv, to_merge) & (positions != first_pos[inv])\n        drop_small = active_idx[merged_mask]\n        if drop_small.numel() > 0:\n            noise = torch.randn_like(self.key_memory[drop_small]) * 0.01\n            with torch.no_grad():\n                self.key_memory.data[drop_small]    = noise\n                self.value_memory.data[drop_small]  = noise.clone()\n                self.concept_energy.data[drop_small] = 0.2\n                self.age.data[drop_small]           = 0\n                self.usage.data[drop_small]         = 0\n                self.access_count.data[drop_small]  = 0\n\n      # Get cluster-wise metrics before merging\n        cluster_usage = torch.zeros_like(means_energy)\n        cluster_access = torch.zeros_like(means_energy)\n        cluster_usage.scatter_add_(0, cluster_ids, self.usage[active_idx][cluster_mask])\n        cluster_access.scatter_add_(0, cluster_ids, self.access_count[active_idx][cluster_mask])\n        with torch.no_grad():\n            self.usage.data[keep_slots] = cluster_usage / counts[counts >= 2]\n            self.access_count.data[keep_slots] = cluster_access\n            self.usage.data[drop_small]        = 0\n            self.access_count.data[drop_small] = 0\n\n        self._update_memory_metadata(keep_slots)\n        \n    def get_memory_metrics(self):\n\n        \"Return memory health and retivel param details\"\n        active_mask  =  self.concept_energy > self.energy_threshold\n        energy  =  self.concept_energy \n        usage =  self.usage \n        access =  self.access_count\n\n        age_hist   = torch.histc(self.memory_age.float(), bins=10, min=0, max=float(self.memory_age.max()))\n        usage_hist = torch.histc(usage, bins=10, min=0, max=1.0)\n        access_hist= torch.histc(access.float(), bins=10, min=0, max=float(access.max()))\n\n        return  {\n            'memory_size': self.memory_size  , \n            'active_concepts':active_mask.sum().item(),\n            'utilization':active_mask.sum().item() / self.memory_size , \n            'energy_mean':energy.mean().item(), \n            'energy_std':energy.std().item(), \n            'age_histogram': age_hist, \n            'usage_histogram':usage_hist, \n            'access_histogram':access_hist, \n            'merge_rate':(energy < 0.3).sum().item() / self.memory_size ,\n            \"prune_rate\":((self.age > self.prune_age_threshold) & (usage < 0.01)).float().mean().item(),            \n            'neuro_rate':(self.memory_age < 10).float().mean().item(), \n            \"reuse_efficiency\":      access[energy > 0.5].float().mean().item(),\n\n             # —— retrieval/write stats ———————————————————————\n            \"steps\":                 self.step_count.item(),\n            \"queries\":               self.query_count.item(),\n            \"novelty_rate\":          self.novel_count.item() / max(1, self.query_count.item()),\n            \"write_rate\":            self.write_count.item() / max(1, self.query_count.item()),\n            \"hit_rate\":              self.hit_count.item() / max(1, self.query_count.item()),\n            \n        }\n\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:00.283742Z","iopub.execute_input":"2025-05-03T13:01:00.284022Z","iopub.status.idle":"2025-05-03T13:01:00.348752Z","shell.execute_reply.started":"2025-05-03T13:01:00.284001Z","shell.execute_reply":"2025-05-03T13:01:00.348028Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Transformer Block","metadata":{}},{"cell_type":"code","source":"\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention_V2(\n        d_in=cfg[\"emb_dim\"],\n        d_out=cfg[\"emb_dim\"],\n        context_length=cfg[\"context_length\"],\n        num_heads=cfg[\"n_heads\"],\n        dropout=cfg[\"drop_rate\"],\n        qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self, x):\n    #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut # Add the original input back\n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut #C\n        return x\n\n\n\nclass TransformerBlock_v2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention =  MultiQueryAttentionBlock(d_model=cfg['emb_dim'], h=cfg['n_heads'] , dropout=cfg['drop_rate'], seq_len=  cfg['context_length'] ,qkv_bias=cfg['qkv_bias'])\n\n        self.feed_forward = FeedForward(cfg)\n\n        self.layernorm1 =  LayerNorm(cfg['emb_dim'])\n    \n        self.layernorm2 =  LayerNorm(cfg['emb_dim'])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x , mask= None):\n\n        attention_output =  self.attention(self.layernorm1(x) , mask =  mask)\n\n        ff_output =  self.feed_forward(self.layernorm2(x))\n\n        return x + self.drop_out(ff_output) + self.drop_out(attention_output)\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.memory = MemorySystem(cfg=cfg)\n        self.feed_forward = FeedForward(cfg=cfg)\n        \n        self.norm1 = LayerNorm(cfg['emb_dim'])\n        self.norm2 = LayerNorm(cfg['emb_dim'])\n        self.norm3 = LayerNorm(cfg['emb_dim'])\n        \n        # Memory gate\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'] * 2, cfg['emb_dim']),\n            nn.Sigmoid()\n        )\n        \n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        attn_out = self.attention(self.norm1(x), mask=mask)\n        x = x + self.dropout(attn_out)\n        \n        norm_x = self.norm2(x)\n        epic_out , semantic_out , memory_out = self.memory(norm_x)\n        \n        gate_input = torch.cat([norm_x, memory_out], dim=-1)\n        memory_gate = self.memory_gate(gate_input)\n        x = x + memory_gate * memory_out\n        \n        ff_out = self.feed_forward(self.norm3(x))\n        x = x + self.dropout(ff_out)\n        \n        return x\nclass TransformerBlockWithMemory(nn.Module):\n    def __init__(self, cfg, shared_memory=None):\n        super().__init__()\n        # Core components\n        self.attention = MultiQueryAttentionBlock(\n            d_model=cfg['emb_dim'],\n            h=cfg['n_heads'],\n            dropout=cfg['drop_rate'],\n            seq_len=cfg['context_length'],\n            qkv_bias=cfg['qkv_bias']\n        )\n        self.ffn = FeedForward(cfg=cfg)\n        \n        # Memory system (shared across blocks)\n        self.memory = shared_memory or MemorySystem(cfg=cfg)\n        \n        # Normalization layers\n        self.pre_ln_attn = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_mem = RMSNorm(cfg['emb_dim'])\n        self.pre_ln_ffn = RMSNorm(cfg['emb_dim'])\n        \n        # Adaptive memory gating\n        self.memory_gate = nn.Sequential(\n            nn.Linear(cfg['emb_dim'], 1),\n            nn.Sigmoid()\n        )\n        \n        # Memory residual weights\n        self.mem_alpha = nn.Parameter(torch.tensor(0.5))\n        self.dropout = nn.Dropout(cfg['drop_rate'])\n\n    def forward(self, x, mask=None):\n        # Attention phase\n        resid = x\n        x = self.pre_ln_attn(x)\n        x = resid + self.dropout(self.attention(x, mask=mask))\n        \n        # Memory phase\n        resid_mem = x\n        x_mem = self.pre_ln_mem(x)\n        print('x shape ', x.shape)\n        _, _, memory_out = self.memory(x_mem)\n        \n        # Adaptive gating\n        gate = self.memory_gate(x_mem)\n        x = resid_mem + self.mem_alpha * gate * memory_out\n        \n        # FFN phase\n        resid_ffn = x\n        x = self.pre_ln_ffn(x)\n        x = resid_ffn + self.dropout(self.ffn(x))\n        \n        return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:00.686051Z","iopub.execute_input":"2025-05-03T13:01:00.686376Z","iopub.status.idle":"2025-05-03T13:01:00.699856Z","shell.execute_reply.started":"2025-05-03T13:01:00.686351Z","shell.execute_reply":"2025-05-03T13:01:00.699141Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# GPTQModel","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass InputEmbedding(nn.Module):\n\n    def __init__(self, vocab_size: int , d_model:int):\n\n        super().__init__()\n\n        self.d_model  =  d_model \n\n        self.vocab_size = vocab_size\n\n        self.embeddings = nn.Embedding(vocab_size , d_model)\n\n    def forward(self ,x):\n\n        return self.embeddings(x) * math.sqrt(self.d_model)\n    \n\n\n\nclass ProjectionLayer(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int, embdding_layer: nn.Embedding):\n        super().__init__()\n        self.weight = embdding_layer.weight  # share weights with input embedding\n        self.bias = nn.Parameter(torch.zeros(vocab_size))  # learnable bias\n\n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n\n\n\n\nclass GPTMQModel2(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim = cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel1(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n\n        # Use ModuleList instead of Sequential\n        self.transformer_blocks =  nn.ModuleList([\n            TransformerBlockWithMemory(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n\n        self.drop_out = nn.Dropout(cfg['drop_rate'])\n        # self.final_norm = LayerNorm(emb_dim=cfg['emb_dim'])\n        self.final_norm =  RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection = ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'], self.embedding.embeddings)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)  # Pass the mask explicitly to each block\n\n        x = self.final_norm(x)\n        logits = self.projection(x)\n\n        return logits\nclass GPTMQMemoryModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.embedding = InputEmbedding(cfg['vocab_size'], cfg['emb_dim'])\n        \n        # Shared memory system across layers\n        self.shared_memory = MemorySystem(cfg=cfg)\n        \n        # Transformer blocks with shared memory\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlockWithMemory(\n                cfg=cfg,\n                shared_memory=self.shared_memory if cfg['share_memory'] else None\n            ) for _ in range(cfg['n_layers'])\n        ])\n        \n        # Final projections\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n        self.projection = ProjectionLayer(\n            cfg['emb_dim'], \n            cfg['vocab_size'], \n            self.embedding.embeddings\n        )\n        self.memory_retention_alpha = nn.Parameter(torch.tensor(0.9))\n\n        # Memory loss coefficient\n        self.mem_loss_coef = cfg.get('mem_loss_coef', 0.3)\n\n    def forward(self, input_tokens, mask=None):\n        x = self.embedding(input_tokens)\n        \n        for block in self.transformer_blocks:\n            x = block(x, mask=mask)\n            x = self.memory_retention_alpha * x + (1 - self.memory_retention_alpha) * x.detach()\n            \n        x = self.final_norm(x)\n        logits = self.projection(x)\n        \n        return logits\n    \n    def get_memory_loss(self):\n        \"\"\"Get combined memory regularization loss\"\"\"\n        return self.mem_loss_coef * self.shared_memory.memory_loss()\n    \n    def transformer_parameters(self):\n        return [p for n, p in self.named_parameters() if 'transformer_blocks' in n and p.requires_grad]\n    \n    def memory_parameters(self):\n        return [p for n, p in self.named_parameters() if 'memory_modules' in n and p.requires_grad]\n    \n    def embedding_parameters(self):\n        return [p for n, p in self.named_parameters() if 'embedding' in n and p.requires_grad]\n    \n    def norm_parameters(self):\n        return [p for n, p in self.named_parameters() if 'normalization' in n and p.requires_grad]\n    \n    def output_parameters(self):\n        return [p for n, p in self.named_parameters() if 'output_projection' in n and p.requires_grad]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:01.148832Z","iopub.execute_input":"2025-05-03T13:01:01.149122Z","iopub.status.idle":"2025-05-03T13:01:01.162994Z","shell.execute_reply.started":"2025-05-03T13:01:01.149098Z","shell.execute_reply":"2025-05-03T13:01:01.162293Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\nclass GPTMemoryEnhanced(nn.Module):\n    def __init__(self,cfg):\n        super().__init__()\n        self.embedding =  InputEmbedding(cfg['vocab_size'] , cfg['emb_dim'])\n        self.memory_proj = nn.Linear(cfg['emb_dim'], cfg['memory_dim'])\n        self.memory_expander = nn.Linear(cfg['memory_dim'], cfg['emb_dim'])\n        self.transformer_block = nn.ModuleList([\n            TransformerBlock_v2(cfg=cfg) for _ in range(cfg['n_layers'])\n        ])\n        self.dropout =  nn.Dropout(cfg['drop_rate'])\n        self.memory =  EfiBioSemanticMemory_V2(input_dim=cfg['memory_dim'] ,semantic_memory_dim=cfg['memory_dim'],num_heads=2)\n\n        self.final_norm = RMSNorm(dim=cfg['emb_dim'])\n\n        self.projection =  ProjectionLayer(cfg['emb_dim'], cfg['vocab_size'] , self.embedding.embeddings)\n\n    def forward(self,input_tokens:torch.Tensor , mask =None):\n        x =  self.embedding(input_tokens)\n        for block in self.transformer_block:\n            x  = block(x, mask = mask)\n        memory_query = self.memory_proj(x) \n        mem_out, retrieved, topk_idx, attn_w = self.memory(memory_query)\n        # memory_out, _ ,_ =  self.memory(x.las_hidden_state.mean(1))\n        mem_out = self.memory_expander(mem_out)\n        fused  = x + mem_out\n\n        fused =  self.final_norm(fused)\n\n        logits =  self.projection(fused)\n        # return logits, {\n        #     \"memory_topk\": topk_idx, \n        #     \"memory_attn\": attn_w,\n        #     \"retrieved\":  retrieved\n        # }\n        return  logits\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:01.187383Z","iopub.execute_input":"2025-05-03T13:01:01.187576Z","iopub.status.idle":"2025-05-03T13:01:01.193657Z","shell.execute_reply.started":"2025-05-03T13:01:01.187560Z","shell.execute_reply":"2025-05-03T13:01:01.192900Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"code","source":"\n\n\n\ndef cal_loss_batch(input_batch , target_batch , model:torch.nn.Module , device:torch.device):\n    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader , model , device , num_batches = None):\n    total_loss = 0\n    if num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        num_batches  = min(num_batches , len(data_loader))\n    for i , (inputs , target) in enumerate(data_loader):\n        if i < num_batches:\n            loss  =  cal_loss_batch(inputs , target , model , device)\n\n            total_loss +=loss.item()\n\n        else:\n            break\n\n        return total_loss  / num_batches\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:02.626487Z","iopub.execute_input":"2025-05-03T13:01:02.626807Z","iopub.status.idle":"2025-05-03T13:01:02.632209Z","shell.execute_reply.started":"2025-05-03T13:01:02.626777Z","shell.execute_reply":"2025-05-03T13:01:02.631371Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Text Generation Function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \n\n\n\n\ndef text_to_token_ids(text,  tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\n\ndef token_ids_to_text(tokens , tokenizer):\n    flat  = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n    \ndef generate_and_sample(model  , idx , context_size ,max_new_tokens ):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n            print(logits.shape)\n        logits  = logits[:, -1  , :]\n        print(logits.shape)\n        probs  = torch.softmax(logits  , dim=-1)\n        print(probs)\n        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n        print(idx_next)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx \n\n#\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]  # shape: [1, current_seq_len]\n\n        # Create causal mask dynamically\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)  # [1, seq_len, seq_len]\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)  # <--- pass mask here\n\n        logits = logits[:, -1, :]  # only take the last token logits\n\n        # Apply top-k sampling if needed\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        # Temperature sampling\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n        \n\n    return idx\ndef real_time_generation(model, initial_input, context_size, temperature, top_k=None, device=\"cpu\"):\n    # Tokenize the initial input and prepare the model context\n    idx = torch.tensor(initial_input).unsqueeze(0).to(device)  # Assuming initial_input is tokenized\n    \n    print(\"Starting real-time generation...\")\n    \n    # Start generating tokens in real-time\n    for new_token in generate(model, idx, max_new_tokens=50, context_size=context_size, temperature=temperature, top_k=top_k, device=device):\n        print(f\"Generated token: {new_token.item()}\")  # Or decode it back to a word\n        \n        # You can check for user input here and update idx with the new input\n        # For instance, wait for the user to input a prompt to append to the context\n        user_input = input(\"Enter new input (or press enter to continue generation): \")\n        \n        if user_input:\n            # Tokenize the new user input and append it to the context\n            user_input_tokens = torch.tensor(tokenize(user_input)).unsqueeze(0).to(device)\n            idx = torch.cat((idx, user_input_tokens), dim=1)  # Append the new tokens to the context\n        else:\n            # Continue generating if no new user input\n            continue\n\n# Function to tokenize input (adjust depending on your tokenizer)\ndef tokenize(text):\n    # Assuming you have a tokenizer function available\n    return [ord(c) for c in text]  # Dummy example: ord() converts char to token id\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:03.047664Z","iopub.execute_input":"2025-05-03T13:01:03.047899Z","iopub.status.idle":"2025-05-03T13:01:03.058497Z","shell.execute_reply.started":"2025-05-03T13:01:03.047879Z","shell.execute_reply":"2025-05-03T13:01:03.057605Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# Dataset and DataLoader ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport tiktoken\nimport json\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef generate_prompt(sample):\n    # return f\"<user> {sample['instruction']} <bot> {sample['output']}\"\n        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']} <|endoftext|>\"\n    \n\n\nclass Dataset_V1(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.max_length = max_length\n        self.input_ids = []\n        self.target_ids = []\n        self.tokenizer =  tokenizer\n\n        all_tokens = []\n        allowed = {'<|endoftext|>'}\n        for sample in data:\n            prompt = generate_prompt(sample)\n            tokens = tokenizer.encode(prompt , allowed_special=allowed)\n            all_tokens.extend(tokens)\n\n        for i in range(0, len(all_tokens) - max_length, stride):\n            input_chunk = all_tokens[i: i + max_length]\n            target_chunk = all_tokens[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  # -100 is ignored by CrossEntropyLoss\n    return inputs, targets\n\ndef create_dataloader_v1(data, batch_size=4,\n    max_length=256, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\") #tokenizer \n    dataset = Dataset_V1(data, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last , collate_fn=collate_fn)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:10.505614Z","iopub.execute_input":"2025-05-03T13:00:10.505899Z","iopub.status.idle":"2025-05-03T13:00:10.563129Z","shell.execute_reply.started":"2025-05-03T13:00:10.505878Z","shell.execute_reply":"2025-05-03T13:00:10.562324Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Dataset And DataLoader for Psycology Dataset ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport tiktoken\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass Dataset_v2(Dataset):\n    def __init__(self, data, tokenizer, max_length, stride):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.stride = stride\n        self.input_ids = []\n\n        all_tokens = []\n        for sample in data:\n            tokens = tokenizer.encode(sample)  \n            all_tokens.extend(tokens)\n\n        # Split the tokens into chunks of size max_length with stride\n        for i in range(0, len(all_tokens) - self.max_length, self.stride):\n            input_chunk = all_tokens[i:i + self.max_length]\n            target_chunk = all_tokens[i + 1:i + self.max_length + 1]\n            self.input_ids.append((torch.tensor(input_chunk), torch.tensor(target_chunk)))\n\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, index):\n        return self.input_ids[index]\n\ndef collect_fn(batch):\n    inputs, targets = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0) \n    targets = pad_sequence(targets, batch_first=True, padding_value=-100)  \n    return inputs, targets\n\ndef create_dataloader_v2(data, batch_size=4, max_length=1024, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")  \n    dataset = Dataset_v2(data, tokenizer, max_length, stride) \n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, collate_fn=collect_fn)\n    return dataloader\n\ndef load_txt_file(filepath):\n    with open(filepath, 'r') as f:\n        text = f.read()\n    return text\n\ndef split_into_chunks(text, chunk_size=1024, overlap=200):\n\n    chunks = []\n    for i in range(0, len(text), chunk_size - overlap):\n        chunk = text[i:i + chunk_size]\n        chunks.append(chunk)\n\n    return chunks\n\n\n\nfile =  '/kaggle/input/datasetcleaned/cleaned_books.txt'\nload_text =  load_txt_file(file)\nchunk = split_into_chunks(load_text)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-03T13:00:12.025856Z","iopub.execute_input":"2025-05-03T13:00:12.026202Z","iopub.status.idle":"2025-05-03T13:00:14.316564Z","shell.execute_reply.started":"2025-05-03T13:00:12.026173Z","shell.execute_reply":"2025-05-03T13:00:14.315620Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Train Script ","metadata":{}},{"cell_type":"code","source":"\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\n\n\ndef evaluate_model(model, train_dataloader, eval_dataloader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(eval_dataloader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n\n    with torch.no_grad():\n        token_ids = generate(\n            model=model,\n            idx=encoded,\n            temperature=1.4,\n            max_new_tokens=64,   # Increase generation length if needed\n            context_size=126,\n            top_k=25\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n\n        # Trim everything before the generation\n        generated_only = decoded_text[len(start_context):].strip()\n\n        # Stop at endoftext token if present\n        end_marker = \"<|endoftext|>\"\n        if end_marker in generated_only:\n            generated_only = generated_only.split(end_marker)[0].strip()\n\n        print(f\"\\n[Prompt]: {start_context.strip()}\\n\")\n        print(f\"[Generated]: {generated_only}\\n\")\n\n    model.train()\ndef save_model_checkpoint(model, optimizer, epoch, path=\"checkpoint_epoch_{}.pt\"):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch\n    }\n    torch.save(checkpoint, path.format(epoch))\ndef after_save_load():\n    checkpoint = torch.load(\"checkpoint_epoch_7.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n\n\ndef train_model(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n    # scheduler = get_cosine_schedule_with_warmup(optimizer,\n    #                                         num_warmup_steps=500,\n    #                                         num_training_steps=total_steps)\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            # scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n        save_model_checkpoint(model , optimizer , epoch+1)\n\n\n    return train_losses, val_losses, track_tokens_seen\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:14.317937Z","iopub.execute_input":"2025-05-03T13:00:14.318268Z","iopub.status.idle":"2025-05-03T13:00:16.389586Z","shell.execute_reply.started":"2025-05-03T13:00:14.318238Z","shell.execute_reply":"2025-05-03T13:00:16.388856Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# MemoryGPT Model training  \n","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.nn.utils import clip_grad_norm_\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:16.391184Z","iopub.execute_input":"2025-05-03T13:00:16.391520Z","iopub.status.idle":"2025-05-03T13:00:16.394958Z","shell.execute_reply.started":"2025-05-03T13:00:16.391500Z","shell.execute_reply":"2025-05-03T13:00:16.394119Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# GPT Config ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nGPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,#126 \n# Context lengt\n\"emb_dim\": 768,\n# Embedding dimension\n\"n_heads\": 12,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False\n# Query-Key-Value bias\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:16.395864Z","iopub.execute_input":"2025-05-03T13:00:16.396135Z","iopub.status.idle":"2025-05-03T13:00:16.420304Z","shell.execute_reply.started":"2025-05-03T13:00:16.396111Z","shell.execute_reply":"2025-05-03T13:00:16.419522Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_memory_model(\n    model: nn.Module,\n    train_dataloader: DataLoader,\n    device: torch.device,\n    eval_dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    max_grad_norm: float = 1.0,\n):\n    torch.autograd.set_detect_anomaly(True)\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    memory_metrics = []\n    tokens_seen, global_step = 0, -1\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # Create TensorBoard writer\n    # writer = SummaryWriter(log_dir='runs/memory_experiment')\n\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        for batch_idx, (inputs_batch, target_batch) in enumerate(train_dataloader):\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            \n            # Forward pass with memory\n            logits = model(inputs_batch)\n            \n            # Calculate loss with memory regularization\n            loss = cal_loss_batch(inputs_batch, target_batch, model, device)\n            loss += model.memory_system.episodic_memory_cell.memory_regularization_loss()\n            loss += model.memory_system.semantic_memory_cell.memory_regularization_loss()\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n\n            # Memory maintenance\n            if batch_idx % 100 == 0:\n                model.memory_system.maintain_memory()\n\n            # Logging and evaluation\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                # Evaluate and get memory metrics\n                train_loss, val_loss, mem_metrics = evaluate_memory_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n                \n                # Record metrics\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                memory_metrics.append(mem_metrics)\n                \n                # TensorBoard logging\n                writer.add_scalar('Loss/Train', train_loss, global_step)\n                writer.add_scalar('Loss/Val', val_loss, global_step)\n                log_memory_metrics(writer, mem_metrics, global_step)\n                \n                print(f\"Step {global_step}:\")\n                print(f\"  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n                print_memory_health(mem_metrics)\n\n            # Generate samples with current memory state\n            if global_step % (eval_freq*2) == 0:\n                generate_with_memory(\n                    model, \n                    train_dataloader.dataset.tokenizer, \n                    device, \n                    start_context,\n                    writer,\n                    global_step\n                )\n\n        # Save model and memory state\n        save_memory_checkpoint(model, optimizer, epoch+1, global_step)\n\n    writer.close()\n    return train_losses, val_losses, track_tokens_seen, memory_metrics\n\ndef evaluate_memory_model(model, train_loader, eval_loader, device, eval_iter):\n    model.eval()\n    train_loss, val_loss = 0, 0\n    memory_stats = []\n    \n    with torch.no_grad():\n        # Evaluate on training set\n        for i, (inputs, targets) in enumerate(train_loader):\n            if i >= eval_iter: break\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            train_loss += cal_loss_batch(inputs, targets, model, device).item()\n            \n        # Evaluate on validation set\n        for i, (inputs, targets) in enumerate(eval_loader):\n            if i >= eval_iter: break\n            inputs, targets = inputs.to(device), targets.to(device)\n         \n            logits =model(inputs)\n            val_loss += cal_loss_batch(inputs, targets, model, device).item()\n            \n        # Get memory health metrics\n        mem_health = model.memory_system.maintain_memory()\n        \n    model.train()\n    return train_loss/eval_iter, val_loss/eval_iter, mem_health\n\ndef log_memory_metrics(writer, metrics, step):\n    # Episodic memory metrics\n    writer.add_scalar('Memory/Episodic/UsedSlots', \n                     metrics['episodic']['active_slots'], step)\n    writer.add_scalar('Memory/Episodic/MeanSimilarity', \n                     metrics['episodic']['memory_similarity']['mean_similarity'], step)\n    \n    # Semantic memory metrics\n    writer.add_scalar('Memory/Semantic/ActiveConcepts', \n                     metrics['semantic']['active_concepts'], step)\n    writer.add_scalar('Memory/Semantic/EnergyMean', \n                     metrics['semantic']['energy_mean'], step)\n\ndef print_memory_health(metrics):\n    print(\"Memory Health:\")\n    print(f\"  Episodic: {metrics['episodic']['active_slots']} active slots\")\n    print(f\"    Similarity: {metrics['episodic']['memory_similarity']['mean_similarity']:.3f}\")\n    print(f\"  Semantic: {metrics['semantic']['active_concepts']} concepts\")\n    print(f\"    Energy: {metrics['semantic']['energy_mean']:.3f}\")\n\ndef generate_with_memory(model, tokenizer, device, start_context, writer=None, step=None):\n    model.eval()\n    with torch.no_grad():\n        # Generate with current memory state\n        input_ids = tokenizer.encode(start_context, return_tensors='pt').to(device)\n        \n        # Generate text with memory context\n        outputs = model.generate(\n            input_ids,\n            max_length=100,\n            temperature=0.8,\n            do_sample=True,\n            memory_context=model.memory_system.get_memory_state()\n        )\n        \n        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(\"\\nGenerated Text with Memory:\")\n        print(text)\n        \n        if writer:\n            writer.add_text(\"Generated Text\", text, step)\n    \n    model.train()\n\ndef save_memory_checkpoint(model, optimizer, epoch, step):\n    state = {\n        'epoch': epoch,\n        'step': step,\n        'model_state': model.state_dict(),\n        'optimizer_state': optimizer.state_dict(),\n        'memory_system': {\n            'episodic': model.memory_system.episodic_memory_cell.get_memory(),\n            'semantic': model.memory_system.semantic_memory_cell.get_memory(),\n            'router': model.memory_system.memory_router.state_dict()\n        }\n    }\n    torch.save(state, f\"memory_checkpoint_{epoch}_{step}.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:16.421264Z","iopub.execute_input":"2025-05-03T13:00:16.421557Z","iopub.status.idle":"2025-05-03T13:00:16.439117Z","shell.execute_reply.started":"2025-05-03T13:00:16.421527Z","shell.execute_reply":"2025-05-03T13:00:16.438321Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# model = GPTMQModel2(GPT_CONFIG_124M)\n\n\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# model.to(device)\n# optimizer  =  torch.optim.AdamW(model.parameters() , lr = 0.0004  , weight_decay= 0.01)\n# num_epochs = 1\n# train_ratio = 0.90\n\n# text_data = chunk\n# print(len(text_data))\n# split = int(train_ratio * len(text_data))\n# print(split)\n# train_data= text_data[:split]\n# val_data = text_data[split:]\n# # train_dataloader = create_dataloader_v1(txt= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True , stride=GPT_CONFIG_124M['context_length'])\n# # val_dataloader = create_dataloader_v1(txt= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False , stride=GPT_CONFIG_124M['context_length'])\n# train_dataloader = create_dataloader_v2(data= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True )\n# val_dataloader = create_dataloader_v2(data= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False )\n# start_context = '### Instruction :Give three tips for staying healthy ### Response:'\n# print('start trainning')\n# train_losses , val_losses  , token_seen = train_model(\n#     model= model , train_dataloader= train_dataloader , \n#     eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n#     eval_iter=3 , start_context=start_context, num_epochs=2\n# )\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:16.439862Z","iopub.execute_input":"2025-05-03T13:00:16.440133Z","iopub.status.idle":"2025-05-03T13:00:16.454180Z","shell.execute_reply.started":"2025-05-03T13:00:16.440105Z","shell.execute_reply":"2025-05-03T13:00:16.453461Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"GPT_CONFIG_124M_Memory = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 126,\n# Context length\n\"emb_dim\": 128,\n# Embedding dimension\n\"n_heads\": 4,\n# Number of attention heads\n\"n_layers\": 12,\n# Number of layers\n\"drop_rate\": 0.1,\n# Dropout rate\n\"qkv_bias\": False,\n'memory_dim':128,\n'max_slots' :1000,\n'memory_heads':2 ,\n\n# Query-Key-Value bias\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:00:16.880662Z","iopub.execute_input":"2025-05-03T13:00:16.880996Z","iopub.status.idle":"2025-05-03T13:00:16.885009Z","shell.execute_reply.started":"2025-05-03T13:00:16.880971Z","shell.execute_reply":"2025-05-03T13:00:16.884397Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\ndevice =  'cuda' if torch.cuda.is_available() else \"cpu\"\nmodel =  GPTMemoryEnhanced(GPT_CONFIG_124M_Memory).to(device)\n\noptimizer =  torch.optim.AdamW(model.parameters() , lr=0.0004,weight_decay=0.01 )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:08.832572Z","iopub.execute_input":"2025-05-03T13:01:08.832875Z","iopub.status.idle":"2025-05-03T13:01:08.947961Z","shell.execute_reply.started":"2025-05-03T13:01:08.832850Z","shell.execute_reply":"2025-05-03T13:01:08.947102Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from collections import defaultdict\n\ndef get_param_group_summary(model):\n    groups = defaultdict(int)\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if \"embedding\" in name:\n            groups[\"embedding\"] += param.numel()\n        elif \"transformer\" in name:\n            groups[\"transformer_blocks\"] += param.numel()\n        elif \"memory\" in name or \"episodic\" in name or \"semantic\" in name:\n            groups[\"memory_modules\"] += param.numel()\n        elif \"norm\" in name:\n            groups[\"normalization\"] += param.numel()\n        elif \"lm_head\" in name or \"projection\" in name:\n            groups[\"output_projection\"] += param.numel()\n        else:\n            groups[\"other\"] += param.numel()\n    total = sum(groups.values())\n    for k, v in groups.items():\n        print(f\"{k:20s}: {v:,} parameters\")\n    print(f\"\\nTotal: {total:,}\")\nget_param_group_summary(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:08.949148Z","iopub.execute_input":"2025-05-03T13:01:08.949439Z","iopub.status.idle":"2025-05-03T13:01:08.957151Z","shell.execute_reply.started":"2025-05-03T13:01:08.949406Z","shell.execute_reply":"2025-05-03T13:01:08.956280Z"}},"outputs":[{"name":"stdout","text":"embedding           : 6,432,896 parameters\nmemory_modules      : 648,587 parameters\ntransformer_blocks  : 2,082,048 parameters\nnormalization       : 128 parameters\noutput_projection   : 50,257 parameters\n\nTotal: 9,213,916\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"\n\n\n\n\nnum_epochs =  1\ntrain_ratio = 0.90\n\nfilename =  '/kaggle/input/alphaco/alpaca_data_cleaned.json'\nwith open(filename , 'r') as f:\n    text_data =  json.load(f)\ntext_data = text_data[:200]\nsplit = int(train_ratio * len(text_data))\n\ntrain_data =  text_data[:split]\nval_data =  text_data[split:]\n\ntrain_dataloader =  create_dataloader_v1(data=train_data , batch_size=2 , max_length=GPT_CONFIG_124M_Memory['context_length'] , shuffle=True , drop_last= True)\nval_dataloader = create_dataloader_v1(data=val_data , batch_size=2 , max_length=GPT_CONFIG_124M_Memory['context_length']  , shuffle=False , drop_last=False )\n\n\nprint('start training')\ntrain_losses , val_losses , token_seen =  train_model(\n    model=model , \n    train_dataloader=train_dataloader, \n    device=device, \n    eval_freq=5 , \n    eval_dataloader=val_dataloader , \n    optimizer=optimizer, \n    eval_iter=3,  \n    num_epochs=1, \n    start_context='Hello '\n)\nprint(model.memory.get_memory_metrics())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:09.525455Z","iopub.execute_input":"2025-05-03T13:01:09.525700Z","iopub.status.idle":"2025-05-03T13:02:17.411011Z","shell.execute_reply.started":"2025-05-03T13:01:09.525680Z","shell.execute_reply":"2025-05-03T13:02:17.410311Z"}},"outputs":[{"name":"stdout","text":"start training\n🚀 Total training steps: 108\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"666f7b8846274d44a0493ef576dd8a4b"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 (step 000000): Train Loss: 41.1002, Val Loss: 41.8124\nEpoch: 1 (step 000005): Train Loss: 39.7114, Val Loss: 40.8948\nEpoch: 1 (step 000010): Train Loss: 39.3642, Val Loss: 39.1970\nEpoch: 1 (step 000015): Train Loss: 35.6180, Val Loss: 35.3556\nEpoch: 1 (step 000020): Train Loss: 27.8489, Val Loss: 26.9149\nEpoch: 1 (step 000025): Train Loss: 16.0474, Val Loss: 16.3597\nEpoch: 1 (step 000030): Train Loss: 13.5512, Val Loss: 12.5578\nEpoch: 1 (step 000035): Train Loss: 11.9057, Val Loss: 11.2702\nEpoch: 1 (step 000040): Train Loss: 10.5021, Val Loss: 10.5689\nEpoch: 1 (step 000045): Train Loss: 9.9649, Val Loss: 10.0796\nEpoch: 1 (step 000050): Train Loss: 8.9862, Val Loss: 9.6967\nEpoch: 1 (step 000055): Train Loss: 8.6183, Val Loss: 9.3394\nEpoch: 1 (step 000060): Train Loss: 8.6824, Val Loss: 9.0257\nEpoch: 1 (step 000065): Train Loss: 8.7497, Val Loss: 8.7808\nEpoch: 1 (step 000070): Train Loss: 8.4276, Val Loss: 8.5648\nEpoch: 1 (step 000075): Train Loss: 8.1757, Val Loss: 8.3697\nEpoch: 1 (step 000080): Train Loss: 7.8516, Val Loss: 8.1919\nEpoch: 1 (step 000085): Train Loss: 8.2779, Val Loss: 8.0269\nEpoch: 1 (step 000090): Train Loss: 8.0021, Val Loss: 7.8737\nEpoch: 1 (step 000095): Train Loss: 8.1598, Val Loss: 7.7286\nEpoch: 1 (step 000100): Train Loss: 7.1357, Val Loss: 7.5920\nEpoch: 1 (step 000105): Train Loss: 7.5952, Val Loss: 7.4619\n\n[Prompt]: Hello\n\n[Generated]: elaela373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373373\n\n{'memory_size': 134, 'active_concepts': 1000, 'utilization': 7.462686567164179, 'energy_mean': 0.979552686214447, 'energy_std': 0.0014502137200906873, 'age_histogram': tensor([   0.,    0.,    0.,    0.,    0., 1000.,    0.,    0.,    0.,    0.],\n       device='cuda:0'), 'usage_histogram': tensor([995.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   5.],\n       device='cuda:0'), 'access_histogram': tensor([   0.,    0.,    0.,    0.,    0., 1000.,    0.,    0.,    0.,    0.],\n       device='cuda:0'), 'merge_rate': 0.0, 'prune_rate': 0.9950000643730164, 'neuro_rate': 1.0, 'reuse_efficiency': 0.0, 'steps': 216, 'queries': 368, 'novelty_rate': 0.32608695652173914, 'write_rate': 0.021739130434782608, 'hit_rate': 0.0}\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:23:02.485829Z","iopub.execute_input":"2025-05-03T09:23:02.486130Z","iopub.status.idle":"2025-05-03T09:23:02.490772Z","shell.execute_reply.started":"2025-05-03T09:23:02.486107Z","shell.execute_reply":"2025-05-03T09:23:02.490057Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:47:03.421280Z","iopub.execute_input":"2025-04-24T09:47:03.421571Z","iopub.status.idle":"2025-04-24T09:47:03.430581Z","shell.execute_reply.started":"2025-04-24T09:47:03.421551Z","shell.execute_reply":"2025-04-24T09:47:03.429796Z"}},"outputs":[{"name":"stdout","text":"memory_modules      : 10,305,011 parameters\nembedding           : 38,725,376 parameters\ntransformer_blocks  : 72,061,464 parameters\nnormalization       : 768 parameters\noutput_projection   : 50,257 parameters\n\nTotal: 121,142,876\n","output_type":"stream"}],"execution_count":60},{"cell_type":"raw","source":"def count_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n# your custom GPT, MemoryLLM, etc.\ntotal_params = count_trainable_parameters(model)\nprint(f\"Trainable parameters: {total_params:,}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-04-24T09:45:21.381944Z","iopub.execute_input":"2025-04-24T09:45:21.382287Z","iopub.status.idle":"2025-04-24T09:45:21.388203Z","shell.execute_reply.started":"2025-04-24T09:45:21.382238Z","shell.execute_reply":"2025-04-24T09:45:21.387376Z"}}},{"cell_type":"code","source":"from collections import defaultdict\n\ndef modulewise_param_count(model):\n    module_params = defaultdict(int)\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            module = name.split('.')[0]  # or custom parsing\n            module_params[module] += param.numel()\n    \n    for module, count in sorted(module_params.items(), key=lambda x: -x[1]):\n        print(f\"{module:<20} : {count:,} parameters\")\n\nmodulewise_param_count(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:45:21.656375Z","iopub.execute_input":"2025-04-24T09:45:21.656704Z","iopub.status.idle":"2025-04-24T09:45:21.664403Z","shell.execute_reply.started":"2025-04-24T09:45:21.656679Z","shell.execute_reply":"2025-04-24T09:45:21.663432Z"}},"outputs":[{"name":"stdout","text":"transformer_blocks   : 72,061,464 parameters\nembedding            : 38,597,376 parameters\nshared_memory        : 10,433,010 parameters\nprojection           : 50,257 parameters\nfinal_norm           : 768 parameters\nmemory_retention_alpha : 1 parameters\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"def train_model_restart(\n    model: nn.Module,\n    train_dataloader: torch.utils.data.DataLoader,\n    device: torch.device,\n    eval_dataloader: torch.utils.data.DataLoader,\n    optimizer: torch.optim.Optimizer,\n    eval_freq: int,\n    eval_iter: int,\n    start_context: str,\n    num_epochs: int = 1,\n    checkpoint_path: str = None\n):\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step, start_epoch = 0, -1, 0\n\n    total_steps = len(train_dataloader) * num_epochs\n    print(f\"🚀 Total training steps: {total_steps}\")\n\n    # 🔁 Load from checkpoint if provided\n    if checkpoint_path is not None:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)\n\n        # ⬇️ Reduce learning rate by half when resuming\n        for param_group in optimizer.param_groups:\n            old_lr = param_group['lr']\n            param_group['lr'] = old_lr * 0.5\n            print(f\"🔧 Reduced LR: {old_lr:.6f} ➜ {param_group['lr']:.6f}\")\n\n        print(f\"✅ Resuming training from Epoch {start_epoch}\")\n\n    # ⚙️ Reinitialize scheduler after changing LR\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=500,\n        num_training_steps=total_steps\n    )\n\n    for epoch in tqdm(range(start_epoch, num_epochs)):\n        model.train()\n        for inputs_batch, target_batch in train_dataloader:\n            inputs_batch, target_batch = inputs_batch.to(device), target_batch.to(device)\n\n            optimizer.zero_grad()\n            loss = cal_loss_batch(input_batch=inputs_batch, target_batch=target_batch, device=device, model=model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            tokens_seen += inputs_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_dataloader, eval_dataloader, device, eval_iter\n                )\n\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n\n                print(\n                    f\"Epoch: {epoch+1} (step {global_step:06d}):\",\n                    f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n                )\n\n        generate_and_print_sample(\n            model, train_dataloader.dataset.tokenizer, device, start_context\n        )\n\n        save_model_checkpoint(model, optimizer, epoch + 1, global_step, tokens_seen)\n\n    return train_losses, val_losses, track_tokens_seen\n\ndef save_model_checkpoint(model, optimizer, epoch, global_step=None, tokens_seen=None, path=\"checkpoint_epoch.pt\"):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    if global_step is not None:\n        checkpoint['global_step'] = global_step\n    if tokens_seen is not None:\n        checkpoint['tokens_seen'] = tokens_seen\n\n    torch.save(checkpoint, f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\")\n    print(f\"💾 Saved checkpoint at epoch {epoch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.412648Z","iopub.status.idle":"2025-04-24T09:34:44.413047Z","shell.execute_reply":"2025-04-24T09:34:44.412872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"        checkpoint = torch.load('/kaggle/working/checkpoint_epoch_6.pt', map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        global_step = checkpoint.get('global_step', -1)\n        tokens_seen = checkpoint.get('tokens_seen', 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.413954Z","iopub.status.idle":"2025-04-24T09:34:44.414329Z","shell.execute_reply":"2025-04-24T09:34:44.414156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses , val_losses  , token_seen = train_model(\n    model= model , train_dataloader= train_dataloader , \n    eval_dataloader= val_dataloader , optimizer= optimizer , eval_freq=5 , device= device,\n    eval_iter=3 , start_context=start_context, num_epochs=2\n)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.415232Z","iopub.status.idle":"2025-04-24T09:34:44.415568Z","shell.execute_reply":"2025-04-24T09:34:44.415454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nloss_history = {\n    \"train_loss\": train_losses,\n    \"val_loss\": val_losses,\n    \"tokens_seen\": token_seen\n}\n\nwith open(\"loss_history.json\", \"w\") as f:\n    json.dump(loss_history, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.416549Z","iopub.status.idle":"2025-04-24T09:34:44.416821Z","shell.execute_reply":"2025-04-24T09:34:44.416720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# If you loaded from a JSON file\n# with open(\"loss_history.json\", \"r\") as f:\n#     data = json.load(f)\n#     train_losses = data[\"train_loss\"]\n#     val_losses = data[\"val_loss\"]\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\nplt.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\nplt.xlabel(\"Evaluation Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"loss_curve.png\")  # Save the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.417430Z","iopub.status.idle":"2025-04-24T09:34:44.417738Z","shell.execute_reply":"2025-04-24T09:34:44.417597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\n\n# Load model\nmodel = GPTMQModel2(GPT_CONFIG_124M)\n# model.load_state_dict(torch.load(\"/kaggle/working/checkpoint_epoch_7.pt\"))\ncheckpoint = torch.load(\"checkpoint_epoch_7.pt\")\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\n\nmodel.eval().to(device)\n\n# Tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Utility functions\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\ndef token_ids_to_text(tokens, tokenizer):\n    flat = tokens.squeeze(0)\n    decode = tokenizer.decode(flat.tolist())\n    return decode\n\n# Sampling-based generate function (uses your logic)\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\nf\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n    return idx\n\n# High-level text generation function\ndef generate_response(prompt, model, tokenizer, max_new_tokens=100, context_size=128, temperature=1.0, top_k=50):\n    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n    generated_ids = generate(\n        model=model,\n        idx=input_ids,\n        max_new_tokens=max_new_tokens,\n        context_size=context_size,\n        temperature=temperature,\n        top_k=top_k\n    )\n    return token_ids_to_text(generated_ids, tokenizer)\n\n# Try it out\n# prompt = \"### Instruction:\\nExplain what is deep learning.\\n\\n### Response:\\n <bot>\"\nprompt = \"\"\"\n\n'### Instruction :Give three tips for staying healthy ### Response:'\n\"\"\".strip()\n\n\noutput = generate_response(prompt, model, tokenizer)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.418499Z","iopub.status.idle":"2025-04-24T09:34:44.418773Z","shell.execute_reply":"2025-04-24T09:34:44.418675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device)\n        causal_mask = causal_mask.unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is generated\n        if idx_next.item() == end_token_id:\n            break\n\n    return idx\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\nraw_output = generate_response(prompt, model, tokenizer)\ncleaned_output = truncate_after_n_bullets(raw_output)\nprint(cleaned_output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.419352Z","iopub.status.idle":"2025-04-24T09:34:44.419579Z","shell.execute_reply":"2025-04-24T09:34:44.419486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_response(\n    prompt, model, tokenizer,\n    temperature=0.8,  # better balance\n    top_k=40,         # a bit narrower selection\n    max_new_tokens=100\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.420138Z","iopub.status.idle":"2025-04-24T09:34:44.420420Z","shell.execute_reply":"2025-04-24T09:34:44.420314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded = torch.tensor(encoded).unsqueeze(0)\n    return encoded\n\nend_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n\ndef generate(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:]\n        seq_len = idx_cond.size(1)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(idx.device).unsqueeze(0)\n\n        with torch.no_grad():\n            logits = model(idx_cond, mask=causal_mask)\n\n        logits = logits[:, -1, :]\n\n        if top_k is not None:\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n\n        if temperature > 0.0:\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n        idx = torch.cat((idx, idx_next), dim=1)\n\n        # Stop generation if <|endoftext|> is in the generated output\n        if end_token_id in idx_next:\n            break\n\n    return idx\n\ndef truncate_after_n_bullets(text, n=3):\n    lines = text.split(\"\\n\")\n    count = 0\n    result = []\n    for line in lines:\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            count += 1\n        result.append(line)\n        if count >= n:\n            break\n    return \"\\n\".join(result)\n\n# 🔁 Input prompt\nprompt = \"### Instruction: What are the three primary colors? \\n### Response:\"\n\n# 🔁 Tokenize input\ninput_ids = text_to_token_ids(prompt, tokenizer).to(device)\n\n# 🔁 Generate output tokens\noutput_ids = generate(\n    model=model,\n    idx=input_ids,\n    max_new_tokens=100,\n    context_size=128,\n    temperature=0.7,\n    top_k=40\n)\n\n# 🔁 Decode and postprocess\noutput_text = tokenizer.decode(output_ids[0].tolist())\n\n# ✂️ Truncate after 3 bullets (optional)\nfinal_output = truncate_after_n_bullets(output_text)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:34:44.421013Z","iopub.status.idle":"2025-04-24T09:34:44.421333Z","shell.execute_reply":"2025-04-24T09:34:44.421177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}